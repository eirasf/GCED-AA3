{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca397a2f",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eirasf/GCED-AA3/blob/main/lab3/lab3.ipynb)\n",
    "\n",
    "# Modelos *Ensemble*\n",
    "\n",
    "Una de las últimas tendencias dentro de lo que serían los modelos de inteligencia artificial viene a resumirse como \"el conocimiento del conjunto o la multitud\". Lo que viene a definir esta frase, un tanto popular, es el uso de multitud de modelos denominados \"débiles\" en un metaclasificador. El objetivo es generar un modelo \"fuerte\" en base al conocimiento extraído por los modelos \"débiles\". Por ejemplo, aunque se detallará más adelante, en un *Random Forest* se desarrollan múltiples *Decision Trees* mucho más simples. La combinación de estos en el *Random Forest* excede el rendimiento de cualquiera de los modelos individuales. Los modelos surgidos de está manera, como metaclasificadores o metaregresores  reciben el nombre genérico de modelos *Ensemble*.\n",
    "\n",
    "Un hecho a destacar es el hecho de que estos modelos pueden no limitarse unicamente a los arboles de decisión, por contra pueden componerse de cualquier tipo de modelo de aprendizaje automático que se ha visto previamente. Incluso pueden ser modelos mixtos donde no todos los modelos se hayan obtenido de la misma manera, si no que pueden ser creados mediante el uso combinado de varias técnicas como pueden ser el K-NN, SVM, etc.\n",
    "\n",
    "En la presente unidad se va a explorar varias maneras de como generar los modelos y como combinarlos posteriormente. Así mismo, se verán dos de las técnicas más habituales dentro de estos modelos *ensemble* como son el _Random Forest_, y _XGBoost_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b2c4fb",
   "metadata": {},
   "source": [
    "## Preparar los datos\n",
    "En este tutorial vamos a utilizar los mismos datos que se emplearon en el priemro de los tutoriales, así que si ya los tiene descargados puede copiarlos y cargarlos en la carpeta o, dado el pequeo tamaño de los mismos, volver a descargarlos con el  siguiente código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67d1c8ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1       2       3       4       5       6       7       8   \\\n",
       "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "\n",
       "       9   ...      51      52      53      54      55      56      57  \\\n",
       "0  0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
       "1  0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
       "2  0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
       "3  0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
       "4  0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
       "\n",
       "       58      59  60  \n",
       "0  0.0090  0.0032   R  \n",
       "1  0.0052  0.0044   R  \n",
       "2  0.0095  0.0078   R  \n",
       "3  0.0040  0.0117   R  \n",
       "4  0.0107  0.0094   R  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_data(folder, file, url):\n",
    "    '''\n",
    "        Función de utilidad que comprueba si los datos están presentes en el directorio, \n",
    "        en caso de que no estén los descarga y en cualquiera de los casos los carga.\n",
    "    '''\n",
    "    \n",
    "    #Comprobación de seguridad\n",
    "    if os.path.exists(folder) and not os.path.isdir(folder):\n",
    "        raise(Exception(\"The name of the root folder is already in use\"))\n",
    "        \n",
    "    if not os.path.exists(folder):\n",
    "        os.mkdir(folder)\n",
    "        \n",
    "    file_path = os.path.join(folder, file)\n",
    "    \n",
    "    if not os.path.isfile(file_path):\n",
    "        print(f'Downloading'.ljust(75,'.'), end='', flush=True)\n",
    "        import urllib.request\n",
    "        urllib.request.urlretrieve(url,file_path)\n",
    "        print(f\"Done!\")\n",
    "        \n",
    "    return pd.read_csv(file_path, delimiter=',', header=None)\n",
    "\n",
    "data_folder = '_data_'\n",
    "file_name = 'sonar.all_data'\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data'\n",
    "\n",
    "\n",
    "data = load_data(data_folder,file_name, url)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445244c9",
   "metadata": {},
   "source": [
    "A modo de recordatorio, se trata de un problema clásico de *machine learning* del repositorio UCIde la univerisidade de Berkley.  Se denomina informalmente como **¿Roca o Mina?** y consta de 111 patrones correspondientes a rocas y 97 a minas acuáticas. Cada uno de los patrones consta de 60 medidas numéricas correspondientes a un tramo de las fecuencias emitidas por el sonar. Estos valores se encuentran ya entre 0.0. y 1.0. Dichas medidas representan el valor de la energia de diferentes rangos de longitud de onda para un cierto periodo de tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ebd1d9",
   "metadata": {},
   "source": [
    "A mayores de esta carga es conveniente hacer una exploración inicial con el fin de detectar posibles problemas en los datos como los comentados en teoría, es decir, datos atípicos o no normalizados, datos ausentes de cualquiera de los 3 subtipos (MCAR, MAR, MNAR) o posibles sesgos que pudieran aparecer. Dicha exploración en el caso de disponer como en este caso de un _Dataframe_ tipo `pandas`, se puede hacer comenzar con las siguientes líneas de código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "245bef01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 208 entries, 0 to 207\n",
      "Data columns (total 61 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       208 non-null    float64\n",
      " 1   1       208 non-null    float64\n",
      " 2   2       208 non-null    float64\n",
      " 3   3       208 non-null    float64\n",
      " 4   4       208 non-null    float64\n",
      " 5   5       208 non-null    float64\n",
      " 6   6       208 non-null    float64\n",
      " 7   7       208 non-null    float64\n",
      " 8   8       208 non-null    float64\n",
      " 9   9       208 non-null    float64\n",
      " 10  10      208 non-null    float64\n",
      " 11  11      208 non-null    float64\n",
      " 12  12      208 non-null    float64\n",
      " 13  13      208 non-null    float64\n",
      " 14  14      208 non-null    float64\n",
      " 15  15      208 non-null    float64\n",
      " 16  16      208 non-null    float64\n",
      " 17  17      208 non-null    float64\n",
      " 18  18      208 non-null    float64\n",
      " 19  19      208 non-null    float64\n",
      " 20  20      208 non-null    float64\n",
      " 21  21      208 non-null    float64\n",
      " 22  22      208 non-null    float64\n",
      " 23  23      208 non-null    float64\n",
      " 24  24      208 non-null    float64\n",
      " 25  25      208 non-null    float64\n",
      " 26  26      208 non-null    float64\n",
      " 27  27      208 non-null    float64\n",
      " 28  28      208 non-null    float64\n",
      " 29  29      208 non-null    float64\n",
      " 30  30      208 non-null    float64\n",
      " 31  31      208 non-null    float64\n",
      " 32  32      208 non-null    float64\n",
      " 33  33      208 non-null    float64\n",
      " 34  34      208 non-null    float64\n",
      " 35  35      208 non-null    float64\n",
      " 36  36      208 non-null    float64\n",
      " 37  37      208 non-null    float64\n",
      " 38  38      208 non-null    float64\n",
      " 39  39      208 non-null    float64\n",
      " 40  40      208 non-null    float64\n",
      " 41  41      208 non-null    float64\n",
      " 42  42      208 non-null    float64\n",
      " 43  43      208 non-null    float64\n",
      " 44  44      208 non-null    float64\n",
      " 45  45      208 non-null    float64\n",
      " 46  46      208 non-null    float64\n",
      " 47  47      208 non-null    float64\n",
      " 48  48      208 non-null    float64\n",
      " 49  49      208 non-null    float64\n",
      " 50  50      208 non-null    float64\n",
      " 51  51      208 non-null    float64\n",
      " 52  52      208 non-null    float64\n",
      " 53  53      208 non-null    float64\n",
      " 54  54      208 non-null    float64\n",
      " 55  55      208 non-null    float64\n",
      " 56  56      208 non-null    float64\n",
      " 57  57      208 non-null    float64\n",
      " 58  58      208 non-null    float64\n",
      " 59  59      208 non-null    float64\n",
      " 60  60      208 non-null    object \n",
      "dtypes: float64(60), object(1)\n",
      "memory usage: 109.4 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.029164</td>\n",
       "      <td>0.038437</td>\n",
       "      <td>0.043832</td>\n",
       "      <td>0.053892</td>\n",
       "      <td>0.075202</td>\n",
       "      <td>0.104570</td>\n",
       "      <td>0.121747</td>\n",
       "      <td>0.134799</td>\n",
       "      <td>0.178003</td>\n",
       "      <td>0.208259</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013420</td>\n",
       "      <td>0.010709</td>\n",
       "      <td>0.010941</td>\n",
       "      <td>0.009290</td>\n",
       "      <td>0.008222</td>\n",
       "      <td>0.007820</td>\n",
       "      <td>0.007949</td>\n",
       "      <td>0.007941</td>\n",
       "      <td>0.006507</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.022991</td>\n",
       "      <td>0.032960</td>\n",
       "      <td>0.038428</td>\n",
       "      <td>0.046528</td>\n",
       "      <td>0.055552</td>\n",
       "      <td>0.059105</td>\n",
       "      <td>0.061788</td>\n",
       "      <td>0.085152</td>\n",
       "      <td>0.118387</td>\n",
       "      <td>0.134416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009634</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>0.007301</td>\n",
       "      <td>0.007088</td>\n",
       "      <td>0.005736</td>\n",
       "      <td>0.005785</td>\n",
       "      <td>0.006470</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>0.005031</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.013350</td>\n",
       "      <td>0.016450</td>\n",
       "      <td>0.018950</td>\n",
       "      <td>0.024375</td>\n",
       "      <td>0.038050</td>\n",
       "      <td>0.067025</td>\n",
       "      <td>0.080900</td>\n",
       "      <td>0.080425</td>\n",
       "      <td>0.097025</td>\n",
       "      <td>0.111275</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007275</td>\n",
       "      <td>0.005075</td>\n",
       "      <td>0.005375</td>\n",
       "      <td>0.004150</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.003675</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.022800</td>\n",
       "      <td>0.030800</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>0.044050</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.092150</td>\n",
       "      <td>0.106950</td>\n",
       "      <td>0.112100</td>\n",
       "      <td>0.152250</td>\n",
       "      <td>0.182400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.009550</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.006850</td>\n",
       "      <td>0.005950</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.035550</td>\n",
       "      <td>0.047950</td>\n",
       "      <td>0.057950</td>\n",
       "      <td>0.064500</td>\n",
       "      <td>0.100275</td>\n",
       "      <td>0.134125</td>\n",
       "      <td>0.154000</td>\n",
       "      <td>0.169600</td>\n",
       "      <td>0.233425</td>\n",
       "      <td>0.268700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016725</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.010575</td>\n",
       "      <td>0.010425</td>\n",
       "      <td>0.010350</td>\n",
       "      <td>0.010325</td>\n",
       "      <td>0.008525</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.137100</td>\n",
       "      <td>0.233900</td>\n",
       "      <td>0.305900</td>\n",
       "      <td>0.426400</td>\n",
       "      <td>0.401000</td>\n",
       "      <td>0.382300</td>\n",
       "      <td>0.372900</td>\n",
       "      <td>0.459000</td>\n",
       "      <td>0.682800</td>\n",
       "      <td>0.710600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070900</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>0.035200</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.039400</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>0.043900</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0           1           2           3           4   \\\n",
       "count   208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "unique         NaN         NaN         NaN         NaN         NaN   \n",
       "top            NaN         NaN         NaN         NaN         NaN   \n",
       "freq           NaN         NaN         NaN         NaN         NaN   \n",
       "mean      0.029164    0.038437    0.043832    0.053892    0.075202   \n",
       "std       0.022991    0.032960    0.038428    0.046528    0.055552   \n",
       "min       0.001500    0.000600    0.001500    0.005800    0.006700   \n",
       "25%       0.013350    0.016450    0.018950    0.024375    0.038050   \n",
       "50%       0.022800    0.030800    0.034300    0.044050    0.062500   \n",
       "75%       0.035550    0.047950    0.057950    0.064500    0.100275   \n",
       "max       0.137100    0.233900    0.305900    0.426400    0.401000   \n",
       "\n",
       "                5           6           7           8           9   ...  \\\n",
       "count   208.000000  208.000000  208.000000  208.000000  208.000000  ...   \n",
       "unique         NaN         NaN         NaN         NaN         NaN  ...   \n",
       "top            NaN         NaN         NaN         NaN         NaN  ...   \n",
       "freq           NaN         NaN         NaN         NaN         NaN  ...   \n",
       "mean      0.104570    0.121747    0.134799    0.178003    0.208259  ...   \n",
       "std       0.059105    0.061788    0.085152    0.118387    0.134416  ...   \n",
       "min       0.010200    0.003300    0.005500    0.007500    0.011300  ...   \n",
       "25%       0.067025    0.080900    0.080425    0.097025    0.111275  ...   \n",
       "50%       0.092150    0.106950    0.112100    0.152250    0.182400  ...   \n",
       "75%       0.134125    0.154000    0.169600    0.233425    0.268700  ...   \n",
       "max       0.382300    0.372900    0.459000    0.682800    0.710600  ...   \n",
       "\n",
       "                51          52          53          54          55  \\\n",
       "count   208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "unique         NaN         NaN         NaN         NaN         NaN   \n",
       "top            NaN         NaN         NaN         NaN         NaN   \n",
       "freq           NaN         NaN         NaN         NaN         NaN   \n",
       "mean      0.013420    0.010709    0.010941    0.009290    0.008222   \n",
       "std       0.009634    0.007060    0.007301    0.007088    0.005736   \n",
       "min       0.000800    0.000500    0.001000    0.000600    0.000400   \n",
       "25%       0.007275    0.005075    0.005375    0.004150    0.004400   \n",
       "50%       0.011400    0.009550    0.009300    0.007500    0.006850   \n",
       "75%       0.016725    0.014900    0.014500    0.012100    0.010575   \n",
       "max       0.070900    0.039000    0.035200    0.044700    0.039400   \n",
       "\n",
       "                56          57          58          59   60  \n",
       "count   208.000000  208.000000  208.000000  208.000000  208  \n",
       "unique         NaN         NaN         NaN         NaN    2  \n",
       "top            NaN         NaN         NaN         NaN    M  \n",
       "freq           NaN         NaN         NaN         NaN  111  \n",
       "mean      0.007820    0.007949    0.007941    0.006507  NaN  \n",
       "std       0.005785    0.006470    0.006181    0.005031  NaN  \n",
       "min       0.000300    0.000300    0.000100    0.000600  NaN  \n",
       "25%       0.003700    0.003600    0.003675    0.003100  NaN  \n",
       "50%       0.005950    0.005800    0.006400    0.005300  NaN  \n",
       "75%       0.010425    0.010350    0.010325    0.008525  NaN  \n",
       "max       0.035500    0.044000    0.036400    0.043900  NaN  \n",
       "\n",
       "[11 rows x 61 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Comprobar el número de medidas y variables, así como los tipos utilizados para los diferentes datos\n",
    "data.info(memory_usage='deep')\n",
    "\n",
    "#Comprobar los rangos de las variables \n",
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df6de63",
   "metadata": {},
   "source": [
    "Como se puede ver en la salida anterior, se dispone de las mencionadas características ordenadas en 60 columnas numéricas correspondientes a cada uno de los agregados de las longitudes de onda.  Por contra, la última columna se corresponde con una categoria o columna de datos discretos en la que se encuentra consignado un valor R cuando se trata de una una roca y una M cuando es una mina.\n",
    "Existen diferentes maneras de tratar este problema. Algunos ejemplos serían cambiar el tipo `object` de la columna por `category`, se podría hacer un *one-hot encoded* creando una columna para cada uno de las posibilidades. En este caso, se ha optado por realizar una codificación binaria en un único valor ya que se trata de un problema de clasificación binaria y con eso será suficiente. Así que se le asignará un valor de *True* o *False* dentro de una columna denominada **Mina**. En concreto, todos aquellos elementos que coincidan en la columna 60 con una M tendrán el valor de *True* y en caso contrario sera un *False*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1d72445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>Mina</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>R</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>R</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>R</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0       1       2       3       4       5       6       7       8  \\\n",
       "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "\n",
       "        9  ...      52      53      54      55      56      57      58  \\\n",
       "0  0.2111  ...  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084  0.0090   \n",
       "1  0.2872  ...  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049  0.0052   \n",
       "2  0.6194  ...  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164  0.0095   \n",
       "3  0.1264  ...  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044  0.0040   \n",
       "4  0.4459  ...  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048  0.0107   \n",
       "\n",
       "       59  60   Mina  \n",
       "0  0.0032   R  False  \n",
       "1  0.0044   R  False  \n",
       "2  0.0078   R  False  \n",
       "3  0.0117   R  False  \n",
       "4  0.0094   R  False  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Mina'] = (data[60]=='M')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215f2f37",
   "metadata": {},
   "source": [
    "El siguiente paso será el preparar los conjuntos de datos de entrada y salida. Como se ve a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7bc2594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patterns: (208, 60) -> (208,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Recoger las 60 primeras mediciones y convertirlas a un Numpy\n",
    "#no tienen nombre así que accedemos según la posición\n",
    "inputs = (data.iloc[:,0:60]).to_numpy()\n",
    "#Convertir la salida a un formato numérico y un numpy\n",
    "outputs = (data['Mina']).to_numpy().astype('int')\n",
    "\n",
    "print(f\"Patterns: {inputs.shape} -> {outputs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284eeb91",
   "metadata": {},
   "source": [
    "Es recomendable siempre que se pueda una inspección visual de los datos para determinar si existe algún tipo de patrón o elemento que vemos a simple vista. Como un espacio de 60 dimensiones no se puede visualizar se reducirá con PCA a dos dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1573281",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f05ef704fa0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_inputs = pca.fit_transform(inputs)\n",
    "\n",
    "plt.figure()\n",
    "colors = [\"darkorange\", \"lightgray\"]\n",
    "lw = 2\n",
    "\n",
    "for color, i, target_name in zip(colors, [0, 1], ['Roca', 'Mina']):\n",
    "    plt.scatter(\n",
    "        pca_inputs[outputs == i, 0], pca_inputs[outputs == i, 1], color=color, alpha=0.8, lw=lw, label=target_name\n",
    "    )\n",
    "plt.legend(loc=\"best\", shadow=False, scatterpoints=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb7ae62",
   "metadata": {},
   "source": [
    "El siguiente paso, ya que se van a comparar diferentes alternativas, sera dividir el conjunto de datos en entrenamieno y test. Para realizar este proceso, ya que se trata de un problema sencillo con sólo 2 posibles clases, lo más sencillo es extraer las matrices y dividir los conjuntos mediante la función `train_test_split`. En este caso se realizará un simple \n",
    "_hold_out_ simplemente con efectos meramente demostrativos, pero el procedimiento más adecuado hubiera sido un _cross-validation_ al ser un conjunto muy pequeño de datos. Por el mismo motivo, los pocos datos presente en lugar de realizar una división habitual 70:30 se reducirá ese montante de datos de test a solo un 10% del total. A mayores y para garantizar una cierta representatividad, la división se hará de manera estratificada, es decir, que las mismas proporciones se mantengan tanto en el entre las calses en el conjunto de test y el de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "528724ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Patterns(187, 60) -> (187,)\n",
      "Test Patterns(21, 60) -> (21,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Crear los conjuntos de entrenamiento y test\n",
    "train_inputs, test_inputs, train_outputs, test_outputs = train_test_split(inputs, outputs, test_size=0.1, stratify=outputs)\n",
    "\n",
    "#Una pequeña comprobación de programación defensiva\n",
    "print(f\"Train Patterns{train_inputs.shape} -> {train_outputs.shape}\")\n",
    "print(f\"Test Patterns{test_inputs.shape} -> {test_outputs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f65ec4c",
   "metadata": {},
   "source": [
    "Es en este punto, una vez hecha la división es cuando se podrán realizar los tratamientos de los datos. Si faltasen datos o, como es este caso, los datos no estuvieran normalizados, será en este punto donde aplicar los diferentes tratamientos.\n",
    "En concreto, como ya se vió antes, en este problema no hay datos ausentes y por lo tanto no será necesario rellenarlos ni completarlos en este caso. De haberlos las funciones más habituales para el tratamiento de los mismos son:\n",
    "\n",
    "* [sklearn.impute.SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html). Permite \"rellenar\" los datos ausentes bien con un valor o la media de los valores presentes.\n",
    "* [sklearn.impute.IterativeImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html). Esta función permite la estimación de un parámetro en función de los restantes. Resaltar que, los modelos creados para la estimación, son realizados siguiendo una estrategia *round-robin*.\n",
    "* [sklearn.impute.KNNImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html). En este caso, el objeto creado rellenará los valores en función de un KNN creado sobre los datos de entrenamiento. Se seleccionaran aquellos patrones más cercanos, y se rellenarían los datos ausentes con la media de los $k$ vecinos más cercanos.\n",
    "\n",
    "Lo que si sucede en este caso es que los datos no están normalizados. La normalización se podría realizar con los siguientes métodos:\n",
    "\n",
    "* [sklearn.preprocessing.StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). En este caso la normalización se hace haciendo uso de la media y la desviación típica de cada característica. Es la opción predilecta si la característica en cuestión tiene una distribución Gaussiana. Para ello aplica la siguiente formula:\n",
    "$$ \\frac{x_i - \\bar x}{\\sigma(x)}$$\n",
    "* [sklearn.preprocessing.MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html). Este objeto realiza la normalización dentro del rango de cada una de las características. Para ello usa el máximo y el mínimo de cada característica con la formula que se ve a continuación. Este tipo de normalización es el indicado cuando no se puede garantizar la distribución Gaussiana de los datos.\n",
    "$$ \\frac{x_i - min(x)}{max(x)-min(x)}$$\n",
    "* [sklearn.preprocessing.RobustScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html). Similar al caso anterior, esta normalización realiza la normalización usando el primer y tercer cuartil. Es indicado cuando pueden existir outlayers que afectarían a la normalización\n",
    "$$ \\frac{x_i - Q_1(x)}{Q_3(x)-Q_1(x)}$$\n",
    "* [sklearn.preprocessing.MaxAbsScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html). Este objeto divide cada uno de las características por el máximo en valor absoluto, de tal manera que ese valor sera 1, pero no se modificará el centroide de la distribución de la característica.\n",
    "$$ \\frac{x_i}{max(|x|)}$$\n",
    "* [sklearn.preprocessing.Normalizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html). En este caso, la normalización se aplica al patrón y no a las características. De tal forma que se aplicaría la siguiente normalización.\n",
    "$$\\frac{x_{ij}}{\\sqrt{\\sum_{j}x_{ij} ^2}}$$\n",
    "En este caso se va a realizar dicha normalización entre los valores mínimo y máximo como se ve a continuación:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96a62e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Crear el método para la normalización de los datos\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Ajustarlo y hacer la transformación de los datos de entrenamiento\n",
    "train_inputs = scaler.fit_transform(train_inputs)\n",
    "\n",
    "# Con el objeto ajustado aplicar la transformación a los datos de entrada de test\n",
    "test_inputs = scaler.transform(test_inputs)\n",
    "\n",
    "#Comprobar la conversión imprimiendo los rangos\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f12a39f",
   "metadata": {},
   "source": [
    "## Estableciendo la línea base\n",
    "\n",
    "Como se ha comentado anteriormente, los ensemble son un conjunto de clasificadores más \"débiles\" que nos permiten posteriormente superar los límites de estos mediante su unión. Es por ello que, antes de comenzar con los ensemble, será preciso disponer de algunos modelos de referencia que se unirán posteriormente en un metaclasificador. En el ejemplo siguiente se entrenan algunos modelos sencillos como son: un SVM con kernel RBF, una Regresión Lineal, un Naïve Bayes y un Árbol de Decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d32b8b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: 85.7143%\n",
      "LR: 85.7143%\n",
      "DT: 76.1905%\n",
      "NB: 61.9048%\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "\n",
    "#Definir los clasificadores\n",
    "clfs = { 'SVM': svm.SVC(probability=True), \n",
    "         'LR': LogisticRegression(),\n",
    "         'DT': DecisionTreeClassifier(max_depth=4),\n",
    "         'NB':GaussianNB()}\n",
    "\n",
    "base_models = [name for name in clfs.keys()]\n",
    "\n",
    "# Realizar el entrenamiento de cada modelo y calcualr su valor de test (accuracy)\n",
    "for key in clfs.keys():\n",
    "    clfs[key].fit(train_inputs, train_outputs)\n",
    "    acc = clfs[key].score(test_inputs, test_outputs)\n",
    "    print(f\"{key}: {(acc*100):.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8018921",
   "metadata": {},
   "source": [
    "#### Ejercicio\n",
    "Amplíe el conjunto de modelo disponibles por ejemplo introduciendo variantes de SVM, otras aproximaciones como por ejemplo, K-NN, Kmeans (cuidado con este caso con la función `score`) o un MLP sencillo. Al menos otros 4 evitando introducir el RandomForest, AdaBoost o cualquier otro modelo que implique ela composición de modelos per sé, lo cuales se verán más adelante. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfa3338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9724b809",
   "metadata": {},
   "source": [
    "Como se puede ver hay métodos que dan un buen resultado de ajuste desde el comienzo, si bien otros es posible que necesiten un mayor ajuste de los hiperparámetros del modelo. Todos estos modelos es lo que se consideran modelos simples o *'weak models'*, en las técnicas ensemble lo que se busca es combinar varios de estos modelos con el fin de mejorar el rendimiento global. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6998695",
   "metadata": {},
   "source": [
    "# Combinando Modelos\n",
    "A la hora de combinar los modelos existen diferentes estrategias según la tarea del modelo, es decir, si estamos clasificando o haciendo una regresión. En este caso nos vamos a centrar en la classificación, si bien para la regresión sería similar, habría que tener en cuenta el caracter contínuo de los valores a la hora de combinar las salidas.\n",
    "\n",
    "En cuanto a la combinación de la classificación, existen principalmente dos maneras de combinar las salidas de varios clasificadores. Estas combinaciones reciben el nombre de Voto mayoritario y Voto mayoritario con pesos  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adf0311",
   "metadata": {},
   "source": [
    "### Voto Mayoritario\n",
    "Sin bien también se conoce como _Hard Voting_, como su nombre indica se basan en seleccionar la opción más votada entre las predichas entre los diferentes modelos. La implementación disponible en `scikit learn` realiza una suma de las predicciones para cada una de las clases y, posteriormente, saca la media dichas estimaciones. La opción seleccionada por mayoría entre los \"expertos\" de los cuales consta el *emsemble* es la seleccionada. Así, se podría resolver el problema teniendo en cuenta diferentes resultados o puntos de vista sobre el mismo. Véase un ejemplo en el código siguiente de construcción de un modelo como este."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fb54fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: 85.7143%\n",
      "LR: 85.7143%\n",
      "DT: 76.1905%\n",
      "NB: 61.9048%\n",
      "Ensemble (Hard Voting): 80.9524%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "#Definir el metaclasificador con los clasificadores definidos previamente.\n",
    "clfs['Ensemble (Hard Voting)'] = VotingClassifier (estimators = [(name,clfs[name]) for name in base_models], \n",
    "                                                   n_jobs=-1)\n",
    "clfs['Ensemble (Hard Voting)'].fit(train_inputs, train_outputs)\n",
    "\n",
    "for key in clfs.keys():\n",
    "    acc = clfs[key].score(test_inputs, test_outputs)\n",
    "    print(f\"{key}: {(acc*100):.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afdba35",
   "metadata": {},
   "source": [
    "Como se puede ver, si bien no mejora al mejor de los modelos que lo componen, esto se debe a, en primer lugar, que este no es un problema especialmente complejo. A mayores otro de los problemas es que nos fiamos por igual de todos los modelos a la hora de decidir la clase de respuesta. Para solucionar este problema se puede hacer que no todos los modelos tengan la misma importancia como veremos en el siguiente partado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd9129e",
   "metadata": {},
   "source": [
    "### Voto Mayoritario con Pesos\n",
    "Como se indicó en el paso anterior, uno de los problemas del modelo clásico de *emsemble* es que todos los resultados pesan lo mismo y en cada uno de los modelos \"débiles\" sólo se contempla la opción más votada. Para solucionar esto una de las propuestas es el uso de de un peso en las ponderaciones de las decisiones. Esto se debe que un modelo puede ser mejor que otro o ser más confiable. Con el fin de reflejar este punto, se puede modificar dicha salida multiplicandola por un factor de confianza dentro de la regla utilizada para tomar las decisiones. Este procedimiento de ponderación en ocasiones también se denomina *Soft Voting* con el fin de contraponerlo al *Hard Voting* o no ponderado. Imagine que a cada uno de los clasificadores le asignamos el mismo peso, es decir {1,1,1}. En un ejemplo como el siguiente con un SVM, una regresión Logarítmica y un modelo basado en Bayes tendríamos las siguientes salidas.\n",
    "\n",
    "|Classificador\t     |Mina\t        |Roca          |\n",
    "| :------------- | :----------: | -----------: |\n",
    "|SVM         \t | 0.9\t    | 0.1      |\t\n",
    "|LR         \t | 0.3\t    | 0.7      |\t\n",
    "|NB         \t | 0.2\t    | 0.8      |\n",
    "|Soft Voting      |0.47\t        |0.63          |\t\n",
    "\n",
    "De esta manera la clase seleccionada sería la de Roca ya que todos los modelos pesan lo mismo en la toma de la decisión al realizar la media. En contra posición, si sabemos que uno de los modelos es mejor se puede ponderar la respuesta de dicho modelo. Imagine en el ejemplo anterior que se supiera que el ejemplo anterior, SVM suele ser mucho mejor que los otros dos para este problema en concreto. En ese caso, se puede inclementar su peso como se ve a continuación con el fin de tener dicho ejemplo más en cuenta. Con el mismo ejemplo pero, hacido que la respuesta de SVM sea mayor, los resultados serían:\n",
    "\n",
    "|Classificador\t     |Mina\t        |Roca          |\n",
    "| :------------- | :----------: | -----------: |\n",
    "|SVM         \t |2 * 0.9\t    |2 * 0.1      |\t\n",
    "|LR         \t |1 * 0.3\t    |1 * 0.7      |\t\n",
    "|NB         \t |1 * 0.2\t    |1 * 0.8      |\n",
    "|Soft Voting      |0.575\t        |0.425          |\n",
    "\n",
    "Como se pude ver en los resultados, si tenemos un modelo de más calidad, las salidas de este se tendrán más en cuenta en cuanto a tomar la decisión correspondiente.\n",
    "\n",
    "Para implementar este tipo de comportamiento se puede hacer simplemente añadiendo dos parámetros adicionales al a función `VotingClassifier` que se había usado previamente para que pondere la salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e4ac351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: 85.7143%\n",
      "LR: 85.7143%\n",
      "DT: 76.1905%\n",
      "NB: 61.9048%\n",
      "Ensemble (Hard Voting): 80.9524%\n",
      "Ensemble (Soft Voting): 80.9524%\n"
     ]
    }
   ],
   "source": [
    "clfs['Ensemble (Soft Voting)'] = VotingClassifier (estimators = [(name,clfs[name]) for name in base_models], \n",
    "                                                   n_jobs=-1, voting='soft',weights=[1,2,2,1])\n",
    "clfs['Ensemble (Soft Voting)'].fit(train_inputs, train_outputs)\n",
    "\n",
    "for key in clfs.keys():\n",
    "    acc = clfs[key].score(test_inputs,test_outputs)\n",
    "    print(f\"{key}: {(acc*100):.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c66c97",
   "metadata": {},
   "source": [
    "Como se puede ver, los resultados son mejores cuando se combinan varios modelos que dan buenos resultados. De hecho, este procedimiento es la base de otras técnicas como son los *Random Forest* que veremos un poco más adelante en este tutorial. Los modelos a usar son la otra de las claves para la creación _ensemble_, en la siguiente sección veremos las estrategias más habituales para la creación de los modelos.\n",
    "\n",
    "El ajuste de esos pesos puede hacerse de muchas maneras diferentes, por ejemplo, se puede hacer de manera manual como hemos hecho en el ejemplo anterior. Otra alternativa sería usar alguna técnica de gradiente descencente para ir ajustandolos como si se tratara de una red neuronal o un SVM. Otra posibilidad es usar el valor de ajuste sobre el conjunto de validación (en este caso no se ha reservado un conjunto de datos para tal fin) como peso de los modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96916803",
   "metadata": {},
   "source": [
    "#### Ejercicio\n",
    "Realice un ensemble diferente con otros modelos sobre el mismo conjunto de datos y ajuste los pesos con el fin de obtener un buen resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693914ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e1e89a0",
   "metadata": {},
   "source": [
    "## Creación de modelos\n",
    "\n",
    "Uno de los elementos clave que todavía no se ha aborada es la crecación de los modelos que compondrán el metaclasificador. Hasta el momento, la aproximación que se ha seguido no es demasiado adecuada ya que el conjunto de datos de entrada de todos los modelos es el mismo. Esto tiene el efecto de una evidente falta de diversidad en los modelos ya que sea cual sea el modelo que creemos, este tendrá la misma información o \"punto de vista\" que los otros. Sin embargo, esto no es la práctica habitual si no que el conjunto de patrones de entrada se suele repartir en conjuntos más pequeños con los que entrenar una o varias técnicas con el fin de, por un lado, reducir el coste computacional y, por el otro, aumentar la divversidad en los modelos. Es necesario recordar en este punto que los modelos \"débiles\" no tienen que ser perfectos en todas las clases ni tan siquiera tienen porque contemplar todas las posibilidades sólo se necesitan modelos rápidos de entrenar y que ofrezcan una salida más o menos consistente.\n",
    "\n",
    "En cuanto a la manera en al que repartir los datos para la creación de los modelos, la mayor parte de las aproximaciones suele contemplar dos aproximaciones principalmente conocidas como *Bagging* y *Boosting*. A continuación, sse describirán brevemente estas dos aproximaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbea3e6",
   "metadata": {},
   "source": [
    "### Bagging o boostrap aggregation\n",
    "La técnica conocida como _Bagging_ o selección con remplazo fue propuesta por Breitman en 1996. Se basa en el desarrollo de múltiples modelos los cuales se pueden entrenar en paralelo. El elemento clave de esos modelos es que, cada uno de los modelos, se entrena sobre un subconjunto del conjunto de entrenamiento. Este subconjunto de datos se extrae de manera aleatoria con remplazo. Este último punto es particularmente importante ya que una vez que un ejemplo ha sido seleccionado de las posibilidades, se coloca nuevamente entre las posibilidades para poder ser seleccionado ya sea en el subconjunto que se está construyendo, o en los subconjuntos de los otros modelos, o lo que es lo mismo se crean conjuntos no disjuntos de ejemplos.\n",
    "\n",
    "<img \n",
    "    style=\"display: block; \n",
    "           margin-left: auto;\n",
    "           margin-right: auto;\n",
    "           width: 50%;\"\n",
    "    src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Ensemble_Bagging.svg/440px-Ensemble_Bagging.svg.png\" \n",
    "    alt=\"Ejemplo de Bagging\">\n",
    "</img>\n",
    "\n",
    "Lo que provoca es que se crean \"expertos\" en datos especializados y dependiendo de la partición. Si bien los datos comunes, o más frecuentes, son correctamente contemplados por todos los modelos, también es cierto que los datos con menor frecuencia tienden a no estar en todas las particiones y pueden no ser contemplados en todos los casos. Así, se obtendrían modelos que estarían más especializados en determinados datos o que tienen un punto de vista diferente, que serían expertos en una región particular del espacio de búsqueda.\n",
    "\n",
    "Si bien se hablará un poco más adelante en más detalle, una técnica muy conocida que usa está aproximación para la construcción de sus modelos \"débiles\" es RandomForest. Está construye los arboles de decision que componen el metaclasificador de esta manera. \n",
    "\n",
    "A continuación, se puede ver un ejemplo de implementación de un metaclasificador que utiliza está técnica pero con SVM como clasificador base.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02932d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: 85.7143%\n",
      "LR: 85.7143%\n",
      "DT: 76.1905%\n",
      "NB: 61.9048%\n",
      "Ensemble (Hard Voting): 80.9524%\n",
      "Ensemble (Soft Voting): 80.9524%\n",
      "Bagging (SVC): 85.7143%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "clfs['Bagging (SVC)'] = BaggingClassifier(base_estimator=SVC(),n_estimators=10, max_samples=0.50, n_jobs=-1)\n",
    "clfs['Bagging (SVC)'].fit(train_inputs, train_outputs)\n",
    "\n",
    "for key in clfs.keys():\n",
    "    acc = clfs[key].score(test_inputs,test_outputs)\n",
    "    print(f\"{key}: {(acc*100):.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15213823",
   "metadata": {},
   "source": [
    "Se puede usar cualquier classificador como base de un *Bagging* con la la clase [BaggingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html). En este caso, en el código, se ha optado por generar 10 modelos de de la clase `SVC`entrenados sólo sobre el 50% de los patrones de entrenamiento. \n",
    "\n",
    "Alternativamente a extraer ejemplos completos, se podría realizar una partición en vertical del _dataset_ de entrenameiento, extrayendo por tanto carácterísticas. Para implementar está alternativa, en la función `BaggingClassifier`se debe de definir el parámetro *max_features*. Esta aproximación se usa cuando el número de características es muy elevado para crear modelos más simples que no usen toda la información que en muchas ocasisones es redundante. Debe de destacarse que este procedimiento de extracción de caracterísitcas para los modelos se hace sin remplazo, es decir, las características sacadas para un clasificador no se vuelven a introducir en la lista de posibilidades hasta crear el conjunto para el siguiente clasificador."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9b4526",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "La otra gran familia de técnicas para la creación de metamodelos de ensemble es lo que se conoce como *Boosting*. En este caso, la aproximación es ligeramente distinta, ya que lo que se pretende es crear una cadena de clasificadores. EL elemento clave de este tipo de clasificadores es buscar que, cada nuevo clasificador, este más especializado en los patrones que los modelos previos han errado. Por tanto, al igual que en el caso anterior, se selecciona un subconjunto de patrones del conjunto origianal. Sin embargo, este proceso se hace de manera secuencial y sin remplazo. Este punto es crucial ya que como se comentó la idea es eliminar aquellos patrones que están ya correctamente clasificadose e ir obtenido modelos más específicos que se concentran en aquellos ejemplos menos frecuentes o que han sido clasificados de manera incorrecta en un paso anterior. Por tanto, al igual que en el *Bagging*, la idea subyacente de esta aproximación es que no todos los modelos tengan que tener todos los patrones como base, pero a diferencia de _Bagging_, este proceso es lineal por la dependencia en la construcción de los modelos. \n",
    "\n",
    "<img \n",
    "    style=\"display: block; \n",
    "           margin-left: auto;\n",
    "           margin-right: auto;\n",
    "           width: 50%;\"\n",
    "    src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Ensemble_Boosting.svg/1920px-Ensemble_Boosting.svg.png\" \n",
    "    alt=\"Ejemplo de Boosting\">\n",
    "</img>\n",
    "\n",
    "Posteriormente, para obtener la combinación de los modelos, se hace uso del Voto Mayoritario con pesos. En dicha aproximación, los pesos es establecen con un sistema de aproximación iterativa. Existen multitud de ejemplos que utilizan este tipo de técnica como [AdaBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) o [Gracient Tree Boosting](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html). En ambos casos lo que se realiza es un ajuste de los pesos con una técnica basada en el Gradiente Descendente. \n",
    "\n",
    "En el caso del AdaBoost, el algoritmo parte dandole un peso a todas las instancias del conjunto de entrenamiento. Con ese conjunto ponderado, se entrena un clasificador con esos datos originales. En función de los errores comentidos se ajustan los pesos del conjunto original y se entrena una nueva copía del clasificador, pero ya sobre esos datos ajustados que se centrarán más en las instancias que han sido clasificadas de manera incorrecta. En el caso de `scikit learn`, el algoritmo implementado es el conocido como [AdaBoost-SAMME](https://hastie.su.domains/Papers/SII-2-3-A8-Zhu.pdf) propuesto por Zhu et.al en 2009. Como particularidad de está implementación, comentar que la función de *loss* utilizada es una exponencial. Esta es la que se utilizará para calcular la ponderación de los errores cometidos, así como el peso de los clasificadores en el metaclasificador. En terminos generales, la salida será la más votada por los clasificadores en base a la ponderación de cada uno de ellos. \n",
    "\n",
    "Por su parte, el Gradiente Tree Boosting es una aproximación diferente del uso del _Boosting_. Ésta construye un árbol en donde los nodos del árbol establecen los criterios de, por ejemplo,  en el caso de la clasificación hacen referencia al `logistic-likelihood` de un determinado patrón. De esa manera, cada una de los nodos del arbol se realiza una clasificación la cual se va ajustando en base a los errores residuales que se van cometiendo ajustando los pesos de los diferentes clasificadores del árbol. Esta división se realiza para cada una de las características de que se dispone, realizando un procedimiento recursivo entrenando varios clasificadores de esta manera. Posteriormente, para hacer la toma de decisión, ésta se basa en las respuestas de los clasificadores por los que ha ido pasando. La principal diferencia con el AdaBoost es que en este caso la salida son las probabilidades de las clases las cuales se suman para dar la respuesta más probable en lugar de que sea la respuesta sobre la instancias.\n",
    "\n",
    "A continuación, vemos una aproximación con un  se puede ver un ejemplo de uso de estos dos metaclasficadores que hacen uso de _Boosting_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7f514c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: 85.7143%\n",
      "LR: 85.7143%\n",
      "DT: 76.1905%\n",
      "NB: 61.9048%\n",
      "Ensemble (Hard Voting): 80.9524%\n",
      "Ensemble (Soft Voting): 80.9524%\n",
      "Bagging (SVC): 85.7143%\n",
      "Ada: 80.9524%\n",
      "GTB: 80.9524%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "clfs['Ada'] = AdaBoostClassifier(n_estimators=30)\n",
    "clfs['Ada'].fit(train_inputs, train_outputs)\n",
    "\n",
    "clfs['GTB'] = GradientBoostingClassifier(n_estimators=30, learning_rate=1.0, max_depth=2, random_state=0)\n",
    "clfs['GTB'].fit(train_inputs, train_outputs)\n",
    "\n",
    "for key in clfs.keys():\n",
    "    acc = clfs[key].score(test_inputs,test_outputs)\n",
    "    print(f\"{key}: {(acc*100):.4f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cdb756",
   "metadata": {},
   "source": [
    "#### Ejercicio\n",
    "Repita el ejercicio anterior pero cambiando el classificador base de `AdaBoostClassifier` y en el caso del `GradientBoostingClassifier`, si bien este no se puede cambiar el clasificador base, consulte la documentación y cambie los parámetros necesarios para que al menos haya 4 patrones en cada uno de los clasificadores terminales del árbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6c049c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3316a789",
   "metadata": {},
   "source": [
    "## Técnicas que integran la aproximación _Ensemble_\n",
    "\n",
    "Algunos de los algoritmos más conocidos y usados actualmente se basan en este tipo de aproximación. De entre esas aproximaciones, puede que las más famosas y utilizadas son aquellas que toman como base la generación de sencillos árboles de decisión (_Decision Tree, DT_). La razón del uso de los árboles es su fácil interpretación, así como la rapidez en el cálculo y entrenamiento. A continuación se verán las dos aproximaciones conocidas a día de hoy en este sentido, ***Random Forest*** y ***XGBoost***.\n",
    "\n",
    "\n",
    "### Random Forest\n",
    "Este algoritmo, propuesto por Breitman y Cutler en 2006 a instancias de una publicación anterior de Ho de 1995 (_Random Subspaces_), es el paradigma de técnica de ensemble. El algoritmo une en un ensemble un conjunto de clasificadores sencillos que toman la forma de *Decision Trees*. Estos clasificadores son entrenados siguiendo una aproximación de *bagging*, y por lo tanto se pueden entrenar cada uno de forma paralela. Para combinar la salidad de los algoritmos se hace para los problemas de clasificación mendiante la opción más votada entre los \"expertos\" o, si es un problema de regresión, mediante la media aritmética de las respuestas. \n",
    "\n",
    "Es un algoritmo que necesita el ajuste de muy pocos hiperparámetros para obtener muy buenos resultados en casi cualquier tipo de problema. En general, el valor más importante es el número de estimadores y por tanto el número de particiones que se va a hacer del conjunto de entrenamiento. Varios autores apuntan que ese número de estimadores debiera de ser *$\\sqrt{\\textrm{número características}}$* para problemas de clasicación, y *$\\frac{\\textrm{número características}}{3}$ para problemas de regresión. Aun así, también apunta que la técnica saturaría entre 500 y 1000 árboles y por mucho que se aumente no mejoraría resultados. Si bien este último dato solo ha sido probado de manera empírica en determinados conjuntos de datos y por lo tanto debe de ser tomado con cuidado al no tener una justificación matemática.\n",
    "\n",
    "A mayores del proceso habitual de *bagging*, los *Random Forest* también incluyen un segundo mecanismo de división. Una vez seleccionados los patrones que formaran parte del conjunto de entrenamiento del árbol de decisión, solamente un subconjunto de características (*features*) aleatorias están disponibles para cada nodo del árbol. Esto hace crecer la diversidad de los árboles del bosque y lo que consigue es centrarse en el rendimiento global con una pequeña varianza en los resultados. Este mecanísmo permite evaluar cuantitativamente el rendimiento individual de cada árbol que forma parte del bosque y sus variables. Por tanto, se puede medir la importancia de cada variable. Está medida que calibra la participación de cada variable en nodos del árbol en la toma de decisiones se denomina impureza y viene a medir la diferencia entre las diferentes ramas del árbol cuando se hace la partición de los ejemplos. En ocasiones, esta misma medida es utilizada a modo de medida para la selección de variables tomando la medida en todos los árboles del bosque de la participación e importancia mediante un filtrado como los vistos en la unidad anterior.\n",
    "\n",
    "Para el calculo de esa medida de impureza, existen diferentes aproximaciones. Por ejemplo, `scikit learn` utiliza una medida que denomina **Gini**. Esta última es la probabilidad de clasificar incorrectamente un elemento elegido al azar en el conjunto de datos si se etiquetara aleatoriamente según la distribución de clases en el conjunto de datos. Se calcula como:\n",
    "$$G = \\sum_{i=1}^C p(i) * (1 - p(i))$$\n",
    "\n",
    "siendo $C$ el número de clases y $p(i)$ la probabilidad de seleccionar al azar un elemento de la clase $i$. Se puede ver un buen ejemplo de como calcular la impureza de las ramas en el siguiente [enlace](https://victorzhou.com/blog/gini-impurity/)\n",
    "\n",
    "A continuación, sobre el ejemplo que se viene utilizando en esta unidad, se ejecutará un modelo de *Random Forest* con la implementación de `scikit learn`. Destacar como parámetros más importante de dicha implementación:\n",
    "\n",
    "- ***n_estimator***, que marca el número de árboles que se van a generar o el número de particiones de *bagging*.\n",
    "- ***criterion***, medida de impureza de los nodos. Por defecto se usa Gini aunque puede cambiarse por la entropía ganada.\n",
    "- ***max_depth***, permite limitar la profundidad máxima de los árboles para así limitar el número de variables a usar.\n",
    "- ***min_sample_split***, para cada árbol de decisión, cuantos patrones son necesario para realizar una división interna en los *Decision Trees*.\n",
    "- ***bootstrap***, puede utilizar la aproximación de *bagging* o *bootstrap* para construir los árboles pero si esta propiedad es falsa, entonces usa todo el conjunto de entrenamiento para generar los árboles. En caso de tener un valor True, se tienen en cuenta las siguientes propiedades:\n",
    "    + ***max_samples***, número de ejemplos a extraer del conjunto original para construir el conjunto de entrenamiento del estimador, el valor por defecto es igual al número de patrones pero recuerde que puede extraerse varias veces el mismo ya que es una selección con remplazo dando variabilidad.\n",
    "    + ***oob_score***, medida de *out of bag* para estimar la generalización. Aquellas muestras que no han formado parte del entrenamiento de un estimador se pueden usar para calcular una medida de validación y, promediarla entre todos los estimadores para saber como de general es el bosque construído.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9983c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: 85.7143%\n",
      "LR: 85.7143%\n",
      "DT: 76.1905%\n",
      "NB: 61.9048%\n",
      "Ensemble (Hard Voting): 80.9524%\n",
      "Ensemble (Soft Voting): 80.9524%\n",
      "Bagging (SVC): 85.7143%\n",
      "Ada: 80.9524%\n",
      "GTB: 80.9524%\n",
      "RF: 76.1905%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clfs['RF'] = RandomForestClassifier(n_estimators=8, max_depth=None,\n",
    "                                    min_samples_split=2, n_jobs=-1)\n",
    "\n",
    "clfs['RF'].fit(train_inputs, train_outputs)\n",
    "for key in clfs.keys():\n",
    "    acc = clfs[key].score(test_inputs,test_outputs)\n",
    "    print(f\"{key}: {(acc*100):.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e62b37c",
   "metadata": {},
   "source": [
    "En esta aproximación se han definido el número de estimadores siguiendo la regla antes indicada de $\\sqrt{\\textrm{número características}}$. En este caso como son pocos estimadores y pocos patrones los resultados pueden variar bastante en función del tipo de particiones obtenidas.\n",
    "\n",
    "A continuación, una vez entrenado el modelo, se puede comporobar el nivel de impureza obtenido para cada una de las frecuencias que se ha calculado con el el algoritmo Gini, como una media de las obtenidas entre los arboles que componen el bosque. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40548bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Valores de importancia de las características ')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#for name, score in enumerate(clfs['RF'].feature_importances_):\n",
    "#    print(f\"Feature {name+1} score: {score:.4f}\")\n",
    "    \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"]=10,10\n",
    "plt.barh(y=range(1,61), width = clfs['RF'].feature_importances_)\n",
    "plt.xlabel(\"Gini Gain\")\n",
    "plt.ylabel(\"Característica\")\n",
    "plt.title(\"Importancia de las características \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aa2495",
   "metadata": {},
   "source": [
    "A puntar que , como se ve en el gráfico, este valor determina que la mayor parte de la información se concentra en algunas de las frecuencias utilizadas. Es por ello, que como se comento antes, se podría realizar un filtrado de la información como el visto en la unidad anterior en base a este valor. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aba0059",
   "metadata": {},
   "source": [
    "### XGBoost (eXtreme Gradient Boosting)\n",
    "\n",
    "Finalmente, en este último apartado destacar nuevamente el Gradient Boosting, en concreto, una implementación que en los últimos años se ha hecho muy famosa por su versatilidad y rapidez. Esta implementación que se conoce como ***XGBoost (eXtreme Gradient Boosting)*** , que ha destacado sobre todo en competiciones como en la plataforma Kaggle por su rápidez en la obtención de resultados y robustez de los mismos. \n",
    "\n",
    "El ***XGBoost*** será un ensemble similar al de los Random Forest pero utiliza un clasificador base diferente conocido como CART (classification and regression trees) en lugar de *Decision Trees*. Este cambio viene de la mano de la necesidad del algoritmo de obtener la  probabilidad de las decisiones, al igual que ocurría con el *Gradient Tree Boosting*. El otro de los cambios fundamentales de este algotimo es, ya que está basado en el * Gradiente Tree Boosting*, es el cambio de la estrategia de *bagging* por la de *boosting* para la creación de los conjuntos de entrenamiento de los clasificadores.\n",
    "\n",
    "Posteriormente, está técnica realiza una aproximación de entrenamiento aditivo cuyos pesos se van ajustanto en base a un **Gradiente Descendente** sobre una función de *loss* a definir. Sumando la función de *loss* con el término de regularización, se puede calcular hasta la segunda derivada de las funciones con el fin de actualizar los pesos de la clasificación realizado por los diferentes árboles. El cálculo de este gradiente, permite por lo tanto el ajuste de los valores de los clasificadores que se generan a continuación de uno dado con el fin de que los pesos permitan focalizar la atención en los patrones que incorrectamente clasificados. Los detalles matemáticos de la implementación se pueden consultar en este [enlace](https://xgboost.readthedocs.io/en/stable/tutorials/model.html).\n",
    "\n",
    "Al diferencia del resto de aproximaciones que hemos visto, el `xgboost` no se encuentra actualmente implementado en `scikit learn`. POr este motivo, se deberá de instalar la versión de referencia si no está ya presente en la máquina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c63cff40",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import xgboost as xgb\n",
    "except ModuleNotFoundError:\n",
    "    !pip install xgboost\n",
    "    import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40e7ad9",
   "metadata": {},
   "source": [
    "Tras esa instalación, se podría hacer uso de la librería como se ve en el ejemplo siguiente. En primer lugar, para hace uso de está librería es necesario hacer una adapción de los datos de entrada al formato [LIBSVM](https://xgboost.readthedocs.io/en/stable/tutorials/input_format.html). Existen varias formas de cargar los datos desde numpy, scipy o pandas, para mayores detalles sobre este punto y para aplicarlo a diferentes problemas, se pude consultar el siguiente enlace [enlace](https://xgboost.readthedocs.io/en/stable/python/python_intro.html). En este caso concreto, el ejemplo está almacenado en un array de `numpy` con lo que para la transformación de los datos bastaría con:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d52ece53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparar las matrices para usarlas con el formato LIBSVM\n",
    "dtrain = xgb.DMatrix(train_inputs, label=train_outputs)\n",
    "dtest = xgb.DMatrix(test_inputs, label=test_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6d9169",
   "metadata": {},
   "source": [
    "Una vezz realizada está adaptación de los datos, se puede proceder con el entrenamiento de un modelo de la librería `xgboost`. Para ello sólo hará falta llamar a la función train con los parámetros correspondientes. Dentro de estos parámetros destacan:\n",
    "\n",
    "- **eta**, término que determinará la compresión de los pesos tras cada nueva etapa del *boosting*. Toma valores entre 0 y 1.\n",
    "- **max_depth**, profundidad máxima de los árboles tiene por defecto un valor de 6 incrementarlo lo que hará será permitir modelos más complejos\n",
    "- **gamma**, parámetro que controla la reducción mínima de pérdidas necesaria para realizar una nueva partición en un nodo hoja del árbol. Cuanto mayor sea será más conservador\n",
    "- **alpha** y **lambda**, son los parámetros que controlan la regulación L1 y L2 respectivamente.\n",
    "- **objective**, establece la fución de loss a ser utilizada que puede ser una de las predefinidas, las cuáles se pueden consultar en este [enlace](https://xgboost.readthedocs.io/en/stable/parameter.html#parameters-for-tree-booster)\n",
    "\n",
    "A mayores solo es necesario establecer el número máximo de iteraciones del proceso de boosting como se ve en el siguiente ejemplo con 40 rondas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c791640f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:40:27] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "# Especificar los parámetros del modelo\n",
    "param = {'max_depth':2, 'eta':1, 'objective':'binary:logistic' }\n",
    "num_round = 40\n",
    "\n",
    "# entrenar el modelo correspondiente\n",
    "xgb_model = xgb.train(param, dtrain, num_round)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de6f078",
   "metadata": {},
   "source": [
    "***NOTA***\n",
    "\n",
    "En caso de usarse un conjunto de validación, este debe de pasarse en el parámetro *evals* de la función de entrenamiento. A mayores, y sólo cuando el mencionado parámetro *evals* está definido, se puede establecer las rondas para la parada previa con el parámetro *early_stopping_rounds* de la función de entrenamiento. El código sería similar a:\n",
    "``` python\n",
    "    evals = xgb.DMatrix(val_inputs, label=val_outputs)\n",
    "    xgb_model = xgb.train(param, dtrain, num_round, evals=evals, early_stopping_rounds=10)\n",
    "```\n",
    "\n",
    "El valor proporcionado en la salida se corresponde con la suma de las salidas de los árboles, estándo está entre 0 y 1 apra la pertenencia a una determinada clase. Como se trata de un es una clase binaria, simplemente se establece un límite de 0.5 a la salida para determinar cual es la respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "12f86156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.4760702e-01 5.1304615e-01 9.5305294e-01 3.7761524e-04 9.9571759e-01\n",
      " 1.2167676e-01 5.5907309e-01 9.9939620e-01 5.7567203e-01 1.8412283e-02\n",
      " 8.8869894e-01 9.0951115e-01 9.9960881e-01 1.4012090e-02 9.9484211e-01\n",
      " 5.5841553e-01 9.9324566e-01 9.9944490e-01 9.9865578e-02 2.3036277e-02\n",
      " 9.9296373e-01]\n",
      "XGB: 80.9524%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(f\"{xgb_model.predict(dtest)}\")\n",
    "\n",
    "# Evaluar la salida\n",
    "acc = accuracy_score(xgb_model.predict(dtest)>0.5,test_outputs)\n",
    "print(f\"XGB: {(acc*100):.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6151b99",
   "metadata": {},
   "source": [
    "Finalmente, al igual que en el caso de los *Random Forest* es posible identidicar la importancia y pintarla para cada una de las variables en la clasificación. Con el siguiente código se pude ver dicho marcador ordenado decendentemente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a75d0ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Feature importance'}, xlabel='F score', ylabel='Features'>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAApoAAAJcCAYAAABZvQRWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAABWxElEQVR4nO39fbhddX3n/z9fAiIGb6ohSo0YKE4Ewo2AUEZMEws1SjrYllEZvImo1OKITqUjnVbstN9Otb9S5fvTXpYKpfUG2wFhHMEUZvBoq+LITZQ741g9LUFRoCKcNGoS3t8/9gpujyfJScjaa2Wf5+O69sXee3322u/T97XrO2vvtV6pKiRJkqRd7TFdFyBJkqTx5KApSZKkVjhoSpIkqRUOmpIkSWqFg6YkSZJa4aApSZKkVjhoSpoTkvyXJB/sug5JmkvidTQlbU+SSeBpwOahp/9NVX3rUe7z9VX1vx5ddbufJL8HHFxVr+y6Fklqk0c0Jc3WL1fVvkO3nR4yd4Uke3b5/jtrd61bknaGg6aknZbkSUkuTvLtJHcn+X+S7NFs+7kk1ye5P8l9ST6S5MnNtg8BBwD/M8lUkv+cZFmSddP2P5nkpOb+7yW5PMmHkzwIrNrW+89Q6+8l+XBzf1GSSvLaJHcl+V6SNyZ5XpKvJHkgyfuGXrsqyeeSvC/J95N8NckvDm3/2SSfSPIvSb6e5A3T3ne47jcC/wV4efO3f7lZ99okdyZ5KMk3kvz60D6WJVmX5G1Jvtv8va8d2r5PkguS/FNT3z8k2afZ9vNJPt/8TV9OsmwnWi1JO8VBU9KjcSmwCTgYeC7wS8Drm20B/gj4WeAQ4JnA7wFU1auAf+bHR0n/eJbvdypwOfBk4CPbef/ZOB54NvBy4L3A7wAnAYcBL0vyC9PW/iMwH3gn8PEkT2m2fQxY1/ytpwH/LckLt1L3xcB/A/6m+duPbNZ8F1gJPBF4LfCeJEcP7ePpwJOAZwCvA96f5GeabX8CHAP8W+ApwH8GHk7yDOBq4P9pnj8XuCLJfjvwfyNJ2mkOmpJm66rmqNgDSa5K8jTgJcBbq2p9VX0XeA/wCoCq+npVXVdVP6yqe4E/BX5h67uflS9U1VVV9TCDgWyr7z9Lf1BVP6iqa4H1wGVV9d2quhv4ewbD6xbfBd5bVRur6m+AtcApSZ4JPB94e7OvNcAHgVfPVHdVbZipkKq6uqr+sQY+A1wLvGBoyUbg95v3vwaYAhYneQxwJvCWqrq7qjZX1eer6ofAK4Frquqa5r2vA25s/u8mSa3zt0KSZuulwyfuJDkO2Av4dpItTz8GuKvZ/jTgQgbD0hOabd97lDXcNXT/Wdt6/1n6ztD9DTM83nfo8d31k2dP/hODI5g/C/xLVT00bduxW6l7RklezOBI6b9h8Hc8Hrh1aMn9VbVp6PG/NvXNBx7H4GjrdM8C/n2SXx56bi/g09urR5J2BQdNSTvrLuCHwPxpA9AW/w0o4PCq+pckLwXeN7R9+iUv1jMYrgBofms5/Sve4dds7/13tWckydCweQDwCeBbwFOSPGFo2DwAuHvotdP/1p94nGRv4AoGR0H/R1VtTHIVg58fbM99wA+AnwO+PG3bXcCHquoNP/UqSRoBvzqXtFOq6tsMvt69IMkTkzymOQFoy9fjT2Dw9e73m98K/ta0XXwHOGjo8deAxyU5JclewO8Cez+K99/VFgDnJNkryb9n8LvTa6rqLuDzwB8leVySIxj8hvLD29jXd4BFzdfeAI9l8LfeC2xqjm7+0myKan5GcAnwp81JSXskOaEZXj8M/HKSFzXPP645sWjhjv/5krTjHDQlPRqvZjAk3cHga/HLgf2bbf8VOBr4PoMTUj4+7bV/BPxu85vPc6vq+8DZDH7feDeDI5zr2LZtvf+u9kUGJw7dB/whcFpV3d9sOx1YxODo5pXAO7dzfdD/3vz3/iQ3N0dCzwH+lsHf8R8YHC2drXMZfM3+JeBfgHcDj2mG4FMZnOV+L4MjnL+F/79f0oh4wXZJ2o4kqxhcXP7ErmuRpN2J/6qVJElSKxw0JUmS1Aq/OpckSVIrPKIpSZKkVuyW19F88pOfXAcffHDXZWjI+vXrmTdvXtdlaIg96Sf70j/2pJ/GqS833XTTfVU1J6Nfd8tB82lPexo33nhj12VoyMTEBMuWLeu6DA2xJ/1kX/rHnvTTOPUlyT91XUNX/OpckiRJrXDQlCRJUiscNCVJktQKB01JkiS1wkFTkiRJrXDQlCRJUiscNCVJktSK3fI6mpIkSepGkkngIWAzsKmqjt3a2laPaCY5J8mdSb6X5CtJ1iS5McmJQ2tek+T/NrfXtFmPJEmSdonlVXXUtoZMgFRVaxUk+SpwEvAAsL6qKskRwN9W1XOSPAW4ETgWKOAm4Jiq+t629nvAQQfXY152YWt1a8e97fBNXHCrB8j7xJ70k33pH3vST5eumDdOyUA3bW8g2500RzSPrar7tre2tSOaST4AHAR8CnhD/XiincdgqAR4EXBdVf1LM1xeB6xoqyZJkiQ9agVcm+SmJGdta2Fr/4SrqjcmWcHg0Op9SX4F+CNgAXBKs+wZwF1DL1vXPPdTmj/kLID58/fj/MM3tVW6dsLT9hkcFVB/2JN+si/9Y0/6aWpqiomJia7L0MxOrKq7kywArkvy1ar67EwLR/ZdQVVdCVyZZCnwBwy+Ut+R118EXASwePHievMZp+76IrXTJiYmeNmYfMUxLuxJP9mX/rEn/TQxMTE2X52Pm6q6u/nvd5NcCRwHzDhojvzyRs3Ee1CS+cDdwDOHNi9snpMkSVLPJJmX5Alb7gO/BNy2tfUjGTSTHJwkzf2jgb2B+4G/A34pyc8k+Zmm2L8bRU2SJEnaYU8D/iHJl4H/A1xdVau3tnhUX53/GvDqJBuBDcDLm5OD/iXJHwBfatb9flX9y4hqkiRJ0g6oqm8AR852fauDZlUtau6+u7nNtOYS4JI265AkSdLoGUEpSZKkVjhoSpIkqRVGIUiSJGnWepF1PpRzfkWSLyT5YZJzp625JMl3k2z1tHhJkiT1zqyyzts8onk2g4uy/wh4FvDSGdZcCrwP+OsW65AkSVIHWhk0p+WcX1JV70lyyvR1VfXZJIt2dP8bNm5m0XlXP/pCtcu87fBNrLInvWJP+sm+9I896adLV8zrugRt3Zas8wL+vElvnFErg+b0nPNdsU+zzvvNrOD+sSf9ZF/6x570k1nnvda/rPNHy6zzfjMruH/sST/Zl/6xJ/1k1nl/9TrrXJIkSbunHc06322OaEqSJKlzTwOuTAKDOfKjnWadJ3k6cCPwRODhJG8FDq2qB5NcBiwD5idZB7yzqi5uuyZJkiTtuN5knQ/lnAMs3Mqa09t6f0mSJHXL32hKkiSpFQ6akiRJaoWDpiRJ6q3Nmzfz3Oc+l5UrV3ZdinZCq4PmUN7595J8JcmaJDcmOXFozebm+TVJPtFmPZIkafdy4YUXcsghh3RdhnZS20c0zwZOBp4JHFlVRwFnAh8cWrOhCWU/qqr+Xcv1SJKk3cS6deu4+uqref3rX991KdpJrZ11PlPeebNpHoOMzJ1m1nn/mBXcP/akn+xL/5ip3V9vfetb+eM//mMeeuihrkvRTmrz8kY/kXee5FeAPwIWAKcMLX1ckhuBTcC7quqqmfZn1nm/mRXcP/akn+xL/5ip3U/XX389Gzdu5KGHHmLNmjXcf//99mk3lKpHdXBx2ztPJoFjq+q+oeeWAudX1UnN42c0wewHAdcDv1hV/7it/S5evLjWrl3bWt3acWbS9o896Sf70j/2pJ/OOOMMPvOZz7Dnnnvygx/8gAcffJBf/dVf5cMf/nDXpe2wJDdV1bFd19GFkZ91XlWfBQ5KMr95vCWY/RvABPDcUdckSZL65Q1veAPr1q1jcnKSj33sY7zwhS/cLYfMuW4kg2aSg9OEYiY5GtgbuD/JzyTZu3l+PvB84I5R1CRJkqR2tZ513vg14NVJNgIbgJdXVSU5BPjzJA8zGHrfVVUOmpIk6RHLli3z5w27qVYHzaG883c3t+nbPw8c3mYNkiRJ6obJQJIkSWqFg6YkSZJa4aApSZKkVjhoSpKk3tq8eTPPfe5zWblyZdelaCd0MmgmOSfJnUm+l+QrSdYkuTHJiV3UI0mS+unCCy/kkEMO6boM7aRRXd5ourOBk4AHgPXNpY6OAP4WeM72XmzWef+Y39w/9qSf7Ev/mHXeX+vWrePqq6/md37nd/jTP/3TrsvRThj5Ec0kHwAOAj4FvKF+nIE5D2gvD1OSJO1W3vrWt/LHf/zHPOYx/tJvdzXyI5pV9cYkK4DlVXVfkl8B/ghYAJyytdclOQs4C2D+/P04//BNI6lXs/O0fQZHatQf9qSf7Ev/TE1NMTEx0XUZmub6669n48aNPPTQQ6xZs4b777/fPu2G8uMDiiN802QSOLaq7ht6bilwflWdtL3XL168uNauXdtihdpRExMTpjb0jD3pJ/vSP/akn8444ww+85nPsOeee/KDH/yABx98kF/91V/dLfPOk9xUVcd2XUcXenMsuqo+CxzUZJ5LkqQ57A1veAPr1q1jcnKSj33sY7zwhS/cLYfMua7TQTPJwUnS3D8a2Bu4v8uaJEmStGt0ddb5Fr8GvDrJRmAD8PLq4rt8SZLUW8uWLfPnDbupTgbNqlrU3H13c5MkSdKY6c1vNCVJkjReHDQlSZLUCgdNSdKc94Mf/IDjjjuOI488ksMOO4x3vvOdXZckjYVWB83ZZJonOSDJtc26O5IsarMmSZKm23vvvbn++uv58pe/zJo1a1i9ejU33HBD12VJu722TwaaTab5XwN/WFXXJdkXeLjlmiRJ+glJ2HfffQHYuHEjGzdupLn6nqRHobVBc1qm+SVV9Z5m0yOZ5kkOBfasqusAqmpqNvvesHEzi867etcXrZ32tsM3scqe9Io96adLV8zrugRtxebNmznmmGP4+te/zpve9CaOP/74rkuSdnutRlAOR01OzzSvqi8keSnweuBHwIHA/wLOq6rNM+xrOOv8mPPf+xet1a0d97R94Dsbuq5Cw+xJPx34pD0eOXKmfpiamvqJnkxNTfGOd7yDc845hwMPPLDDyua26X3ZnS1fvnzORlCO7DqaVXUlcGWTaf4HDL5S3xN4AfBc4J+BvwFWARfP8PqLgItgkHX+5jNOHU3hmpWJiQle5sV0e8We9JO52v0zU09uvvlm7r//fl772td2U5T8rIyJkZ91Pi3TfB2wpqq+UVWbgKuAo0ddkyRpbrv33nt54IEHANiwYQPXXXcdz3nOc7b9IknbNZIjmkkOBv6xORloONP8e8CTk+xXVfcCLwRuHEVNkiRt8e1vf5vXvOY1bN68mYcffpiXvexlrFy5suuypN3eqL4631qm+eYk5wL/O4PT+24C/PGlJGmkjjjiCG655Zauy5DGTquD5mwyzZszzo9osw5JkiSNnslAkiRJaoWDpiRJklrhoClJmvPMOpfaMaqs8480j5+XZFOS05rHy5v88y23HzQXcZckaWTMOpfaMZKs86pal2QPBicEXbtlY1V9GjgKIMlTgK8Pb5ckaRTMOpfaMZKs8ySXMMg3vwJ43lZechrwqar61+3t26zz/jFXu3/sST+Zdd5fZp1Lu95Iss4ZXKD9o8By4BLgk1V1+bS11wN/WlWf3Mq+zDrvMXO1+8ee9JNZ5/1j1nk/mXU+HkZ1wfb3Am+vqodn+ioiyf7A4cDfbW0Hw1nnBxx0cF1w68hi2jULbzt8E/akX+xJP126Yp75zT1j1nk/mXU+Hkb1v0LHAh9rhsz5wEuSbKqqq5rtLwOurKqNs9nZPnvtwdp3ndJKodo5ExMTTJ6xrOsyNMSe9NPExETXJWgG9957L3vttRdPfvKTH8k6f/vb3951WdJubySDZlU98t1DkksZfHV+1dCS04HfHkUtkiRNZ9a51I7Ov1dLsgh4JvCZjkuRJM1RZp1L7RhV1vnwc6umPZ4EntFmHZIkSRo9k4EkSZLUCgdNSZIktcJBU5IkSa1w0JQkzXk/+MEPOO644zjyyCM57LDDeOc739l1SdJYaG3QTHJOkjuTVJKvJLk1yeeTHNlsX5xkzdDtwSRvbaseSZK2Zu+99+b666/ny1/+MmvWrGH16tXccMMNXZcl7fbaPOv8bOAk4ADgzqr6XpIXM0j3Ob6q1gJHASTZA7gbuHI2OzbrvH/M1e4fe9JPZp33U5JH4g43btzIxo0bmSnJTtKOaeWIZpIPAAcBn2IwVH6v2XQDsHCGl/wi8I9V9U9t1CNJ0vZs3ryZo446igULFnDyySdz/PHHd12StNtLVbWz42QSOLaq7ht67lzgOVX1+mlrLwFurqr3bWN/ZwFnAcyfv98x57/3L1qpWzvnafvAdzZ0XYWG2ZN+OvBJezxy5Ez9MDU19RM9mZqa4h3veAfnnHMOBx544DZeqTZN78vubPny5TdV1bFd19GFkQ2aSZYDfwacWFX3D617LPAt4LCq+s5s9r148eJau3btri9aO21iYoJly5Z1XYaG2JN+si/9M1NPfv/3f5/HP/7xnHvuud0UpbH6rCSZs4PmSM46T3IE8EHg1OEhs/FiBkczZzVkSpK0q91777088MADAGzYsIHrrruO5zznOd0WJY2B1rPOkxwAfBx4VVV9bYYlpwOXtV2HJElb8+1vf5vXvOY1bN68mYcffpiXvexlrFy5suuypN1e64MmcD7wVODPmjP4Nm05fJxkHnAy8OsjqEOSpBkdccQR3HLLLV2XIY2d1gbNqlrU3H19c5tpzXoGQ6gkSZLGjMlAkiRJaoWDpiRJklrhoClJmvPMOpfa0WXW+TOTfDrJHUluT/KWtmqRJGlbzDqX2tFZ1jmwCXhbVd2c5AnATUmuq6o7WqxJkqSfYta51I5WBs1pWeeXVNXnm02PZJ1X1beBbzf3H0pyJ/AMYLuD5oaNm1l03tVtlK6d9LbDN7HKnvSKPemnS1fM67oEbcXmzZs55phj+PrXv86b3vQms86lXaAvWeeLgM8CS6rqwa3sz6zzHjNXu3/sST+Zdd4/Zp33k1nn42EUF2wHHsk6fx1w4rTn9wWuAN66tSEToKouYvC1O4sXL643n3Fqi9VqR01MTPCyMcmkHRf2pJ/GKb95XMzUk5tvvpn777+f1772td0UJT8rY6LTrPMkezEYMj9SVR8fRS2SJE1n1rnUjs6yzjP4lfXFDE4U+tO265AkaWvMOpfa0WXW+fOBVwG3JlnTrP0vVXXNCGqSJOkRZp1L7egs67yq/gHw2hGSJEljymQgSZIktcJBU5IkSa1w0JQkzXlmnUvtaHXQHMo7vzvJ95OsaW7nT1u3R5JbknyyzXokSZqJWedSO9o+63xL3vnBwLlVtbVrRbwFuBN4Ysv1SJL0U8w6l9rR2qA5Pe98G+sWAqcAfwj85mz2bdZ5/5ir3T/2pJ/MOu8vs86lXa+1rHP4cd45sIRBAtA64FsMjm7e3qy5HPgj4Als46inWef9Zq52/9iTfjLrvH/MOu8ns87Hw6iyzm8GnlVVU0leAlwFPDvJSuC7VXVTkmXb2sFw1vkBBx1cF9w6sph2zcLbDt+EPekXe9JPl66YZ35zz5h13k9mnY+HkfyvUFU9OHT/miR/lmQ+g3Sgf9cMn48Dnpjkw1X1ym3tb5+99mDtu05pt2jtkImJCSbPWNZ1GRpiT/ppYmKi6xI0g3vvvZe99tqLJz/5yY9knb/97W/vuixptzeSQTPJ04HvVFUlOY7B2e73V9VvA7/drFnG4KvzbQ6ZkiTtamadS+0Y1fdqpwG/kWQTsAF4RbX541BJknaAWedSO1odNIfyzt/X3La1dgKYaLMeSZIkjY7JQJIkSWqFg6YkSZJa4aApSZKkVjhoSpLmvB/84Accd9xxHHnkkRx22GG8853v7LokaSy0OmgmOSfJnUnuTvL9JGua2/lDa1YkWZvk60nOa7MeSZJmsvfee3P99dfz5S9/mTVr1rB69WpuuOGGrsuSdnttX97obOAk4GBmiJdMsgfwfuBkBvGUX0ryiaq6o+W6JEl6RJJH4g43btzIxo0bSdJxVdLur7VBM8kHgIOATwGXbGXZccDXq+obzWs+BpwKbHPQ3LBxM4vOu3oXVqtH622Hb2KVPekVe9JPl66Y13UJ2orNmzdzzDHH8PWvf503velNHH/88V2XJO320uZ105NMAscCS4ArGBy1/BaDo5u3JzkNWFFVr2/Wvwo4vqr+4wz7Ogs4C2D+/P2OOf+9f9Fa3dpxT9sHvrOh6yo0zJ7004FP2uORI2fqh6mpqZ/oydTUFO94xzs455xzOPDAAzusbG6b3pfd2fLly2+qqmO7rqMLo0oGuhl4VlVNNbnmVwHP3pEdVNVFwEUAixcvrjefceouL1I7b2JigpctW9Z1GRpiT/ppYmKCZfalV2bqyc0338z999/Pa1/72m6Kkp+VMTGSs86r6sGqmmruXwPslWQ+cDfwzKGlC5vnJEkamXvvvZcHHngAgA0bNnDdddfxnOc8p9uipDEwkiOaSZ4OfKeqKslxDAbc+4EHgGcnOZDBgPkK4D+MoiZJkrb49re/zWte8xo2b97Mww8/zMte9jJWrly5/RdK2qZRfXV+GvAbSTYBG4BX1ODHoZuS/Efg74A9gEuq6vYR1SRJEgBHHHEEt9xyS9dlSGOn1UGzqhY1d9/X3GZacw1wTZt1SJIkafRMBpIkSVIrHDQlSZLUCgdNSRqxu+66i+XLl3PooYdy2GGHceGFF3ZdkiS1YlRZ599L8pUm5/zGJCc225cP5Z+vSfKDJC9tsyZJ6tqee+7JBRdcwB133MENN9zA+9//fu64w+RdSeNnVFnnDwDrm8sbHQH8LfCcqvo0cBRAkqcAXweubbkmSerU/vvvz/777w/AE57wBA455BDuvvtuDj300I4rk6Rda2RZ51X1nmbTPGCm3MvTgE9V1b9ub99mnfePudr9Y6b27mFycpJbbrnFXG1JY2kkWedVdV+SXwH+CFgAnFJVX5i29nrgT6vqk1vZl1nnPWaudv+Yqd1Pw/nNGzZs4C1veQuvfOUrWbp0aceVzV3jlKk9TsapL3M563xkg+bQc0uB86vqpKHn9ge+AvxsVW3c3n4XL15ca9eubaFi7SwzafvHnvTTlr5s3LiRlStX8qIXvYjf/M3f7LqsOc3PSj+NU1+SzNlBc+RnnVfVZ4GDmqzzLV4GXDmbIVOSdndVxete9zoOOeQQh0xJY20kg2aSg5OkuX80sDeDrPMtTgcuG0UtktS1z33uc3zoQx/i+uuv56ijjuKoo47immsMSJM0fkaVdf5rwKuTbGSQdf7yJuucJIuAZwKfGVEtktSpE088kTZ/tiRJfTGqrPN3N7eZ1kwCz2izDkmSJI2eyUCSJElqhYOmJEmSWuGgKUkjZta5pLliVCcD/YQk5wC/AXwD+BHwc8APgDOr6rYuapKkUdmSdX700Ufz0EMPccwxx3DyyScbQSlp7HR1RPNs4GTgDmBNVR0BvBrwn/WSxt7+++/P0UcfDfxk1rkkjZuRH9GcloF+ELACoKq+mmRRkqdV1Xe2tQ+zzvvHrPP+Met892DWuaRx1moE5VbftImmBH4T2Keq/lOS44DPA8dX1U0zvMas8x4z67x/zDrvJ7PO+2ecMrXHyTj1xazzUb/pjwfNHzH4uvy5wK3Ac4A3VNWabb3+gIMOrse8zG/Z++Rth2/igls7+cmvtuLSFfPGJid4nJh13j/jlKk9TsapL3M567zTyaCqHgReC9BEVH6TwQlC27TPXnuw9l2ntFyddsTExASTZyzrugwNmZiY6LoEbYVZ55Lmik4vb5TkyUke2zx8PfDZZviUpLFl1rmkuaLr7zoPAf4qSQG3A6/ruB5Jap1Z55Lmik4GzaEM9PuAf9NFDZIkSWqXyUCSJElqhYOmJEmSWuGgKUmSpFY4aErSiN11110sX76cQw89lMMOO4wLL/S6wJLGU2uDZpJzktyZ5IokX0jywyTnTlvz5CSXJ/lqs/aEtuqRpL7Yc889ueCCC7jjjju44YYbeP/7388dd9zRdVmStMu1edb52cBJDNJ/ngW8dIY1FwKrq+q05nqaj2+xHknqhf3335/9998fgCc84Qkccsgh3H333Rx66KEdVyZJu1Yrg2aSDwAHAZ8CLqmq9yQ5ZdqaJwFLgVUAVfUjBkPpdm3YuJlF5129S2vWo/O2wzexyp70yqUr5nVdgmZhcnKSW265heOPP77rUiRpl2st63xLnnlV3dc8/j1gqqr+pHl8FHARcAdwJHAT8JaqWr+V/Z0FnAUwf/5+x5z/3r9opW7tnKftA9/Z0HUVGnbgk/Zg33337boMTTM1NfVIXzZs2MBb3vIWXvnKV7J06dKOK5u7hnui/hinvixfvnzOZp13OWgeC9wAPL+qvpjkQuDBqnrH9va9ePHiWrt2bSt1a+dMTEywbNmyrsvQEHvST1v6snHjRlauXMmLXvQi88475meln8apL0nm7KDZ5Vnn64B1VfXF5vHlwNEd1iNJI1FVvO51r+OQQw5xyJQ01jobNKvqHuCuJIubp36RwdfokjTWPve5z/GhD32I66+/nqOOOoqjjjqKa665puuyJGmXaz3rPMnTgRuBJwIPJ3krcGhVPQi8GfhIc8b5N4DXtl2PJHXtxBNPpK2fLUlSn7Q2aFbVoqGHC7eyZg0wJ3+zIEmSNO5MBpIkSVIrHDQlSZLUCgdNSRoxs84lzRWtnww0kyTnAL8BfLWp4YDmv39SVX/ZRU2SNCpbss6PPvpoHnroIY455hhOPvlkIygljZ2ujmieDZwMfAm4o6qOBJYBFzRnoEvS2Np///05+ujBZYOHs84ladyM/IjmtBz0jwJPSBJgX+BfgE3b24dZ5/1j1nn/mHW+ezDrXNI4ay2Ccptv2sRTAj8EPgE8B3gC8PKqmnFaMeu838w67x+zzvvJrPP+GadM7XEyTn0x63zUb/rjQXMZ8HzgN4GfA64Djmwu5r5VZp33zzhl0o4Le9JPZp33j5+Vfhqnvph13p3XAh+vga8D32RwdFOSxpZZ55Lmiq4HzX9mkHFOkqcBixlEUUrS2DLrXNJc0cnljYb8AXBpkluBAG+vqvs6rkmSWmXWuaS5opNBc1oO+i91UYMkSZLa1fVX55IkSRpTDpqSJElqhYOmJEmSWuGgKUkjdtddd7F8+XIOPfRQDjvsMC688MKuS5KkVrQ2aCY5J8mdSa5I8oUkP0xy7tD2xUnWDN0eTPLWtuqRpL7Yc889ueCCC7jjjju44YYbeP/7388dd9zRdVmStMu1edb52cBJwI+AZwEvHd5YVWuBowCS7AHcDVw5mx2bdd4/Zp33j1nn/bX//vuz//77A/CEJzyBQw45hLvvvptDDz2048okaddq5Yhmkg8ABwGfAs6oqi8BG7fxkl8E/rGq/qmNeiSpryYnJ7nllls4/vjjuy5Fkna51rLOt+SZb7kAe5LfA6aq6k9mWHsJcHNVvW8b+zsLOAtg/vz9jjn/vX/RRtnaSU/bB76zoesqNOzAJ+3Bvvvu23UZmmZqauqRvmzYsIG3vOUtvPKVr2Tp0qUdVzZ3DfdE/TFOfVm+fPmczTrvfNBM8ljgW8BhVfWd2ex78eLFtXbt2l1bsB6ViYkJli1b1nUZGmJP+mlLXzZu3MjKlSt50YteZN55x/ys9NM49SXJnB00+3DW+YsZHM2c1ZApSbu7quJ1r3sdhxxyiEOmpLHWh0HzdOCyrouQpFH53Oc+x4c+9CGuv/56jjrqKI466iiuueaarsuSpF2u9azzJE8HbgSeCDzcXMLo0Kp6MMk84GTg19uuQ5L64sQTT6Stny1JUp+0NmhW1aKhhwu3smY98NS2apAkSVJ3+vDVuSRJksaQg6YkSZJa4aApSSNm1rmkuaLVQXMo7/x7Sb7SZJrfmOTEZvtRTQ767c32l7dZjyT1gVnnkuaKts8635J3/gCwvqoqyRHA3wLPAf4VeHVV/d8kPwvclOTvquqBluuSpM6YdS5prmht0JyWd35JVb2n2TQPKICq+tqW9VX1rSTfBfZjMJhu1YaNm1l03tVtlK2d9LbDN7HKnvTKpSvmdV2CZsGsc0njrLUISvjJGMokvwL8EbAAOKWqvjBt7XHAXzGIonx4hn2Zdd5jZp33j1nn/WTWef+MU6b2OBmnvph13tbOp+WdN88tBc6vqpOGntsfmABeU1U3bG+/Zp33zzhl0o4Le9JPZp33j5+Vfhqnvph1PkJV9VngoCTzAZI8Ebga+J3ZDJmStLsz61zSXDGSQTPJwUnS3D8a2Bu4P8ljgSuBv66qy0dRiyR1zaxzSXNF61nnjV8DXp1kI7ABeHlzBvrLgKXAU5Osatauqqo1I6pLkkbOrHNJc0Wrg+ZQ3vm7m9v07R8GPtxmDZIkSeqGyUCSJElqhYOmJEmSWuGgKY25M888kwULFrBkyZKuS5EkzTGtDZpDOefV5JjfmuTzSY4cWvOWJLc1WedvbasWaS5btWoVq1ev7roMSdIc1OYRzbOBk4HnA79QVYcDfwBcBJBkCfAG4DjgSGBlkoNbrEeak5YuXcpTnvKUrsuQJM1BrZx1PkPO+eebTTcAC5v7hwBfrKp/bV7zGeBXgT/e3v7NOu8fc7UlSdJ0rQyaVfXGJCuA5cPxk8DrGAyfALcBf5jkqQyurfkS4Mat7XNa1jnnH76pjdK1k6amppiYmOi6DA0Z7sk999zD+vXr7VEP+FnpH3vST/ZlPIzqgu0kWc5g0DwRoKruTPJu4FpgPbAG2Ly111fVRTRfuy9evLjefMapbZesHTBOmbTjYrgnk5OTzJs3zx71gJ+V/rEn/WRfxsOoIiiPAD4InFpV9295vqourqpjqmop8D3ga6OoR5IkSe1rfdBMcgDwceBVVfW1adsWDK35VeCjbdcjzTWnn346J5xwAmvXrmXhwoVcfPHFXZckSZojRvHV+fnAU4E/SwKwqaqObbZd0fxGcyPwpqp6YAT1SHPKZZdd1nUJkqQ5qrVBcyjn/PXNbaY1L2jr/SVJktQtk4EkSZLUCgdNSZIktcJBU5IkSa1w0JTG3JlnnsmCBQtYsmRJ16VIkuaYVgfNJOckuTPJFUm+kOSHSc4d2v64JP8nyZeT3J7kv7ZZjzQXrVq1itWrV3ddhiRpDmr78kZnAycBPwKeBbx02vYfAi+sqqkkewH/kORTVXXDtnZq1nn/mHXeX0uXLmVycrLrMiRJc1BrRzSTfAA4iEG2+RlV9SUG18t8RA1MNQ/3am7VVk2SJEkanTavo/nGJCuA5VV139bWJdkDuAk4GHh/VX1xK+vOAs4CmD9/P84/fFMLVWtnTU1NMTEx0XUZGjLck3vuuYf169fbox7ws9I/9qSf7Mt4GEUy0DZV1WbgqCRPBq5MsqSqbpth3UXARQCLFy+uN59x6mgL1TZNTEywbNmyrsvQkOGeTE5OMm/ePHvUA35W+see9JN9GQ+9Oeu8iZ/8NLCi41IkSZK0C3Q6aCbZrzmSSZJ9gJOBr3ZZkzRuTj/9dE444QTWrl3LwoULufjii7suSZI0R4zkq/MkTwduBJ4IPJzkrcChwP7AXzW/03wM8LdV9clR1CTNFZdddlnXJUiS5qhWB82qWjT0cOEMS74CPLfNGiRJktSN3vxGU5IkSePFQVOSJEmtcNCUxpxZ55KkrnQyaA5loFeSryS5NcnnkxzZRT3SODPrXJLUla4u2L4lA/0A4M6q+l6SFzO4IPvxHdUkjSWzziVJXRn5oDktA/2Sqvp8s+kGZj4z/ads2LiZRedd3VKF2hmXrpjXdQmSJKlnRj5obiMD/XUMhs8ZmXXeb2bS9o9Z5/3kZ6V/7Ek/2Zfx0HnWOUCS5QwGzRO3tsas834zk7Z/zDrvJz8r/WNP+sm+jIfOB80kRwAfBF5cVfd3XY8kSZJ2ja6zzg8APg68qqq+1mUt0rgy61yS1JWuj2ieDzwV+LMkAJuq6thuS5LGi1nnkqSudDJoDmWgv765SZIkacyYDCRJkqRWOGhKkiSpFQ6a0pgz61yS1JXWBs2hPPO7k3w/yZrmdv7Qmskm53xNkhvbqkWay8w6lyR1pc2TgbbkmR8MnFtVK7eybnpCkKRdyKxzSVJXWhk0p+eZ7+r9m3XeP2adS5Kk6VJV7ew4mQSOBZYAVwDrgG8xOLp5e7Pmm8D3gAL+vImZ3Nr+hrPOjzn/vX/RSt3aOQc+aQ/23XffrsvQkKmpqUd6cs899/Dbv/3b/OVf/mXHVWm4L+oHe9JP49SX5cuX3zRXrxM+iuto3gw8q6qmkrwEuAp4drPtxKq6O8kC4LokX62qz860k+Gs8wMOOrguuLXra81r2KUrzNHuG7PO+8n85v6xJ/1kX8ZD69NaVT04dP+aJH+WZH5V3VdVdzfPfzfJlcBxwIyD5rB99tqDte86pb2itcMmJia6LkGSJPVM65c3SvL0NPmSSY5r3vP+JPOSPKF5fh7wS8BtbdcjzTVmnUuSujKK759PA34jySZgA/CKqqokTwOubGbQPYGPVpXXYJF2MbPOJUldaW3QHMozf19zm779G8CRbb2/JEmSujWrr86T/FySvZv7y5qLsT+51cokSZK0W5vtbzSvADYnOZjBmd/PBD7aWlWSJEna7c120Hy4qjYBvwL8/6vqt4D92ytLkiRJu7vZDpobk5wOvAb4ZPPcXu2UJGlXOvPMM1mwYAFLlizpuhRJ0hwz20HztcAJwB9W1TeTHAh8aHsvan7LeWeSjzSPn5dkU5LTmsdHJflCktuTfCXJy3f2D5E0s1WrVrF6tRd0kCSN3qzOOq+qO5K8HTigefxN4N2zeOnZwElVtS7JHs1rrh3a/q/Aq6vq/yb5WeCmJH9XVQ9sa6dmnfePWef9tXTpUiYnJ7suQ5I0B832rPNfBtYAq5vHRyX5xHZe8wHgIOBTSf4T8GYGJxV9d8uaqvpaVf3f5v63mm377fifIUmSpL6Z7XU0f49BPOQEQFWtSXLQtl5QVW9MsgJYDuzN4Cz15cDzZlrfpAY9FvjHrWw/CzgLYP78/Tj/8E2zLF2jMDU1ZQxlzwz35J577mH9+vX2qAf8rPSPPekn+zIeZjtobqyq7zcpPls8vAPv817g7VX18LR9AJBkfwa/+XxNVc2436q6iMGllVi8eHG9+YxTd+Dt1baJiQmWLVvWdRkaMtyTyclJ5s2bZ496wM9K/9iTfrIv42G2g+btSf4DsEeSZwPnAJ/fgfc5FvhYM2TOB16SZFNVXZXkicDVwO9U1Q07sE9JkiT12GzPOn8zcBjwQwZfgX8feOts36SqDqyqRU0s5eXA2c2Q+VjgSuCvq+ryHSlc0uycfvrpnHDCCaxdu5aFCxdy8cUXd12SJGmO2O4RzeZs8aurajnwO7v4/V8GLAWemmRV89yqqlqzi99HmrMuu+yyrkuQJM1R2x00q2pzkoeTPKmqvr8jO2+OYE5/btXQ/Q8DH96RfUqSJGn3MNvfaE4Btya5Dli/5cmqOqeVqiRJkrTbm+2g+fHmJkmSJM3KbJOB/qrtQiS148wzz+STn/wkCxYs4Lbbbuu6HEnSHDLbZKBvJvnG9NssXrcl6/x7TZb5miQ3Jjmx2f6sJDc3z9+e5I2P9g+S9JPMOpckdWW2X50fO3T/ccC/B54yi9edDZwEPACsr6pKcgTwt8BzgG8DJ1TVD5PsC9yW5BNNHKWkXcCsc0lSV2b71fn90556b5KbgPO39prhrHPgkqp6T7NpHlDNfn809JK9meUR1g0bN7PovKtns1QjcumKeV2XIEmSemZWg2aSo4cePobBEc5tvnY467yq7kvyK8AfAQuAU4b2/UwGyUAHA7+1taOZZp33m5m0/WPWeT/5Wekfe9JP9mU8pKq2vyj59NDDTcA3gQuqau12XjcJHFtV9w09txQ4v6pOmrb2Z4GrgF+uqu9sa7+LFy+utWu3+dYaMTNp+2d61vnKlSs9GagH/Kz0jz3pp3HqS5KbqurY7a8cP7P9jebrquonTv5JcuDOvGFVfTbJQUnmDw+gVfWtJLcBL2AQUylJkqTd2Gyzzmca/GY9DCY5OEma+0cz+D3m/UkWJtmnef5ngBMBD1VKu5BZ55KkrmzziGaS5wCHAU9K8qtDm57I4Ozz2fo14NVJNgIbgJc3Z6AfAlyQpIAAf1JVt+7QXyBpm8w6lyR1ZXtfnS8GVgJPBn556PmHgDdsb+dDWefvbm7Tt18HHDGLOiVJkrSb2d6Z4/8D+B9JTqiqL4yoJkmSJI2B2Z4MdEuSNzH4Gv2Rr8yr6sxWqpIkSdJub7YnA30IeDrwIuAzwEIGX59L6rkzzzyTBQsWsGTJkq5LkSTNMbMdNA+uqncwiJH8KwYXXD9+ey8ayjq/IskXkvwwybnT1vynJuf8tiSXJdmRk4wkbYdZ55Kkrsz2q/ONzX8fSLIEuIdBws/2bMk6/xHwLOClwxuTPAM4Bzi0qjYk+VvgFcCls6xL0naYdS5J6spsB82LmutcvgP4BLAv28g5h5mzzpOcMsPSPYF9mksfPR6YMYJymFnn/WPWuSRJmm5Wg2ZVfbC5+xkGw+NsXvMTWedbWXN3kj8B/pnB9TWvraprZ1pr1nm/mUnbP2ad95Oflf6xJ/1kX8bDrAbNJE8D/hvws1X14iSHAidU1aOKGGmOkp4KHAg8APz3JK+sqg9PX1tVFwEXARxw0MF1wa2zPRirUbh0xbyxyaQdF9OzzufNs0d9ME75zePCnvSTfRkPs53WLgX+Evid5vHXgL8BHm2W3UnAN6vqXoAkHwf+LfBTg+awffbag7XvmulbeHXFf3VKkqTpZnvW+fyq+lvgYYCq2gRs3gXv/8/Azyd5fJOF/ovAnbtgv5IaZp1Lkroy2yOa65M8FSiAJD8PfH+2b5Lk6cCNDDLSH07yVgZnmn8xyeXAzcAm4Baar8cl7RpmnUuSujLbQfM3GZxt/nNJPgfsB5y2vRcNZZ3D4CLvM615J/DOWdYhSZKk3cQ2B80kB1TVP1fVzUl+AVgMBFhbVRu39VpJkiTNbdv7jeZVQ/f/pqpur6rbHDIlSZK0PdsbNDN0f1bXz5QkSZJg+4NmbeW+pN3EmWeeyYIFC1iyZEnXpUiS5pjtDZpHJnkwyUPAEc39B5M8lOTBbb0wyTlJ7kxyd5LvJ1nT3M4fWvPkJJcn+Wqz9oRd8UdJ+rFVq1axevXqrsuQJM1B2zwZqKr2eBT7PpvBBdkPBs6tqpUzrLkQWF1VpyV5LIOs8+0y67x/zDrvr6VLlzI5Odl1GZKkOWi2F2zfIUk+wOA3nZ8CnruVNU8CltKkC1XVj6rqgTbqkSRJ0ui1EhheVW9MsgJYDiwBfjfJl4FvMTi6eTuDfPN7gb9MciRwE/CWqlo/0z6TnAWcBTB//n6cf/imNkrXTpqamjKGsmeGe3LPPfewfv16e9QDflb6x570k30ZD6lq5xyfJJPAscCPgIerairJS4ALq+rZSY4FbgCe3yQEXQg8WFXv2N6+Fy9eXGvXrm2lbu2ciYkJli1b1nUZGjLck8nJSVauXMltt93WbVHys9JD9qSfxqkvSW6qqmO7rqMLrXx1PqyqHqyqqeb+NcBeSeYD64B1VfXFZunlwNFt1yNJkqTRaH3QTPL0JGnuH9e85/1VdQ9wV5LFzdJfBO5oux5prjn99NM54YQTWLt2LQsXLuTiiy/uuiRJ0hzRym80pzkN+I0km4ANwCvqx9/Xvxn4SHPG+TeA146gHmlOueyyy7ouQZI0R7U2aFbVoubu+5rbTGvWMPgdpyRJksZM61+dS5IkaW5y0JQkSVIrHDSlMWfWuSSpK60OmkN55x9pHj8vyaYkpzWPn5Xk5iYD/fYkb2yzHmkuMutcktSVts86Pxs4qarWJdkDeDdw7dD2bwMnVNUPk+wL3JbkE1X1rZbrkuYMs84lSV1pbdAczjtPcglQwBXA87asqaofDb1kb2Z5hHXDxs0sOu/qXVitHq1LV8zrugRJktQzbV7eaDjvfG/go8395w2vS/JM4GrgYOC3tnY006zzfjOTtn/MOu8nPyv9Y0/6yb6Mh1FcsB3gvcDbq+rhJiToEVV1F3BEkp8FrkpyeVV9Z/oOquoi4CIYZJ2/+YxT269aszZOmbTjYnrW+bx58+xRD/hZ6R970k/2ZTyMatA8FvhYM2TOB16SZFNVXbVlQVV9K8ltwAsY5J5LkiRpNzaSyxtV1YFVtahJC7ocOLuqrkqyMMk+AEl+BjgRWDuKmqS5wqxzSVJXRnVEc2sOAS5IUkCAP6mqWzuuSRorZp1LkrrS6qA5lHc+/NyqofvXAUe0WYMkSZK6YTKQJEmSWuGgKUmSpFY4aEpjzqxzSVJXWhs0h3LOK8lXktya5PNJjhxa8+Qklyf5arP2hLbqkeYqs84lSV1p82Sgs4GTgAOAO6vqe0lezOCi68c3ay4EVlfVaUkeCzy+xXqkOcmsc0lSV1oZNIdzzoFLqurzzaYbgIXNmicBS4FV8Eju+Y9+amczMOu8f8w6lyRJ07UyaA7nnFfVfUObXsdg+AQ4ELgX+Mvm6/SbgLdU1fqZ9mnWeb+ZSds/Zp33k5+V/rEn/WRfxsPILtieZDmDQfPEofc+GnhzVX0xyYXAecA7Znr9cNb5AQcdXBfc2vW15jXs0hXmaPeNWef9ZH5z/9iTfrIv42Ek01qSI4APAi+uqvubp9cB66rqi83jyxkMmtu1z157sPZdp+z6QrXT/FenJEmarvXLGyU5APg48Kqq+tqW56vqHuCuJIubp34RuKPteqS5xqxzSVJXRnFE83zgqcCfJQHYVFXHNtveDHykOeP8G8BrR1CPNKeYdS5J6kprg+ZQzvnrm9tMa9YAx860TZIkSbs3k4EkSZLUCgdNSZIktcJBU5IkSa1w0JTG3JlnnsmCBQtYsmRJ16VIkuaYVgfNJOckuTNJJflKkluTfL5JAtqy5j8luT3JbUkuS/K4NmuS5ppVq1axevXqrsuQJM1BbR/RPBs4GXg+8AtVdTjwBzQJP0meAZwDHFtVS4A9gFe0XJM0pyxdupSnPOUpXZchSZqDWru8UZIPAAcxyDa/pKo+32y6AVg4rYZ9kmwEHg98a3v73rBxM4vOu3oXV6xH49IV87ouQZIk9Uyb19F8Y5IVwPKqum9o0+sYDJ9U1d1J/gT4Z2ADcG1VXTvT/pKcBZwFMH/+fpx/+Ka2StdOmJqaMoayZ4Z7cs8997B+/Xp71AN+VvrHnvSTfRkPI8k63yLJcgaD5onN458BTgUOBB4A/nuSV1bVh6e/tqouovnKffHixfXmM04dVdmahYmJCZYtW9Z1GRoy3JPJyUnmzZtnj3rAz0r/2JN+si/jYWRnnSc5AvggcGpV3d88fRLwzaq6t6o2MshE/7ejqkmSJEntGcmgmeQABkPkq6rqa0Ob/hn4+SSPzyAI/ReBO0dRkzRXnH766ZxwwgmsXbuWhQsXcvHFF3ddkiRpjhjVV+fnA08F/mwwT7Kpqo6tqi8muRy4GdgE3ELz9bikXeOyyy7rugRJ0hzV6qBZVYuau69vbjOteSfwzjbrkCRJ0uiZDCRJkqRWOGhKkiSpFQ6a0pgz61yS1JVRZZ1fkeQLSX6Y5NwZ1u2R5JYkn2yzHmkuMutcktSVts86P5vBtTJ/BDwLeOlW1r2FwWWNnthyPdKcs3TpUiYnJ7suQ5I0B40y6/w9SU6ZYd1C4BTgD4HfnM2+zTrvH7POJUnSdF1knU/3XuA/A0/Y1v7MOu83M2n7x6zzfvKz0j/2pJ/sy3gYadb5dElWAt+tqpuSLNvWWrPO+81M2v4x67yf/Kz0jz3pJ/syHro+6/z5wL9LMgl8DHhhkg93W5IkSZJ2hU4Hzar67apa2CQIvQK4vqpe2WVN0rgx61yS1JWRfHWe5OnAjQzOKn84yVuBQ6vqwVG8vzSXmXUuSerKqLLOARZuZ+0EMNFiOZIkSRqhrn+jKUmSpDHloClJkqRWOGhKY86sc0lSV0aVdV5JvpLk1iSfT3Lk0JoVSdYm+XqS89qsR5qLzDqXJHVlVFnnBwB3VtX3kryYwYXXj0+yB/B+4GRgHfClJJ+oqjtarkuaM8w6lyR1ZZRZ559vNt3Aj89APw74elV9o3nNx4BTgW0Ommad949Z55Ikabouss5fx2D4BHgGcNfQtnXA8TPtz6zzfjOTtn/MOu8nPyv9Y0/6yb6Mh5FmnSdZzmDQPHFHXzucdX7AQQfXBbd2GtOuaS5dYY5235h13k/mN/ePPekn+zIeRjatJTkC+CDw4qq6v3n6buCZQ8sWNs9t0z577cHad52y64vUTvNfnZIkabqRXN4oyQHAx4FXVdXXhjZ9CXh2kgOTPJZB3vknRlGTNFeYdS5J6sqojmieDzwV+LMkAJuq6tiq2pTkPwJ/B+zB4KSh20dUkzQnmHUuSerKqLLOX9/cZlpzDXBNm3VIkiRp9EwGkiRJUiscNCVJktQKB01JkiS1wkFTGnNnnnkmCxYsYMmSJV2XIkmaY1obNJOck+TOJHcn+X6SNc3t/Gb74qHn1iR5MMlb26pHmqtWrVrF6tWruy5DkjQHtXnW+dnAScDBwLlVtXJ4Y1WtBY4CSLIHgwu1X9liPdKctHTpUiYnJ7suQ5I0B7UyaCb5AHAQg0zzS2bxkl8E/rGq/mk2+9+wcTOLzrv6UVSoXe3SFfO6LkGSJPVMK4NmVb0xyQpgObAE+N0kXwa+xeDo5vSLsr8C2OZVpZOcBZwFMH/+fpx/+KZdX7h22tTUlDGUPTPck3vuuYf169fbox7ws9I/9qSf7Mt4SFW1s+NkEjgW+BHwcFVNJXkJcGFVPXto3WMZDKCHVdV3ZrPvxYsX19q1a1uoWjtrYmKCZcuWdV2Ghgz3ZHJykpUrV3Lbbbd1W5T8rPSQPemncepLkpuq6tiu6+hC62edV9WDVTXV3L8G2CvJ/KElLwZunu2QKUmSpN1D64NmkqenCThPclzznvcPLTmd7XxtLmnnnX766ZxwwgmsXbuWhQsXcvHFF3ddkiRpjmg167xxGvAbSTYBG4BXVPN9fZJ5wMnAr4+gDmlOuuwy/x0nSepGa4NmVS1q7r6vuc20Zj3w1LZqkCRJUndMBpIkSVIrHDQlSZLUCgdNacyZdS5J6kqrg+ZQ3vlHmsfPS7IpyWlDa1YneSDJJ9usRZqrzDqXJHWl7bPOzwZOqqp1TZ75u4Frp635/wGPxzPPpVaYdS5J6kprg+Zw3nmSS4ACrgCeN7yuqv53kmU7sm+zzvvHrHNJkjRdm5c3Gs473xv4aHP/edt84VaYdd5vZtL2j1nn/eRnpX/sST/Zl/Ewigu2A7wXeHtVPdyEBO2wqroIuAgGWedvPuPUXVedHrVxyqQdF9OzzufNm2ePesDPSv/Yk36yL+NhVIPmscDHmiFzPvCSJJuq6qoRvb8kSZJGbCSXN6qqA6tqUZMWdDlwtkOmNBpmnUuSujKqI5pbleTvgecA+yZZB7yuqv6u47KksWHWuSSpK60OmkN558PPrZr2+AVt1iBJkqRumAwkSZKkVjhoSpIkqRUOmpIkSWqFg6Y05s4880wWLFjAkiVLui5FkjTHtDZoJjknyZ1J7k7y/SRrmtv5Q2vekuS2JLcneWtbtUhz2apVq1i9enXXZUiS5qA2zzo/GzgJOBg4t6pWDm9MsgR4A3Ac8CNgdZJPVtXXt7djs877x6zz/lq6dCmTk5NdlyFJmoNaOaKZ5APAQcCngOduZdkhwBer6l+rahPwGeBX26hHkiRJo9fKEc2qemOSFcByYAnwu0m+DHyLwdHN24HbgD9M8lRgA/AS4Mat7TPJWcBZAPPn78f5h29qo3TtpKmpKSYmJrouQ0OGe3LPPfewfv16e9QDflb6x570k30ZD6NIBroZeFZVTSV5CXAV8OyqujPJu4FrgfXAGmDz1nZSVRcBFwEsXry43nzGqW3XrR0wMTHBsmXLui5DQ4Z7Mjk5ybx58+xRD/hZ6R970k/2ZTy0ftZ5VT1YVVPN/WuAvZLMbx5fXFXHVNVS4HvA19quR5IkSaPR+qCZ5OlJ0tw/rnnP+5vHC5r/HsDg95kfbbseaa45/fTTOeGEE1i7di0LFy7k4osv7rokSdIcMYqvzk8DfiPJJga/xXxFVVWz7YrmN5obgTdV1QMjqEeaUy677LKuS5AkzVGtDZpVtai5+77mNtOaF7T1/pIkSeqWyUCSJElqhYOmJEmSWuGgKY05s84lSV1pddAcyjv/SPP4eUk2JTltaM3moRz0T7RZjzQXmXUuSepK22ednw2cVFXrkuwBbLlA+7ANVXVUy3VIc5ZZ55KkrrQ2aA7nnSe5BCjgCuB5j3bfGzZuZtF5Vz/a3WgXunTFvK5LkCRJPdPm5Y2G8873ZnAx9uX89KD5uCQ3ApuAd1XVVTPtz6zzfjOTtn/MOu8nPyv9Y0/6yb6Mh1FcsB3gvcDbq+rhJiRo2LOq6u4kBwHXJ7m1qv5x+iKzzvvNTNr+Meu8n/ys9I896Sf7Mh5GNWgeC3ysGTLnAy9JsqmqrqqquwGq6htJJoDnAj81aEqSJGn3MpLLG1XVgVW1qEkLuhw4u6quSvIzSfYGSDIfeD5wxyhqkuYKs84lSV0Z1RHNrTkE+PMkDzMYet9VVQ6a0i5k1rkkqSutDppDeefDz60auv954PA2a5AkSVI3TAaSJElSKxw0JUmS1AoHTWnMmXUuSerKqLLOv5fkK02e+Y1JThxa88dJbm/W/b+Z4UKbknaeWeeSpK6MJOsceABYX1WV5Ajgb4HnJPm3DC5pdESz/h+AXwAmWq5LmjPMOpckdWUkWefAJVX1nmbTPAa55zT/fRzwWCDAXsB3trdvs877x6xzSZI03UiyzqvqviS/AvwRsAA4pVnzhSSfBr7NYNB8X1XdOdP+zDrvNzNp+8es837ys9I/9qSf7Mt4GNkF26vqSuDKJEuBPwBOSnIwg4u2L2yWXZfkBVX19zO83qzzHjOTtn/MOu8nPyv9Y0/6yb6Mh5GfdV5VnwUOaiInfwW4oaqmqmqKwdfsJ4y6JkmSJO16Ixk0kxy85WzyJEcDewP3A/8M/EKSPZPsxeBEoBm/Ope0c8w6lyR1ZVRfnf8a8OokG4ENwMubM9AvB14I3MrgxKDVVfU/R1STNCeYdS5J6sqoss7f3dymb98M/HqbNUiSJKkbJgNJkiSpFQ6akiRJaoWDpiRJklrhoCmNuTPPPJMFCxawZMmSrkuRJM0xrQ6aSc5JcmeSjzSPn5dkU5LThtYckOTaZt0dSRa1WZM016xatYrVq1d3XYYkaQ5q+/JGZwMnVdW6JHswOPP82mlr/hr4w6q6Lsm+wMPb26lZ5/1j1nl/LV26lMnJya7LkCTNQa0Nmkk+ABwEfCrJJQyuk3kF8LyhNYcCe1bVdQBNOpAkSZLGQGuDZlW9MckKYDmDJKCPNvefN7Ts3wAPJPk4cCDwv4Dzmutr/oQkZwFnAcyfvx/nH76prdK1E6amppiYmOi6DA0Z7sk999zD+vXr7VEP+FnpH3vST/ZlPIwqGei9wNur6uEmiXL4/V8APJdBHOXfAKuAn8rIq6qLgIsAFi9eXG8+49R2K9YOmZiYYNmyZV2XoSHDPZmcnGTevHn2qAf8rPSPPekn+zIeRjVoHgt8rBky5wMvSbIJWAesqapvACS5Cvh5Zhg0JUmStHsZyeWNqurAqlrURFJeDpxdVVcBXwKenGS/ZukLgTtGUZM0V5x++umccMIJrF27loULF3Lxxf47TpI0GqM6ojmjqtqc5Fzgf2dwuPMm4C+6rEkaN5dddlnXJUiS5qhWB83mCOb051ZNe3wdcESbdUiSJGn0TAaSJElSKxw0JUmS1AoHTWnMmXUuSepKa4PmUM753Um+n2RNczt/aM0lSb6b5La26pDmOrPOJUldafOI5tnAycAZwN9X1VHN7feH1lwKrGixBmnOW7p0KU95ylO6LkOSNAe1ctb5cM45cMnW1lXVZ5Ms2tH9b9i4mUXnXb3zBWqXu3TFvK5LkCRJPdPKoDkt53wJ8LtJvgx8Czi3qm7f0X2add5vZtL2j1nn/eRnpX/sST/Zl/Ewigu23ww8q6qmkrwEuAp49o7uxKzzfjOTtn/MOu8nPyv9Y0/6yb6Mh9bPOq+qB6tqqrl/DbBXkvltv68kSZK61fqgmeTpTbwkSY5r3vP+tt9X0oBZ55Kkroziq/PTgN9IsgnYALyiqgogyWXAMmB+knXAO6vK/xWUdiGzziVJXWlt0BzKOX9fc5tpzeltvb8kSZK6ZTKQJEmSWuGgKUmSpFY4aEpjzqxzSVJXRpF1Xkm+kuTWJJ9PcuTQmsnm+TVJbmyrFmkuM+tcktSVNs86Pxs4CTgAuLOqvpfkxQwuun780LrlVXVfi3VIc9rSpUuZnJzsugxJ0hw0kqzzqvp8s+kGYOGj3b9Z5/1j1rkkSZqu9azzaUcrX8dg+HxkKXBtkgL+vImZnJFZ5/1mJm3/mHXeT35W+see9JN9GQ+juGA7AEmWMxg0Txx6+sSqujvJAuC6JF+tqs/O9PrhrPMDDjq4Lrh1ZKVrFi5dYY5235h13k/mN/ePPekn+zIeRjKtJTkC+CDw4qp6JH6yqu5u/vvdJFcCxwEzDprD9tlrD9a+65S2ytVO8F+dkiRpulFknR8AfBx4VVV9bej5eUmesOU+8EvAbW3XI801Zp1LkroyiiOa5wNPBf4sCcCmqjoWeBpwZfPcnsBHq8prsEi7mFnnkqSujCLr/PXNbfr2bwBHTn9ekiRJ48FkIEmSJLXCQVOSJEmtcNCUJElSKxw0pTF35plnsmDBApYsWdJ1KZKkOaa1QTPJOUnuTHJ3ku8nWdPczm+2PzPJp5PckeT2JG9pqxZpLlu1ahWrV3tBB0nS6LV5eaOzgZOAg4Fzq2rltO2bgLdV1c3N9TRvSnJdVd2xvR2bdd4/Zp3319KlS5mcnOy6DEnSHNTKEc0kHwAOYpBr/tyZ1lTVt6vq5ub+Q8CdwDPaqEeSJEmjl6pqZ8fJJHAssAS4AlgHfIvB0c3bp61dxCB6cklVPbiV/Z0FnAUwf/5+x5z/3r9opW7tnAOftAf77rtv12VoyNTU1CM9ueeee/jt3/5t/vIv/7LjqjTcF/WDPemncerL8uXLb2rCauacUQyaPwIerqqpJC8BLqyqZw+t2xf4DPCHVfXx2ex78eLFtXbt2haq1s6amJhg2bJlXZehIcM9mZycZOXKldx2mymvXfOz0j/2pJ/GqS9J5uyg2fpZ51X1YFVNNfevAfZKMh8gyV4MjnZ+ZLZDpiRJknYPrQ+aSZ6eJtA8yXHNe97fPHcxcGdV/WnbdUhz1emnn84JJ5zA2rVrWbhwIRdffHHXJUmS5og2zzrf4jTgN5JsAjYAr6iqSnIi8Crg1iRrmrX/pTnqKWkXueyyy7ouQZI0R7U2aFbVoubu+5rb9O3/AKSt95ckSVK3TAaSJElSKxw0JUmS1AoHTWnMmXUuSerKKLLOK8lXktya5PNJjhxac0mS7ybx4n5SS8w6lyR1pc0jmmcDJwPPB36hqg4H/gC4aGjNpcCKFmuQ5rylS5fylKc8pesyJElzUCtnnU/LOr+kqj7fbLoBWLhlXVV9tomf3CEbNm5m0XlX74pStYtcumJe1yVIkqSeaWXQrKo3JlkBLK+q+4Y2vY7B8LnDpmWdc/7hmx59odplpqammJiY6LoMDRnuyT333MP69evtUQ/4Wekfe9JP9mU8jOKC7QAkWc5g0DxxZ15fVRfRfO2+ePHievMZp+7C6vRojVMm7biYnnU+b948e9QDflb6x570k30ZDyMZNJMcAXwQeHFV3T+K95QkSVK3RpF1fgDwceBVVfW1tt9P0k8y61yS1JVRHNE8H3gq8GdJADZV1bEASS4DlgHzk6wD3llV/q+gtAuZdS5J6sooss5f39xmWnN6W+8vSZKkbpkMJEmSpFY4aEqSJKkVDprSmDPrXJLUlVYHzaG88480j5+XZFOS04bW/HGS25t1/2+aM4Yk7RpmnUuSutL2Ec2zgZOr6owkewDvBq7dsjHJv2WQhX4EsAR4HvALLdckzSlmnUuSutLaWefDeedJLgEKuILBMLlFAY8DHgsE2Av4zvb2bdZ5/5h1LkmSpmvz8kaP5J0DewMfbe4/b2jNF5J8Gvg2g0HzfVV150z7M+u838yk7R+zzvvJz0r/2JN+si/jYVRZ5+8F3l5VDw//BDPJwcAhwMLmqeuSvKCq/n76Doazzg846OC64NaRxbRrFi5dYY5235h13k/mN/ePPekn+zIeRjWtHQt8rBky5wMvSbIJeDZwQ1VNAST5FHAC8FOD5rB99tqDte86pd2KtUP8V6ckSZpuJJc3qqoDq2pRkxZ0OXB2VV0F/DPwC0n2TLIXgxOBZvzqXNLOMetcktSVrr9/vhx4IXArgxODVlfV/+y2JGm8mHUuSepKq4PmUN758HOrhu5vBn69zRokSZLUDZOBJEmS1AoHTUmSJLXCQVOSJEmtcNCUxtyZZ57JggULWLJkSdelSJLmmFYHzSTnJLkzyUeax89LsinJadPWPTHJuiTva7MeaS5atWoVq1ev7roMSdIc1Pbljc4GTqqqdUn2AN4NXDvDuj8APttyLdKctHTpUiYnJ7suQ5I0B7U2aCb5AHAQ8KkklzC4TuYVDGWdN+uOAZ4GrGaQILRdGzZuZtF5V+/agvWoXLpiXtclSJKknmlt0KyqNyZZASwH9gY+2tx/ZNBM8hjgAuCVwEnb2l+Ss4CzAObP34/zD9/UUuXaGVNTU8ZQ9sxwT+655x7Wr19vj3rAz0r/2JN+si/jYVTJQO8F3l5VDzd551ucDVzTfLW+zR1U1UXARQCLFy+uN59xakulamdMTEywbNmyrsvQkOGeTE5OMm/ePHvUA35W+see9JN9GQ+jGjSPBT7WDJPzgZck2QScALwgydnAvsBjk0xV1XkjqkuSJEktGcnljarqwKpa1ERSXg6cXVVXVdUZVXVA8/y5wF87ZEq71umnn84JJ5zA2rVrWbhwIRdffHHXJUmS5ohRHdGU1JHLLrus6xIkSXNUq4Nmc6Ry+nOrtrL2UuDSNuuRJEnS6JgMJEmSpFY4aEqSJKkVDprSmDPrXJLUldYGzaGc8yuSfCHJD5OcO23NZJJbk6xJcmNbtUhzmVnnkqSutHky0NkM0n5+BDwLeOlW1i2vqvtarEOa08w6lyR1pZVBczjnHLikqt6T5JRdtX+zzvvHrHNJkjRdK4PmcM75do5WFnBtkgL+vImZnJFZ5/1mJm3/mHXeT35W+see9JN9GQ9dX7D9xKq6O8kC4LokX62qz8600KzzfjOTtn/MOu8nPyv9Y0/6yb6Mh07POq+qu5v/fhe4Ejiuy3okSZK063Q2aCaZl+QJW+4DvwTc1lU90rgy61yS1JXWvzpP8nTgRuCJwMNJ3gocCswHrkyypY6PVpXXYJF2MbPOJUldaW3QnJZzvnCGJQ8CR7b1/pIkSeqWyUCSJElqhYOmJEmSWuGgKY05s84lSV1pddCcZd75iiRrk3w9yXlt1iPNRWadS5K60vZZ59vMO0+yB/B+4GRgHfClJJ+oqjtarkuaM8w6lyR1pbVBc5Z558cBX6+qbzSv+RhwKrDNQdOs8/4x61ySJE3X5uWNZpN3/gzgrqHH64DjZ1po1nm/mUnbP2ad95Oflf6xJ/1kX8ZD11nnszacdX7AQQfXBbfuNqXPCZeuMEe7b8w67yfzm/vHnvSTfRkPXU9rdwPPHHq8sHlum/bZaw/Wvmv6t/Dqkv/qlCRJ03V9eaMvAc9OcmCSxwKvAD7RcU3SWDHrXJLUlZEc0dxa3nlVPZjkPwJ/B+zB4KSh20dRkzRXmHUuSepKq4PmLPLOqaprgGvarEOSJEmj1/VX55IkSRpTDpqSJElqhYOmJEmSWuGgKUmSpFY4aEqSJKkVDpqSJElqhYOmJEmSWpGq6rqGHZbkIWBt13XoJ8wH7uu6CP0Ee9JP9qV/7Ek/jVNfnlVV+3VdRBe6zjrfWWur6tiui9CPJbnRnvSLPekn+9I/9qSf7Mt48KtzSZIktcJBU5IkSa3YXQfNi7ouQD/FnvSPPekn+9I/9qSf7MsY2C1PBpIkSVL/7a5HNCVJktRzDpqSJElqxW41aCZZkWRtkq8nOa/regRJLkny3SS3dV2LBpI8M8mnk9yR5PYkb+m6JkGSxyX5P0m+3PTlv3ZdkwaS7JHkliSf7LoWQZLJJLcmWZPkxq7r0aOz2/xGM8kewNeAk4F1wJeA06vqjk4Lm+OSLAWmgL+uqiVd1yNIsj+wf1XdnOQJwE3AS/2sdCtJgHlVNZVkL+AfgLdU1Q0dlzbnJflN4FjgiVW1sut65rokk8CxVTUuF2uf03anI5rHAV+vqm9U1Y+AjwGndlzTnFdVnwX+pes69GNV9e2qurm5/xBwJ/CMbqtSDUw1D/dqbrvHv/THWJKFwCnAB7uuRRpHu9Og+QzgrqHH6/B/PKVtSrIIeC7wxY5LEY98RbsG+C5wXVXZl+69F/jPwMMd16EfK+DaJDclOavrYvTo7E6DpqQdkGRf4ArgrVX1YNf1CKpqc1UdBSwEjkviz006lGQl8N2quqnrWvQTTqyqo4EXA29qfqKl3dTuNGjeDTxz6PHC5jlJ0zS/AbwC+EhVfbzrevSTquoB4NPAio5LmeueD/y75jeBHwNemOTD3Zakqrq7+e93gSsZ/HROu6ndadD8EvDsJAcmeSzwCuATHdck9U5z0snFwJ1V9add16OBJPsleXJzfx8GJzZ+tdOi5riq+u2qWlhVixj8b8r1VfXKjsua05LMa05iJMk84JcAr2qyG9ttBs2q2gT8R+DvGJzc8LdVdXu3VSnJZcAXgMVJ1iV5Xdc1iecDr2JwdGZNc3tJ10WJ/YFPJ/kKg384X1dVXk5H+klPA/4hyZeB/wNcXVWrO65Jj8Juc3kjSZIk7V52myOakiRJ2r04aEqSJKkVDpqSJElqhYOmJEmSWuGgKUmSpFbs2XUBkrQrJNkM3Dr01EurarKjciRJeHkjSWMiyVRV7TvC99uzub6vJGkr/Opc0pyQZP8kn20uYH9bkhc0z69IcnOSLyf5381zT0lyVZKvJLkhyRHN87+X5ENJPgd8qEn7uSLJl5rb8zv8EyWpd/zqXNK42CfJmub+N6vqV6Zt/w/A31XVHybZA3h8kv2AvwCWVtU3kzylWftfgVuq6qVJXgj8NXBUs+1Q4MSq2pDko8B7quofkhzAILnskNb+QknazThoShoXG6rqqG1s/xJwSZK9gKuqak2SZcBnq+qbAFX1L83aE4Ffa567PslTkzyx2faJqtrQ3D8JOHQQLw/AE5PsW1VTu+qPkqTdmYOmpDmhqj6bZClwCnBpkj8FvrcTu1o/dP8xwM9X1Q92RY2SNG78jaakOSHJs4DvVNVfAB8EjgZuAJYmObBZs+Wr878HzmieWwbcV1UPzrDba4E3D73HUS2VL0m7JY9oSporlgG/lWQjMAW8uqruTXIW8PEkjwG+C5wM/B6Dr9m/Avwr8Jqt7PMc4P3Nuj2BzwJvbPWvkKTdiJc3kiRJUiv86lySJEmtcNCUJElSKxw0JUmS1AoHTUmSJLXCQVOSJEmtcNCUJElSKxw0JUmS1Ir/DzgX0kqgTr1hAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imprimir la importancia de las características\n",
    "xgb.plot_importance(xgb_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74276938",
   "metadata": {},
   "source": [
    "#### Ejercicio\n",
    "En el siguiente [enlace](https://archive.ics.uci.edu/ml/datasets/Parkinson+Speech+Dataset+with++Multiple+Types+of+Sound+Recordings) se describe un problema de clasificación. Dicho toma son las grabaciones de una serie de sonidos emitidos por 20 pacientes diagnosticados de la enfermedad de Parkinson y 20 sujetos sanos. A cada uno de ellos se les hace repetir una serie de números, vocales y consonantes, recogiendo en cada uno de ellos 27 propiedades de la audición. A mayores de estas 27 características, el problema cuenta, en la primera columna, con el identificador del sujeto y, en la última a la clase a la que pertenece. En el fichero asosciado existen dos ficheros txt, utilice solo el de entrenamiento train_data tanto para entrenamiento como para test diviediéndolo en un porcentaje de 90:10 entre entrenamiento y test.\n",
    "\n",
    "Aplique las técnicas tanto de *Random Forest* como de *XGBoost* al problema descrito teniendo como objetivo el poder clasificar a los pacientes.\n",
    "\n",
    "**IMPORTANTE** Los patrones proceden de un conjunto de voluntarios, a la hora de dividir los conjuntos de entrenamiento y test es importante el tenerlo presente. Ningun patrón perteneciente a un voluntario debiera de estar repartido entre los conjuntos de entrenamiento y test. Este esquema de división del es lo que se denomina un ***Subject-wise*** y en ámbitos como el médico es esencial para garantizar la correcta generalización de los modelos por lo tanto tengalo presente a la hora de dividir los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb2d7c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
