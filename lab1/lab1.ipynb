{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7de94d8d",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eirasf/GCED-AA3/blob/main/lab1/lab1.ipynb)\n",
    "\n",
    "# Práctica 1:  Reducción de la dimensionalidad\n",
    "\n",
    "## El problema de la maldición de las dimensionalidades\n",
    "\n",
    "Uno de los principales problemas en machine learning es el que se denomina la maldición de las dimensionalidades o de las dimensiones. Este problema surge desde el momento en el que se pretende mejorar una aproximación por el mero hecho de usar más variables. Quizá se podría, pero lo más problable es que su efecto sea contraproducente. Es aquí donde se incurre en la mencionada maldición  ya que, **a medida que aumenta el número de características o dimensiones, la cantidad de datos que son necesarios\n",
    "para obtener una generalización precisa aumenta exponencialmente.** Sin embargo antes de poder acometer el problema, es necesario saber de donde viene.\n",
    "\n",
    "![Comportamiento del rendimiento cuando se aumenta el número de variables](./Images/Perfomance_Dimension_plot.png)\n",
    "\n",
    "\n",
    "### La dimensionalidad de los problemas\n",
    "\n",
    "Rara vez se piensa en el impacto que tendrá una determinada variable en un proceso de optimización. Tómese por ejemplo un caso en el que se dispone de 5 observaciones para una determinada variable **X**, y dichas observaciones están uniformemente distribuidas en el espacio. De esa manera cada una de las observaciones tratará de representar a $\\frac{1}{5}$ del mencionado espacio.\n",
    "\n",
    "![](./Images/Sampling_Examples.png)\n",
    "\n",
    "Cuando se añade una vueva variable **Y** pasando , por tanto, a un espacio bidimensional. Con el fin de mantener la misma distancia entre las muestras, la misma tasa de representatividad del espacio se debería de aumentar el número de muestras a 25. Con una tercera se necesitarían 125 muestras para explorar el espacio en las mismas condiciones, etc. \n",
    "\n",
    "Por lo tanto, el presente problema se acrecenta de manera exponencial cuantas más dimensiones tenemos.\n",
    "\n",
    "\n",
    "### La maldición\n",
    "\n",
    "¿Qué sucede en un problema real? Pues que habitualmente no se puede aumentar el número de muestras para mantener la representatividad de los puntos muestrales y la equidistancia. Es por ello que, si agregamos una nueva característica pero no se dota de puntos suficientes, el resultado sería un modelo más complejo pero cuyo con un rendimiento empobrecido.\n",
    "\n",
    "El porqué de esa afirmación puede verse claramente con el siguiente ejemplo, en el que se pretende clasificar entre las imagenes de gatos y perros. Si solo se tiene en cuenta una dimensión, los ejemplos se encuentran uniformemente distribuidos. \n",
    "\n",
    "![](./Images/Doom_1.png)\n",
    "\n",
    "En este caso se dispone de 10 muestras que cubren todo el espacio. Pero, si se aumenta una dimensión, esa distribución pasa a ser algo como la siguiente figura.\n",
    "\n",
    "![](./Images/Doom_2.png)\n",
    "\n",
    "Esta situación podría hacer pensar que una nueva dimensión haría aun más fácil el dividir el espacio. Por ejemplo, añadiendo al problema anterior una tercera dimensión se obtine algo como:\n",
    "\n",
    "![](./Images/Doom_3.png)\n",
    "\n",
    "Dondé el resultado es linealmente separable como se puede ver en la siguiente imagen.\n",
    "\n",
    "![](./Images/Doom_4.png)\n",
    "\n",
    "La conclusión errónea a la que nos puede llevar es que, cuanto más se aumenta la dimensionalidad, más sencillo será la separación en base a las características. Nótese cómo ha variado la distribución de los datos: mientras que en una dimensión se tienen 2 muestras por cada intervalo de cinco muestras antes mencionado, en el espacio trimensional apenas llega a 0.08 muestras por intervalo (10/125). Por lo tanto, es más complicado que se encuentren contra ejemplos en un mismo lado del clasificador. El problema surge cuando proyectamos esos datos a un espacio dimensional inferior como pasa cuando se aplica cualquier red de neuronas artificiales al crear un clasificador. En esa situación el resultado sería similar a la siguiente figura:\n",
    "\n",
    "![](./Images/Doom_5.png)\n",
    "\n",
    "Como se puede ver en la siguiente figura, el clasificador hab sido sobreentrenado y, por lo tanto, el resultado no es tan bueno ante nuevas instancias como podría ser. Por ejemplo, véase la figura siguiente en donde se ha aplicado un clasificador lineal sencillo sobre menos dimensiones\n",
    "\n",
    "![](./Images/Doom_6.png)\n",
    "\n",
    "\n",
    "### ¿Cómo evitar la maldición?\n",
    "\n",
    "No existe una regla fija que defina cuántas características deben usarse en un problema de regresión/clasificación. El número dependerá de la cantidad de datos de entrenamiento disponibles, la complejidad de los límites de decisión y el tipo de clasificador utilizado. \n",
    "\n",
    "Existen principalmente dos tipos de aproximaciones con el fin de reducir la dimensionalidad. Esos dos tipos son:\n",
    "* las proyecciones \n",
    "* las transformaciones\n",
    "\n",
    "La diferencia entre una y la otra es que, mientras que las proyecciones operan sobre el propio espacio definido por el conjunto de muestras de entrada, las transformaciones tratan de modificar dicho espacio para encontrar una función de transición que permita una representación adecuada y separable de los datos.\n",
    "Algunas de las técnicas más habituales son:\n",
    "\n",
    "* *Principal Component Analysis (PCA)*\n",
    "* *Linear Discriminant Analysis (LDA)*\n",
    "* *Independent Components Analysis (ICA)*\n",
    "* *Locally linear embedding (LLE)*\n",
    "* *t-distributed Stochastic Neighbor Embedding (t-SNE)*\n",
    "* *IsoMaps*\n",
    "* *Autoencoders*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631a3e5e",
   "metadata": {},
   "source": [
    "# Prerrequisitos de la práctica\n",
    "Los ejemplos de esta unidad se realizarán en Python y será necesario tener instaladas algunas librerías en el sistema para asegurarse del correcto funcionamientos de los mismos. Si estás ejecutando en Colab, ignora la siguiente celda, que sirve para configurar el entorno en Windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb01b26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminal no funciona\n",
    "# Teclear en shell SET SHELL=C:\\Windows\\SysWOW64\\WindowsPowerShell\\v1.0\\powershell.exe\n",
    "\n",
    "# Instalar Kernel en Jupyter\n",
    "# conda env list\n",
    "# jupyter kernelspec list\n",
    "# jupyter kernelspec uninstall unwanted-kernel\n",
    "# python -m ipykernel install --user --name=firstEnv\n",
    "\n",
    "# Directorio inicio JupyterLab\n",
    "# JupyterLab >= 3, Jupyter Notebook Classic, and RetroLab\n",
    "# Open cmd (or Anaconda Prompt) and run jupyter server --generate-config instead\n",
    "# This writes a file to C:\\Users\\username\\.jupyter\\jupyter_notebook_config.py.\n",
    "# Browse to the file location and open it in an Editor\n",
    "# Search for the following line in the file: #c.NotebookApp.notebook_dir = ''\n",
    "# Replace by c.ServerApp.root_dir = '/the/path/to/home/folder/'\n",
    "# Make sure you use forward slashes in your path and use /home/user/ instead of ~/ for your home directory, backslashes could be used if placed in double quotes even if folder name contains spaces as such : \"D:\\yourUserName\\Any Folder\\More Folders\\\"\n",
    "# Remove the # at the beginning of the line to allow the line to execute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dd510a",
   "metadata": {},
   "source": [
    "La siguiente celda es sólo necesaria ejecutarla una única vez, tanto en Colab como en local. Para todas las ejecuciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca1f3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librería Sklearn\n",
    "!pip install scikit-learn\n",
    "# Librería matplotlib\n",
    "!pip install matplotlib\n",
    "# Librería seaborn\n",
    "!pip install seaborn\n",
    "\n",
    "# Librería pra tratar con datos matriciales\n",
    "!pip install numpy \n",
    "# Librería para poder utilizar Dataframes y consultas sobre datos estructurados\n",
    "!pip install pandas\n",
    "# Librería para calculos científicos con multitud de funcines de utilidad\n",
    "!pip install scipy   \n",
    "# Librería que facilita el uso de ficheros\n",
    "!pip install pathlib "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562de85d",
   "metadata": {},
   "source": [
    "## Carga de los datos para los ejemplos\n",
    "Antes de proseguir, se van a cargar un conjunto de datos de ejemplo para poder ver la influencia de las diferentes técnicas que abordaremos en este tutorial. El problema de ejemplo que se va a utilizar, es un problema clásico que lleva por título, **¿roca o mina?**. Se trata de una base de datos pequeña que consta de 111 patrones correspondientes a rocas y 97 patrones correspondientes a minas acuáticas (simuladas como cilindros metálicos). Cada uno de los patrones consta de 60 medidas numéricas entre 0.0. y 1.0. Esas medidas se corresponden con el valor de la energia de diferentes rangos de longitud de onda para un cierto periodo de tiempo.\n",
    "\n",
    "El primer paso será la descarga del conjunto de datos si no está disponible ya. Para ello se hará uso del siguiente código con la función de utilidad que se ve a continuación. Si por lo que fuera prefiere usar otra herramienta para la descarga es libre de seleccionar las que mejor se adecue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c852ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def load_data(filename, url):\n",
    "    #comprobar si el fichero ya existe y si no descargarlo\n",
    "    p_filename = Path(filename)\n",
    "    if not p_filename.exists():\n",
    "        print(f'Downloading'.ljust(75,'.'), end='', flush=True)\n",
    "        import urllib.request\n",
    "        urllib.request.urlretrieve(url,p_filename)\n",
    "        print(f\"Done!\")\n",
    "    return pd.read_csv(str(p_filename), delimiter=',', header=None)\n",
    "\n",
    "file_name = 'sonar.all_data'\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data'\n",
    "data = load_data(file_name, url)\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb9f09e",
   "metadata": {},
   "source": [
    "### Preprocesado de los datos\n",
    "Una vez descargados los datos, si bien las medidas ya están normalizadas, es necesario hacer una pequeña adaptación en los mismo. En concreto, es necesario cambiar la interpretación de la última columna, la cual contiene la etiqueta del problema, por un entero. A mayores, también se procederá a dividir el conjunto de datos en entrenamiento y test. Dicha división mantendrá un porcentaje del 10% de patrones para el test. Un hecho a resaltar es que la división se hace de manera que se mantengan las proporciones de las diferentes clases de salida, de tal manera que se espera que el conjunto de test tenga 11 rocas y 10 minas en su composición."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37b54c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Recoger las 60 primeras mediciones y convertirlas a un Numpy\n",
    "#no tienen nombre así que accedemos según la posición\n",
    "inputs = (data.iloc[:,0:60]).to_numpy()\n",
    "#La última columna nos marca el tipo de patron que es\n",
    "outputs = (data[60]=='M').astype('int')\n",
    "print(f\"Tipos de patrones: {np.unique(outputs)}\")\n",
    "\n",
    "#Crear los conjuntos de entrenamiento y test\n",
    "train_inputs, test_inputs, train_outputs, test_outputs = train_test_split(inputs, outputs, test_size=0.1, \n",
    "                                                                          stratify=outputs)\n",
    "\n",
    "print(f\"Train Patterns{train_inputs.shape} -> {train_outputs.shape}\")\n",
    "print(f\"Test Patterns{test_inputs.shape} -> {test_outputs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b786b0",
   "metadata": {},
   "source": [
    "## Principal Component Analisis (PCA)\n",
    "\n",
    "Probablemente la técnica de reducción de dimensionalidad más utilizada. Se puede usar tanto de manera individual como en combinación con otras técnicas. Se trata de un método que transforma los datos mediante una proyección sobre un conjunto de ejes ortogonales. Para realizar este cometido, el metodo busca las mejores combinaciones lineales de las variables originales, intentando maximizar la varianza a lo largo de la nueva variable. Por ejemplo, en la imagen de la derecha que se muestra un conjunto de puntos en tres dimensiones. Esas tres dimensiones al ser proyectadas  se obtienen las imagenes de la derecha que pemiten ver la variabilidad de los datos para cada uno de los ejes. La línea continua es la que mayor variabilidad presenta y, por tanto, la que se tomará como base o primera componente. Para la segunda componente, entre las restantes posibilidades se escogerá aquella que maximiza y sigue siendo perpendicular (ortogonal) a la primera dimensión seleccionada.\n",
    "\n",
    "![](./Images/PCA.png)\n",
    "\n",
    "Si se necesitase una tercera dimensión, PCA tendría que buscar una que fuera perpendicualar a estas. Este proceso se basa en la conocida como matriz *Single Value Decomposition (SVD)* que extrae los autovectores del espacio de muestras. Estos son ordenados de manera decreciente y se seleccionan los que mejor representan el espacio correspondiente.\n",
    "\n",
    "Para aquellos que deseen entender en detalle como funciona, en el siguiente [enlace](https://sebastianraschka.com/Articles/2014_pca_step_by_step.html) pueden encontrar una descripción de como implementar PCA paso por paso.\n",
    "\n",
    "En terminos generales si se quiere hacer uso de PCA, una buena alternativa es el uso de la implementación como la que se puede encontrar en la librería `scikit-learn`. Dicha librería cuenta con la función `PCA` que nos permite ejecutar esta técnica de reducción. Véase el siguiente ejemplo:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b0ee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Definir PCA en función del que se desea conservar\n",
    "\n",
    "pca = PCA(2)\n",
    "\n",
    "#Ajustar las matrices en función de las entradas del entrenamiento\n",
    "pca.fit(train_inputs)\n",
    "\n",
    "#Una vez se tiene la transformación, simplemente es necesario aplicar la \n",
    "#transformación a los conjuntos de datos\n",
    "\n",
    "pca_train_inputs = pca.transform(train_inputs)\n",
    "pca_test_inputs = pca.transform(test_inputs)\n",
    "\n",
    "print(f\"Train Patterns{train_inputs.shape} -> {pca_train_inputs.shape}\")\n",
    "print(f\"Test Patterns{test_inputs.shape} -> {pca_test_inputs.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdba2668",
   "metadata": {},
   "source": [
    "**Nótese que es importante que el ajuste de la transformación se haga sobre los datos de entrenamiento solamente**. En caso de hacerlos sobre el total de los datos, se estaría contaminando con la transformación los posibles entrenamientos de técnicas de clasificación o regresión que pudieran aplicarse posteriormente.\n",
    "\n",
    "Una de las principales ventajas de aplicar la reducción de la dimensionalidad es que permite realizar un primer estudio visual transformando un espacio multidimensional en otro de 2 o 3 dimensiones que sí se puede representar graficamente. En el ejemplo, se utilizarán los datos trasformados por PCA para la representación. En primer lugar, se define una función para facilitar la presentación los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7402437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('muted')\n",
    "sns.set_context(\"notebook\", font_scale=1.5,\n",
    "                rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "def draw_results(x, colors,target_names=None):\n",
    "    \"\"\" \n",
    "        Función de utilidad que permite imprimir un scatter plot \n",
    "        con el fin de ver como reparten los clusters\n",
    "    \"\"\"\n",
    "    import matplotlib.patheffects as PathEffects\n",
    "    \n",
    "    # Seleccionar los colores (que se corresponden \n",
    "    # con el vector de salida) en función \n",
    "    # del número de clases. Este será el vector de salida.\n",
    "    num_classes = len(np.unique(colors))\n",
    "    palette = np.array(sns.color_palette(\"hls\", num_classes))\n",
    "    \n",
    "    if target_names is not None:\n",
    "        assert num_classes == len(target_names)\n",
    "        label = target_names\n",
    "    else:\n",
    "        label = [str(i) for i in range(num_classes)]\n",
    "\n",
    "    # Crear el scatter plot \n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    #Coger solo las dos primeras dimensiones de cada p\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40, \n",
    "                    c=palette[colors.astype(int)], alpha=.8)\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    #ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "\n",
    "    # Añadir las etiquetas al listado de elementos graficos\n",
    "    txts = []\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        # Colocar las etiquetas en los valores medios medio del cluster\n",
    "        xtext, ytext = np.median(x[colors == i, :], axis=0)\n",
    "        txt = ax.text(xtext, ytext, label[i], fontsize=24)\n",
    "        txt.set_path_effects([\n",
    "            PathEffects.Stroke(linewidth=5, foreground=\"w\"),\n",
    "            PathEffects.Normal()])\n",
    "        txts.append(txt)\n",
    "\n",
    "    return f, ax, sc, txts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299db48d",
   "metadata": {},
   "source": [
    "De no haber reducido la dimensionalidad el experto sería el responsable de escoger las dos variables a representar. Al hacer esto, se correría el riego de no representar correctamente la distribución.\n",
    "A continuación, se imprime dos de las dimensiones que prefiera y compare los resultados con el obtenido por PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61176b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the PCA dataset\n",
    "draw_results(pca_train_inputs, train_outputs, target_names=train_outputs.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dff5db3",
   "metadata": {},
   "source": [
    "Reducir a 2 o 3 dimensiones puede ser de ayuda cuando se procura hacer un primer análisis para, por ejemplo, determinar si un clasificador lineal puede dar buenos resultados o si se observa algún patrón en la distribución de los datos. Sin embargo, lo más normal es intentar reducir la dimensionalidad pero manteniendo la mayor variabilidad posible. Para ello, la función de `scikit-learn` permite pasar un valor entre 0 y 1, que determina el porcentaje de variabilidad que debe mantener. Un valor típico es un 0.95 ya que mantiene casi toda la información relevante eliminando gran parte del ruido que pudiera haber."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaaacdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A continuación realice dicha reducción al 95%   \n",
    "pca = PCA(0.95)\n",
    "pca.fit(train_inputs)\n",
    "pca_train_inputs = pca.transform(train_inputs)\n",
    "pca_test_inputs = pca.transform(test_inputs)\n",
    "\n",
    "# Compare los tamaños con los que teníamos antes\n",
    "print(f\"Train Patterns{train_inputs.shape} -> {pca_train_inputs.shape}\")\n",
    "print(f\"Test Patterns{test_inputs.shape} -> {pca_test_inputs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6230fffd",
   "metadata": {},
   "source": [
    "A mayores de poder representar la información, el reducir la dimensionalidad suele venir asociado con un aceleramiento del entrenamiento. Esto se debe a que la complejidad computacional y el esfuerzo computacional de la mayoría de algoritmos de aprendizaje está condicionado en función del número de variables. Además, también es frecuente que haya una mejoría en los modelos al eliminarse parte del ruido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a1657f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10\n",
    "# A continuación veamos unas cuantas aproximaciones básicas y el tiempo que tardan\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "\n",
    "clfs = { 'SVM': svm.SVC(probability=True), \n",
    "         'LR': LogisticRegression(),\n",
    "         'DT': DecisionTreeClassifier(max_depth=4),\n",
    "         'NB':GaussianNB()}\n",
    "base_models = ['SVM', 'LR','DT','NB']\n",
    "\n",
    "for key in clfs.keys():\n",
    "    clfs[key].fit(train_inputs, train_outputs)\n",
    "    acc = clfs[key].score(test_inputs, test_outputs)\n",
    "    print(f\"{key}: {(acc*100):.4f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206d4bc2",
   "metadata": {},
   "source": [
    "Compara los resultados tras haber aplicado PCA sobre los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e289998",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10\n",
    "# realice el mismo entrenamiento que arriba pero con los datos de PCA\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4ca87f",
   "metadata": {},
   "source": [
    "Otro de los parámetros importantes dentro de la función `PCA` indica cómo se calcula la matriz SVD. La implementación de que se dispone en `scikit-learn` presenta diferentes alternativas, de las cuales destacan dos:\n",
    "1. la primera extrae los autovectores en base a la implementación de `LAPACK` de otro paquete conocido como es `scipy`.\n",
    "1. En el caso de que las dimensiones de la matriz sean pequeñas (menos de 500x500) y el número de componentes a extraer sea menor del 80%, se recomienda el uso de la implementación del algoritmo `randomized truncated SVD` presentado en los artículos \\[1, 2\\]\n",
    "\n",
    "Adicionalmente, también comentar que `scikit-learn` presenta algunas de las variantes más frecuentes de PCA como son:\n",
    "* [Kernel Principal Component Analysis](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html#sklearn.decomposition.KernelPCA), este método transforma el espacio inicial con la aplicación de un kernel en el que sea más sencillo aplicar posteriormente el PCA. A diferencia de PCA, no es una tranformación lineal.\n",
    "\n",
    "* [Sparse Principal Component Analysis](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.SparsePCA.html#sklearn.decomposition.SparsePCA), implementación que, primeramente, busca un conjunto de características dispersas que permita reconstruir el conjunto original controlando el grado de difusión para, a continuación, aplicar el PCA.\n",
    "\n",
    "* [Dimensionality reduction using truncated SVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD), versión que no centra los datos antes del calculo de SVD y que puede trabajar con matrices dispersas (_sparse_) de manera eficiente. \n",
    "\n",
    "* [Incremental Principal Components Analysis (IPCA)](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html#sklearn.decomposition.IncrementalPCA) Aplicación en batches de PCA sin escalar los datos antes del calculo de SVD. Puede suponer una mejora en cuanto al uso de memoria, si bien debido al uso de batches puede no ser tan preciso en ocasiones.\n",
    "\n",
    "\\[1\\]  [Halko, N., Martinsson, P. G., and Tropp, J. A. (2011). “Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions”. SIAM review, 53(2), 217-288](https://doi.org/10.1137/090771806)\n",
    "\n",
    "\\[2\\] [ Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011). “A randomized algorithm for the decomposition of matrices”. Applied and Computational Harmonic Analysis, 30(1), 47-68](https://doi.org/10.1016/j.acha.2010.02.003)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ffecae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplique alguna de las variantes a los datos del problema y represente el resultado, ¿qué diferencias se aprecian?.\n",
    "# NOTA: cuidado con la semilla aleatoria si se usa una decisión \"randomized\"\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a021e23",
   "metadata": {},
   "source": [
    "##  Independent Component Analysis (ICA)\n",
    "\n",
    "Aunque muchas veces no se la califica como una técnica de reducción de dimensionalidad, si no de extracción de las componentes principales, es probablemente la segunda técnica más popular aplicada en este sentido. El ICA es también un método de reducción lineal de la dimensión, que transforma el conjunto de datos en columnas de componentes independientes. La separación ciega de fuentes y el \"problema del cóctel\" son otros de sus nombres. ICA es una herramienta importante en el análisis de neuroimágenes, fMRI y EEG que ayuda a separar las señales normales de las anormales.\n",
    "\n",
    "En el caso de este algoritmo, asume que los datos presentados son el resultado de una combinación lineal de dos entradas y que ninguna de ellas tiene una distribución gaussiana. De no darse esta condición, los resultados no serán buenos o serán inconsistentes.\n",
    "\n",
    "Sin entrar demasiado a la parte matemática en la que se fundamenta, se debe tener en cuenta que los problemas derivados de dependencias no lineales o bien de tener una distribución gaussiana se pueden minimizar con el uso de la entropía en la formulación del algoritmo.\n",
    "En terminos generales el pseudo código de ICA se puede resumir como:\n",
    "```\n",
    "Inicializar W\n",
    "X = PCA(X)\n",
    "While W changes:\n",
    "      W = average(X*G(WX)) - average(g(WTX))W\n",
    "      W = orthogonalize(W)\n",
    "return S = WX\n",
    "```\n",
    "\n",
    "Donde $W$ es la matriz de pesos que permite hacer el cambio en los datos originales, $G$ es una matriz de Gentropía (cálculo de la difencia de entropía entre dos elementos) y $g$ es la derivada de la función anterior. La función `orthogonalize` hace referencia al proceso por el cual las columnas de una matriz se convierten en ortogonales. Por suerte una implementación mucho más rápida de este proceso puede encontrarse en `scikit-learn` con la función [FastICA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FastICA.html). A continuación veremos cómo aplicarlo a los datos anteriores:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aea532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "#Definir la función y ajustarla\n",
    "ica = FastICA(n_components=2)\n",
    "ica.fit(train_inputs)\n",
    "\n",
    "# Proceder con las transformaciones\n",
    "\n",
    "ica_train_inputs = ica.transform(train_inputs)\n",
    "ica_test_inputs = ica.transform(test_inputs)\n",
    "\n",
    "print(f\"Train Patterns{train_inputs.shape} -> {ica_train_inputs.shape}\")\n",
    "print(f\"Test Patterns{test_inputs.shape} -> {ica_test_inputs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cca8802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pinte los datos a continuación y observe las diferencias con PCA\n",
    "#TODO\n",
    "\n",
    "# Aplique un número distinto y compare los resultados de clasificación con las técnicas anteriores\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79597b5",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis (LDA)\n",
    "\n",
    "Este algoritmo de aprendizaje automático lineal es utilizado para la clasificación multiclase, si bien, en ocasiones también se utiliza como un algoritmo de reducción de la dimensionalidad. Cuidado con confundirlo con el \"Latent Dirichlet Allocation\" (LDA), que también es una técnica de reducción de la dimensionalidad pero solo aplicable a documentos de texto.\n",
    "\n",
    "LDA trata de separar (o discriminar) de la mejor manera posible las muestras del conjunto de datos de entrenamiento por su valor de clase. En concreto, el modelo trata de encontrar una combinación lineal de variables de entrada que consiga la máxima separación de las muestras entre clases (centroides o medias de las clases) y la mínima separación de las muestras dentro de cada clase. Por lo tanto, su mayor diferencia con PCA es que LDA tiene en cuenta la clase de salida, mientras que PCA es completamente agnóstico a este hecho.\n",
    "\n",
    "Dentro de la librería `scikit-learn`, la implementación del algoritmo nos permite usarlo para clasificar o bien para transformar los datos correspondientes. Véase el siguiente ejemplo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aff848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# Tenga en cuenta que el número de componentes cuando se reduce la dimensionalidad debe de ser\n",
    "# n_components <= min(n_classes - 1, n_features))  \n",
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "lda.fit(train_inputs, train_outputs)\n",
    "\n",
    "lda_train_inputs = lda.transform(train_inputs)\n",
    "lda_test_inputs = lda.transform(test_inputs)\n",
    "\n",
    "print(f\"Train Patterns{train_inputs.shape} -> {lda_train_inputs.shape}\")\n",
    "print(f\"Test Patterns{test_inputs.shape} -> {lda_test_inputs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510e7b59",
   "metadata": {},
   "source": [
    "Entre los parámetros más importantes de la función se pueden destacar:\n",
    "\n",
    "* `solver{‘svd’, ‘lsqr’, ‘eigen’}, default=’svd’` Indica qué algoritmo se utiliza para resolver la relación\n",
    " - ‘svd’: Singular value decomposition que no calcula la matriz de covarianza y, por lo tanto, es el indicado con una cantidad grande de características.\n",
    "\n",
    " - ‘lsqr’: Least squares solution. Método que puede usar otro de los parámetros llamado *shrinkage* o cálculos personalizados de la covarianza.\n",
    "\n",
    " - ‘eigen’: Eigenvalue decomposition. Pasa como con el método anterior, en este caso se calculan los autovalores de la matriz.\n",
    "* `n_components`, hace referencia al número de componentes que se usarán para la reducción de la dimensionalidad cuando se aplique el método `transform`. Tenga encuenta como restricción general $n\\_components <= min(n\\_classes - 1, n\\_features))$\n",
    "\n",
    "* `store_covariance (default=False)` Marca si se calculará la matriz de covarianza ponderada por clase para el estimador ‘svd’. En los otros casos siempre se calcula independientemente de este parámetro\n",
    "\n",
    "* `tol(default=1.0e-4)` Marca el límite de tolerancia para que una característica sea considerada significativa\n",
    "\n",
    "Para más detalle, por favor revisa la documentación en la página de [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9adbe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar otro problema con al menos 3 clases, reducir la dimensionalidad y representarla\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26d704e",
   "metadata": {},
   "source": [
    "## t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "Es una de las técnicas NO lineales que están enfocadas a la reducción de la dimensionalidad. Habitualmente se reserva a problemas de alta dimensionalidad. Mientras que PCA es una técnica lineal que busca maximizar la varianza a la vez que  preserva las grandes distancias entre pares (en otras palabras, las cosas que son diferentes acaban estando muy separadas), t-SNE está basada en la estadística y trata de paliar defectos de otras técnicas cuando se trata de explorar datos que tienen una estructura no lineal.\n",
    "![Comparativa de distribuciones](./Images/t-SNE.png) \n",
    "\n",
    "El algoritmo comienza calculando la probabilidad de similitud de los puntos en el espacio de alta dimensión (función Gaussiana) y calculando la probabilidad de similitud de los puntos en el espacio de baja dimensión (basado en una función de Cauchy). La similitud se calcula como la probabilidad condicional de que un punto $A$ elija como vecino al punto $B$ si los vecinos se eligen en proporción a su densidad de probabilidad bajo una gaussiana (distribución normal) centrada en $A$. A continuación, se minimiza la diferencia entre estas probabilidades condicionales en el espacio de mayor y menor dimensión para una representación de los puntos de datos en el espacio de menor dimensión. Para medir la minimización de la suma de la diferencia de probabilidad condicional, t-SNE minimiza la suma de la **divergencia de Kullback-Leibler** de los puntos de datos globales utilizando un método de descenso de gradiente. Esta función estadística asimétrica también utilizada en las *Generative Adversarial Networks(GAN)* o en los *Variational Auto Encoders(VAE)* permite optimización la distancia entre las distribuciones probabilísticas. \n",
    "\n",
    "En términos generales, t-SNE procura minimizar la divergencia entre dos distribuciones, la primera, una distribución que mide las similitudes por pares de los objetos de entrada y, la segunda, una distribución que mide las similitudes por pares de los correspondientes puntos en el espacio de baja dimensionalidad.\n",
    "\n",
    "Un punto a destacar es que como simplificación de las distribución de Cauchy, esta se suele simplificar a un único grado de libertad lo que da como resultado una distribución de t-student, de ahí el nombre de la transformación.\n",
    "\n",
    "El otro punto a destacar es que, después de este proceso, las características de entrada ya no son identificables, y no se puede hacer ninguna inferencia basada únicamente en el resultado de t-SNE. De ahí que sea principalmente una técnica de exploración y visualización de datos.\n",
    "\n",
    "\n",
    "Para emplearlo se puede hacer uso de la correspondiente librería de `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2682e83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "tsne_train_inputs = tsne.fit_transform(train_inputs) \n",
    "tsne_test_inputs = tsne.fit_transform(test_inputs)\n",
    "\n",
    "print(f\"Train Patterns{train_inputs.shape} -> {tsne_train_inputs.shape}\")\n",
    "print(f\"Test Patterns{test_inputs.shape} -> {tsne_test_inputs.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29252c0",
   "metadata": {},
   "source": [
    "Uno de los elementos que destacan del código anterior es que, como la transformación de t-SNE no puede ser almacenada al no ser supervisada, no puede aplicarse dicha transformación posteriormente a otro conjunto. A mayores, como no tienen sentido más que gráfico y no se pueden/deben usar como entrada de un método de entrenamiento, no dispone de la llamada a la función `transform`.\n",
    "\n",
    "Cabe destacar que, en cuanto al uso de la implementación de t-SNE, se recomienda que para su uso con espacios de alta dimensionalidad (>50) o de tipo difuso (*sparse*) se aplique, previamente, otro tipo de técnica de reducción de la dimensionalidad como , por ejemplo, PCA. Posteriormente, se aplicaría t-SNE sobre el resultado de dicha técnica de reducción de la dimensionalidad. \n",
    "\n",
    "Entre los parámetros más importantes que se le pueden pasar a t-SNE cabe destacar los siguientes:\n",
    "\n",
    "* `n_components (default: 2)`: Dimensiones a las que se quiere comprimir el resultado.\n",
    "* `perplexity (default: 30)`: este parámetro está relacionado con la vecindad usada por las particiones del algoritmo. Los valores habituales están en el intervalo $[5,50]$\n",
    "* `early_exaggeration (default: 12.0)`: Controla como de ajustadas están las clases en el espacio de reducción y, por lo tanto, el hueco esperable entre ellas.\n",
    "* `learning_rate (default: 200.0)`: como su nombre indica es el parámetro de entrenamiento que controla el aprendizaje por descenso de gradiente. En términos generales se suele usar un valor que esté en el rango $[10.0, 1000.0]$.\n",
    "* `n_iter (default: 1000)`: Número máximo de iteraciones aplicadas al algoritmo de optimización. En la documentación de la librería se recomienda fijar un número igual o superior a 250.\n",
    "* `method (default: ‘barnes_hut’)`: El método a ser aplicado. Por defecto se usa la aproximación de Barnes-Hut ya que esta tiene un coste computacional de $O(N log(N))$. Alternativamente, aunque más lento ya que tiene una complejidad $O(N^2)$, se puede usar el método ’exact’ que como su nombre indica no hace asunciones o aproximaciones si no que es el valor exacto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c4bec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir la representación de los patrones\n",
    "#TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0970605a",
   "metadata": {},
   "source": [
    "## Ejercicio\n",
    "\n",
    "Carge el problema [fashion-NMIST](https://github.com/zalandoresearch/fashion-mnist) y aplique las técnicas *PCA*, *LDA* y *t-SNE*. Analize/Represnte las diferencias en el resultado de manera gráfica. Puede usar para dicha representación la función `draw_results` definida anteriormente en este tutorial para así representar , de manera sencilla, las 10 clases de que consta este problema de clasificación.\n",
    "\n",
    "El problema en cuestión cuenta con 70.000 imágenes de 10 tipos de ropa diferente. Cada clase está representada por 7.000 imágenes. Las clases que se contemplan, tal y como se encuentran descritas en el documento del problema, son:\n",
    "\n",
    "0. T-shirt/top\n",
    "1. Trouser\n",
    "2. Pullover\n",
    "3. Dress\n",
    "4. Coat\n",
    "5. Sandal\n",
    "6. Shirt\n",
    "7. Sneaker\n",
    "8. Bag\n",
    "9. Ankle boot\n",
    "\n",
    "Los patrones son imágenes de $28\\times28$ píxeles con la imagen centrada. Estos patrones han sido divididos siguiendo un esquema _hold-out_ con 60.000 imágenes para entrenamiento y 10.000 para test. \n",
    "\n",
    "En este caso, sólo vamos a utilizar las imágenes de entrenamiento y, dependiendo de las capacidades del sistema, puede que sea necesario reducir el número de estas a las que se le aplicarán las técnicas de reducción para poder hacerlo en un tiempo asumible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161dc40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(path_str, pattern_sz=(28,28), kind='train'):\n",
    "    from pathlib import Path\n",
    "    import gzip\n",
    "    import numpy as np\n",
    "\n",
    "    \"\"\"Cargar problemas de tipo NMIST con imagenes y etiquetas\"\"\"\n",
    "    path = Path(path_str)\n",
    "    labels_path = path / f'{kind}-labels-idx1-ubyte.gz'\n",
    "    images_path = path / f'{kind}-images-idx3-ubyte.gz'\n",
    "\n",
    "    with gzip.open(str(labels_path.absolute()), 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "        \n",
    "    with gzip.open(str(images_path.absolute()), 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(labels), pattern_sz[0]*pattern_sz[1])\n",
    "        \n",
    "    return images, labels\n",
    "\n",
    "#TODO\n",
    "path_descarga = 'Ajustar segun donde se hayan descargado los ficheros'\n",
    "\n",
    "X_train, y_train = load_mnist(path_descarga, kind='train')\n",
    "\n",
    "#En caso de que las capacidades de la máquina sean limitadas, reducir el número de patrones de 60.000 a 20.000\n",
    "\n",
    "#TODO en caso de necesidad\n",
    "\n",
    "#Comprobar que se tienen las 10 clases en al submuestra seleccionada en otro caso barajar \n",
    "#y seleccionar un conjunto estratificado\n",
    "#TODO\n",
    "\n",
    "#realizar la reducción con PCA, LDA y t-SNE \n",
    "#TODO\n",
    "\n",
    "#Pintar los resultados\n",
    "#TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
