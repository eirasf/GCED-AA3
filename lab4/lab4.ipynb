{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "564b4ae7",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eirasf/GCED-AA3/blob/main/lab4/lab4.ipynb)\n",
    "\n",
    "# Lab4: Aprendizaje por refuerzo - Programación dinámica\n",
    "\n",
    "En este laboratorio nos familizarizaremos con [Gym](https://www.gymlibrary.dev/), una librería de Python para simular problemas que se pueden resolver utilizando aprendizaje por refuerzo.\n",
    "Además, desarrollaremos algunos métodos de control basados en programación dinámica.\n",
    "\n",
    "## Gym\n",
    "Gym es una herramienta para desarrollar y comparar algoritmos de aprendizaje por refuerzo. Facilita la simulación de interacciones de un agente con entornos muy diversos.\n",
    "\n",
    "Para utilizar OpenGym, el primer paso es importar la librería. Una vez hecho, podremos crear un entorno que nos permita simular el problema que deseemos de entre los muchos disponibles.\n",
    "\n",
    "En esta práctica utilizaremos un entorno muy sencillo que simula un *GridWorld*, es decir, un mundo compuesto por casillas por las que el agente podrá desplazarse. En particular, cargaremos el entorno `MiniGrid-Empty-5x5-v0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d6beda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si los paquetes no están instalados, hay que ejecutar estas líneas:\n",
    "#!pip install gymnasium[classic-control]\n",
    "#!pip install minigrid \n",
    "import gymnasium as gym\n",
    "import minigrid\n",
    "import numpy as np\n",
    "env = gym.make('MiniGrid-Empty-5x5-v0', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb86d8c4",
   "metadata": {},
   "source": [
    "El método `gym.make` devuelve un objeto de tipo entorno que nos ofrece, entre otros, los siguientes métodos y propiedades:\n",
    " - `reset`: Devuelve el entorno a su estado original\n",
    " - `actions`: Muestra una lista de las acciones disponibles\n",
    " - `max_steps`: Fija el número máximo de acciones que puede realizar el agente en cada episodio\n",
    " - `render`: Devuelve una imagen donde aparece representada la situación actual\n",
    " - `step(accion)`: Ejecuta una acción y actualiza el entorno en consecuencia\n",
    " \n",
    "El objetivo del agente en este problema es alcanzar la meta, representada por una casilla verde. El agente aparece representado como una flecha roja que refleja su posición y orientación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc579aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra las acciones disponibles\n",
    "acciones = env.get_wrapper_attr('actions')\n",
    "print([a.name for a in acciones])\n",
    "\n",
    "# Resetea el entorno\n",
    "env.reset()\n",
    "\n",
    "# Muestra el entorno en su estado inicial\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def muestra_entorno(env:gym.Env) -> None:\n",
    "    im = plt.imshow(env.render())\n",
    "    plt.show()\n",
    "muestra_entorno(env)\n",
    "\n",
    "# TODO: Ejecuta la acción forward (puedes referirte a la acción i-ésima como acciones(i) o como acciones.nombre) y muestra el entorno de nuevo\n",
    "...\n",
    "\n",
    "# TODO: Prueba a ver cómo afectan las distintas acciones al entorno\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d3d8cf",
   "metadata": {},
   "source": [
    "El método [`step(acción)`](https://gymnasium.farama.org/api/env/#gymnasium.Env.step) devuelve cinco valores:\n",
    " - Observación: Representa lo que puede percibir el agente del entorno en su estado actual\n",
    " - Recompensa: Indica el valor de la señal de recompensa para la ejecución de la acción aplicada en el estado en que se aplicó.\n",
    " - Terminated: Valor booleano que indica si se ha terminado el episodio\n",
    " - Truncated: Valor booleano que indica si el episodio ha alcanzado el número máximo de pasos\n",
    " - Info: Información adicional\n",
    " \n",
    "De cara a aprender una política, debemos buscar una manera de representar los estados. Habitualmente, el agente se basaría en sus observaciones para generar una representación del estado. Sin embargo, para este problema en particular, vamos a utilizar una representación del estado más sencilla que la observación que puede hacer el agente. Esta labor de representación del estado la llevaría a cabo el **intérprete** en el esquema habitual de un problema de aprendizaje por refuerzo.\n",
    "![Esquema de aprendizaje por refuerzo](img/Reinforcement_learning_diagram.svg)\n",
    "\n",
    "El intérprete identificará el estado con un vector de tres componentes que indicarán, respectivamente, la columna, fila y orientación del agente. Las filas/columnas se numerarán del 0 al 2 (de arriba a abajo y derecha a izquierda) y la orientación podrá tomar los siguientes valores:\n",
    " - 0 $\\rightarrow$ derecha\n",
    " - 1 $\\rightarrow$ abajo\n",
    " - 2 $\\rightarrow$ izquierda\n",
    " - 3 $\\rightarrow$ arriba\n",
    " \n",
    "El entorno de este problema nos da acceso directo a dos variables que serán de utilidad:\n",
    " - `agent_pos`: Indica la casilla en la que está el agente. Las casillas del tablero están numeradas del 1 al 3, por lo que será necesario adaptar dicha numeración.\n",
    " - `agent_dir`: Indica la orientación del agente usando el código descrito en el párrafo anterior.\n",
    " \n",
    " Puedes acceder a estos valores como `env.get_wrapper_attr('agent_pos')` y `env.get_wrapper_attr('agent_dir')` respectivamente.\n",
    "Creemos una función que haga las labores de intérprete y devuelva la codificación del estado actual del entorno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed261f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "# Representaremos los estados como tuplas con los siguientes campos con nombre:\n",
    "#  - x: Representa la columna en que se encuentra el agente. La columna visitable más a la izquierda será la 0\n",
    "#  - y: Representa la fila en que se encuentra el agente. La fila visitable más arriba será la 0\n",
    "#  - dir: Representa la dirección en que apunta el agente. Respetaremos el código numérico indicado por agent_dir\n",
    "Estado = namedtuple('Estado', ['x', 'y', 'dir'])\n",
    "\n",
    "def get_estado(env:gym.Env) -> Estado:\n",
    "    # TODO - Completa la función\n",
    "    ...\n",
    "\n",
    "env.reset()\n",
    "\n",
    "# COMPROBACIÓN\n",
    "estado_actual = get_estado(env)\n",
    "assert estado_actual.x == 0, f'El estado inmediatamente después de resetear debe indicar x=0 y el tuyo indica {estado_actual.x}'\n",
    "assert estado_actual.y == 0, f'El estado inmediatamente después de resetear debe indicar y=0 y el tuyo indica {estado_actual.y}'\n",
    "assert estado_actual.dir == 0, f'El estado inmediatamente después de resetear debe indicar dir=0 y el tuyo indica {estado_actual.dir}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b41aec0",
   "metadata": {},
   "source": [
    "### Simulación de un episodio con política aleatoria\n",
    "\n",
    "Ahora que sabemos manejar el entorno, vamos a probar qué tal funciona una política aleatoria.\n",
    "\n",
    "Crea un bucle que simule un episodio completo siguiendo una política aleatoria, es decir, que aplique una acción aleatoria hasta que el entorno nos indique que el episodio ha acabado. Puedes obtener una acción aleatoria llamando a `env.action_space.sample()`.\n",
    "\n",
    "Muestra el entorno tras aplicar cada acción. Además muestra un mensaje que indique:\n",
    " - Qué número de paso se ha ejecutado\n",
    " - El nombre de la acción aplicada\n",
    " - El estado resultante\n",
    " - La recompensa obtenida en ese paso\n",
    " - Si ha terminado la ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee2d09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos el límite de pasos por episodio. Solo acabará el episodio al alcanzar la meta.\n",
    "env.max_steps = float('inf')\n",
    "print(f'Max steps modificado: {env.max_steps}')\n",
    "\n",
    "\n",
    "completado = False\n",
    "contador = 0\n",
    "while not completado:\n",
    "    # TODO - Completa el bucle\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6575e43b",
   "metadata": {},
   "source": [
    "### Simplificación del espacio de acciones y definición de políticas\n",
    "Habrás comprobado que hay acciones que no son útiles para este problema. Vamos a simplificar la búsqueda reduciendo el espacio de acciones a tres:\n",
    " 1. left\n",
    " 2. right\n",
    " 3. forward\n",
    " \n",
    "Al hacer esto, ya no podemos generar una acción aleatoria utilizando `env.action_space.sample()`. Además, dicha función elige una acción al azar, lo cual no nos interesa. Nuestro objetivo es obtener una **política** que determine la mejor acción en cada situación, así que vamos a aprovechar el momento para definir una estructura de datos que nos permita hacer eso.\n",
    "\n",
    "La política debe indicar una probabilidad $\\pi(a|s)$ de elegir la acción $a$ estando en el estado $s$ (así que $\\sum_{a'}\\pi(a'|s)=1$, con $s\\in \\mathcal{S}$ y $a\\in \\mathcal{A}$). Declararemos una política como un `numpy.array` que asocie a cada estado $s$ una distribución de probabilidad sobre las distintas acciones.\n",
    "\n",
    "La política aleatoria otorgará la misma probabilidad a todas las acciones para cada estado (como hay tres acciones, para cualquier par estado-acción la probabilidad será 1/3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df80203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos solo las tres acciones indicadas\n",
    "ACCIONES_UTILES = [acciones.left, acciones.right, acciones.forward]\n",
    "\n",
    "\n",
    "# TODO - Indica el shape de la policy\n",
    "politica_aleatoria = np.zeros(...)\n",
    "# Inicializamos todos los valores a 1/3\n",
    "politica_aleatoria[:] = 1.0/3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc343375",
   "metadata": {},
   "source": [
    "Una vez hemos definido la política, podemos definir una función para sustituir a `env.action_space.sample()`. Esta función recibirá la política que debe seguir y el estado para el que seleccionar la acción. Devolverá la acción muestreada siguiendo la distribución descrita por la política para ese estado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950cd84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def muestrea_politica(estado:Estado, politica:np.ndarray) -> int:\n",
    "    # Obtenemos la distribución de probabilidad\n",
    "    probs = politica[estado.x, estado.y, estado.dir, :]\n",
    "    # Comprobación de que la política está bien (se puede comentar cuando estemos seguros de que es así)\n",
    "    np.testing.assert_almost_equal(np.sum(probs), 1.0, err_msg='La política para un estado siempre debe sumar 1')\n",
    "    # Devolvemos una acción aleatoria muestreada según las probabilidades indicadas por la política\n",
    "    return np.random.choice(ACCIONES_UTILES, p=probs)\n",
    "    \n",
    "# TODO - Obtén la acción muestreada de la policy_aleatoria para el estado en que el agente está en la casilla central mirando hacia abajo\n",
    "accion = ...\n",
    "\n",
    "print(accion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57eac4de",
   "metadata": {},
   "source": [
    "### Simulación de episodios siguiendo una política prefijada\n",
    "Al disponer ya de una definición para políticas y de una función de muestreo sobre cualquier política, podemos adaptar nuestro bucle que simula un episodio para que lo haga siguiendo una política concreta.\n",
    "\n",
    "Definiremos una función llamada `simula_episodio` para ello. La función debe recibir la política a seguir y dos parámetros que nos indicarán, respectivamente, si se mostrarán los *renders* de los pasos del episodio y si se mostrarán mensajes de texto.\n",
    "\n",
    "La función deberá devolver una tupla con el **retorno** obtenido, es decir, la suma de las recompensas recibidas en cada paso y el **número de pasos** necesarios para completar el episodio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672c6904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simula_episodio(politica:np.ndarray, muestra_renders:bool=False, imprime:bool=False) -> tuple[float, int]:\n",
    "    env.reset()\n",
    "    # TODO - Adapta el bucle que escribiste 3 celdas de código más arriba para que use la política para decidir qué acción tomar en cada paso\n",
    "    ...\n",
    "    return (ret, contador)\n",
    "\n",
    "retorno, num_pasos = simula_episodio(politica_aleatoria, muestra_renders=False, imprime=True)\n",
    "print(f'Simulado un episodio con retorno {retorno} ({num_pasos})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439fd175",
   "metadata": {},
   "source": [
    "### Comprobación del rendimiento de la política\n",
    "\n",
    "Hagamos un experimento para evaluar la eficacia de la política aleatoria. Simularemos 200 episodios, anotando el número de pasos necesarios para completar cada uno. No es necesario que llevemos cuenta de los retornos obtenidos porque en este problema siempre será 1 (dado que solo la última acción da recompensa, con valor 1).\n",
    "\n",
    "Mostraremos estadísticas y gráficos que nos puedan ayudar a tener una idea de cómo de bien funciona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f3324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprueba_politica(politica:np.ndarray) -> None:\n",
    "    episodios_pasos = []\n",
    "    \n",
    "    # TODO - Simula 200 episodios, añadiendo sus respectivos números de pasos a episodios_pasos y mostrando por cada uno un mensaje de la forma '{iteracion} - Simulado un episodio con retorno {retorno} ({num_pasos})'\n",
    "    ...\n",
    "\n",
    "    assert(len(episodios_pasos)==200)\n",
    "    \n",
    "    # Mostramos un histograma para la duración de los episodios\n",
    "    plt.hist(episodios_pasos)\n",
    "    plt.show()\n",
    "\n",
    "    # Mostramos también un diagrama de caja\n",
    "    plt.boxplot(episodios_pasos)\n",
    "    plt.show()\n",
    "\n",
    "    # Por último, escribimos un par de estadísticas\n",
    "    print('Se han necesitado de media',np.mean(episodios_pasos),'pasos (+-',np.std(episodios_pasos),')')\n",
    "    print('El episodio más corto duró',np.min(episodios_pasos),'pasos y el episodio más largo duró',np.max(episodios_pasos),'pasos')\n",
    "    \n",
    "comprueba_politica(politica_aleatoria)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd32241d",
   "metadata": {},
   "source": [
    "# Obtención de políticas óptimas\n",
    "## Métodos de programación dinámica\n",
    "\n",
    "A la hora de encontrar una política óptima que guíe las acciones de nuestro agente, los métodos basados en programación dinámica son una buena opción cuando disponemos de conocimento respecto a las dinámicas del entorno.\n",
    "\n",
    "En particular, estos métodos requieren conocer $p(s',r | s,a)$, es decir, la probabilidad de acabar en un estado $s'$ recibiendo una recompensa $r$ si aplicamos la acción $a$ al estado $s$.\n",
    "\n",
    "OpenGym no nos proporciona esta información, pero en este problema sencillo podemos reconstruirla. Este entorno es determinista, lo que quiere decir que si aplicamos la acción $a$ al estado $s$ siempre obtendremos el mismo estado $s'$ y la misma recompensa $r$ con probabilidad 1 (cualquier otro estado $s_j$ o recompensa $r_j$ tendrán $p(s_j,r_j|s,a)=0$).\n",
    "\n",
    "Podemos, por tanto, representar el modelo con dos variables:\n",
    " - `modelo_transiciones`: Para cada combinación $s,a$, almacena el estado $s'$ al que conduce aplicar la acción $a$ a $s$.\n",
    " - `modelo_recompensas`: Para cada combinación $s,a$, almacena el valor de la recompensa obtenida al aplicar la acción $a$ a $s$.\n",
    " \n",
    "Sabiendo cómo se comporta el entorno, definimos estas dos variables con los valores adecuados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb998cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINICIÓN DEL MODELO\n",
    "\n",
    "# Modelo de transiciones - Almacena a qué estado lleva aplicar cada acción a cada estado\n",
    "# Lo almacenamos en un array numpy en el que para cada estado almacenemos, para cada acción, el estado al que lleva.\n",
    "modelo_transiciones = np.zeros((3,3,4,3,3),dtype=np.int16)\n",
    "# Recorremos cada estado para completar el estado al que lleva cada una de las tres acciones posibles\n",
    "for i in range(3): # Columnas (x)\n",
    "    for j in range(3): # Filas (y)\n",
    "        for k in range(4): # Orientaciones (dir)\n",
    "            # Aplicar la acción left gira el agente a la izquierda, así que el estado resultante tendrá el mismo valor para filas y columnas pero variará la orientación\n",
    "            modelo_transiciones[i,j,k,0]=[i,j,(k+4-1)%4]\n",
    "            # Aplicar la acción right gira el agente a la derecha, así que el estado resultante tendrá el mismo valor para filas y columnas pero variará la orientación\n",
    "            modelo_transiciones[i,j,k,1]=[i,j,(k+1)%4]\n",
    "        # Aplicar la acción forward mantiene la orientación, pero cambia filas o columnas en función de hacia dónde esté orientado\n",
    "        if i<2:\n",
    "            modelo_transiciones[i,j,0,2]=[i+1,j,0] # Derecha\n",
    "        if j<2:\n",
    "            modelo_transiciones[i,j,1,2]=[i,j+1,1] # Abajo\n",
    "        if i>0:\n",
    "            modelo_transiciones[i,j,2,2]=[i-1,j,2] # Izquierda\n",
    "        if j>0:\n",
    "            modelo_transiciones[i,j,3,2]=[i,j-1,3] # Arriba\n",
    "\n",
    "# Modelo de transiciones - Almacena qué recompensa proporciona aplicar cada acción a cada estado\n",
    "# Lo almacenamos en un array numpy en el que para cada estado almacenemos, para cada acción, la recompensa.\n",
    "modelo_recompensas = np.zeros((3,3,4,3))\n",
    "# Será 0 siempre menos en dos casos:\n",
    "# 1 - Está en la segunda columna de la tercera fila, orientado a la derecha y se usa la acción forward\n",
    "modelo_recompensas[1,2,0,2] = 1\n",
    "# 2 - Está en la tercera columna de la segunda fila, orientado hacia abajo y se usa la acción forward\n",
    "modelo_recompensas[2,1,1,2] = 1\n",
    "\n",
    "# Función auxiliar para consultar los estados a los que lleva cada acción para un estado dado\n",
    "def get_resultados_por_accion_en(estado: Estado) -> list[Estado]:\n",
    "    valores_estados_para_cada_accion = modelo_transiciones[estado.x, estado.y, estado.dir]\n",
    "    return list(map(lambda x: Estado(*x), valores_estados_para_cada_accion))\n",
    "\n",
    "# Función auxiliar para consultar las recompensas de aplicar cada acción en un estado dado\n",
    "def get_recompensas_por_accion_en(estado: Estado) -> np.ndarray:\n",
    "    # TODO - Completa la función\n",
    "    return modelo_recompensas[estado.x, estado.y, estado.dir]\n",
    "\n",
    "# TODO - Muestra los estados resultantes y las recompensas de aplicar las distintas acciones cuando el agente está en la tercera casilla de la segunda fila, orientado hacia abajo y se ejecuta la acción forward\n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8291dfd",
   "metadata": {},
   "source": [
    "## Algoritmo 1: Evaluación de políticas iterativa\n",
    "El algoritmo de evaluación de políticas iterativa calcula el valor $v_\\pi(s)$ para todo estado $s$ siguiendo la política $\\pi$. La fórmula que utiliza, que está basada en la ecuación de Bellman para $v_\\pi(s)$, es esta:\n",
    "\n",
    "$$v_\\pi(s)=\\mathbb{E}_\\pi [R_{t+1}+\\gamma v_k(S_{t+1}) | S_t=s]$$\n",
    "\n",
    "$$=\\sum_a\\pi(a|s)\\sum_{s',r}p(s',r|s,a)[r+\\gamma v_k(s')]$$\n",
    "\n",
    "Al ser este un problema determinista, la expresión se simplifica, dado que $p(s',r|s,a)$ será 0 en todos los casos menos en 1 (porque aplicar la acción $a$ al estado $s$ conduce siempre a un estado $s'$ fijo y único y otorga siempre una recompensa $r$ fija y única; ambos valores los podemos obtener del modelo).\n",
    "\n",
    "Puedes ver el algoritmo al completo aquí (y un ejemplo en la transparencia 19 del Tema 3 de teoría):\n",
    "![Evaluación de políticas iterativa](./img/iterative-policy-evaluation.png)\n",
    "\n",
    "Implementemos el algoritmo para obtener los valores asignados a cada estado siguiendo una política fija.\n",
    "\n",
    "**CONSEJO: Para asegurar la convergencia, actualiza todos los valores de $V_{t+1}(s)$ a partir de los valores $V_t(s)$, es decir, crea una variable `nuevos_valores_estados` donde almacenes todos los calculados en una iteración y al completar la iteración establece `valores_estados = nuevos_valores_estados`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9317cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.1\n",
    "\n",
    "def iterative_policy_evaluation(politica: np.ndarray) ->np.ndarray:\n",
    "    THETA = 0.00001\n",
    "    \n",
    "    # En esta variable almacenaremos los valores calculados para los estados\n",
    "    # TODO - Indica el shape adecuado. Debes almacenar un valor por cada estado posible\n",
    "    valores_estados = np.zeros(...)\n",
    "    \n",
    "    # TODO - Completa el algoritmo según lo descrito arriba\n",
    "    ...\n",
    "    return valores_estados\n",
    "\n",
    "valores_estados = iterative_policy_evaluation(politica_aleatoria)\n",
    "print(valores_estados)\n",
    "\n",
    "# COMPROBACIÓN\n",
    "np.testing.assert_almost_equal(valores_estados[0,0,1], 4.11522634e-07)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3f935d",
   "metadata": {},
   "source": [
    "### Optimización de políticas\n",
    "\n",
    "En el apartado anterior hemos obtenido un valor para cada estado del problema cuando se sigue la política $\\pi$. Según lo visto en teoría, podemos utilizar estos valores para encontrar una nueva política $\\pi'$ que sea igual o mejor que $\\pi$.\n",
    "\n",
    "Cuando disponemos de $v_\\pi(s)$ y del modelo que describe el funcionamiento del entorno, podemos obtener una política $\\pi'\\geq\\pi$ simplemente seleccionando, para cada estado, la acción que nos da un mayor retorno, que podemos calcular como la recompensa inmediata más el valor del estado al que llegamos (descontado por $\\gamma$).\n",
    "\n",
    "$$\\pi'(s)=\\arg\\max_a\\sum_{s',r}p(s',r|s,a)\\left[r+\\gamma v_\\pi(s')\\right]$$\n",
    "\n",
    "Escribamos un algoritmo que obtenga $\\pi'$ a partir de `state_values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598f46dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crea_politica_avara(valores_estados: np.ndarray) -> np.ndarray:\n",
    "    nueva_shape = list(valores_estados.shape)\n",
    "    nueva_shape.append(3)\n",
    "    # TODO - Indica la shape apropiada. Por cada par estado-acción debemos almacenar la probabilidad que da esta política de tomar esa acción estando en dicho estado\n",
    "    # Como la política es determinista, para un estado concreto todas las acciones tendrán probabilidad 0 menos una que tendrá probabilidad 1\n",
    "    politica = np.zeros(...)\n",
    "    # TODO - Haz bucles anidados para recorrer todos los estados\n",
    "    ...\n",
    "                # Ahora para este estado debemos calcular, para todas las acciones a, G=(r + GAMMA*state_values[s[0],s[1],s[2]]) donde r es la recompensa de aplicar a y s es el estado resultante.\n",
    "                # De todos esos valores, tomaremos el mejor (supongamos que es el i-ésimo) y pondremos probabilidad 1 en la acción i-ésima para dicho estado.\n",
    "                # TODO - Recorre las acciones, calculando su G. Selecciona el mejor y pon a 1 la probabilidad de la acción correspondiente.\n",
    "                ...\n",
    "    return politica\n",
    "\n",
    "politica_mejorada = crea_politica_avara(valores_estados)\n",
    "\n",
    "# COMPROBACIÓN\n",
    "assert(np.array_equal(politica_mejorada[2,1,1],[0., 0., 1.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f4bd55",
   "metadata": {},
   "source": [
    "## Obtención de la política óptima - Policy iteration\n",
    "Ahora que podemos evaluar una política y, a partir de esos valores, obtener una política mejorada, podemos repetir el proceso para seguir mejorando nuestra política hasta que no cambie. Este procedimiento se denomina *policy iteration* y lo tienes aquí descrito:\n",
    "\n",
    "![Policy iteration](./img/policy-iteration.png)\n",
    "\n",
    "Implementémoslo (la mayoría ya lo tenemos hecho)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7c5980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(politica_inicial: np.ndarray) -> np.ndarray:\n",
    "    politica_actual = politica_inicial\n",
    "    politica_estable = False\n",
    "    # Llevaremos cuenta del paso en que estamos\n",
    "    paso = 0\n",
    "    while not politica_estable:\n",
    "        # TODO - Calcula los valores para current_policy\n",
    "        valores_actuales = ...\n",
    "        # TODO - Obtén la nueva política mejorada a partir de los valores calculados\n",
    "        nueva_politica = ...\n",
    "        # Comprobamos si la política ha cambiado\n",
    "        politica_estable = np.array_equal(politica_actual, nueva_politica)\n",
    "        # Preparamos la siguiente iteración\n",
    "        politica_actual = nueva_politica\n",
    "        paso+=1\n",
    "        print('Paso #',paso)\n",
    "    # Al terminar el bucle habremos obtenido una política que ya no mejora\n",
    "    return politica_actual\n",
    "        \n",
    "politica_optima = policy_iteration(politica_aleatoria)\n",
    "print(politica_optima)\n",
    "\n",
    "# COMPROBACIÓN\n",
    "assert(np.array_equal(politica_optima[2,0,1], [0., 0., 1.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df354bd",
   "metadata": {},
   "source": [
    "Comprobemos que la política obtenida funciona bien simulando un episodio siguiéndola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcb8b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "simula_episodio(politica_optima, muestra_renders=True, imprime=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b72ff7",
   "metadata": {},
   "source": [
    "## Obtención de la política óptima - Iteración de valores\n",
    "El proceso de iteración de políticas es costoso porque requiere hacer una estimación iterativa de $v_\\pi(s)$ para todas las políticas por las que se pasa. Este proceso se puede simplificar y obtener una política óptima siguiendo el algoritmo de iteración de valores:\n",
    "\n",
    "![Value iteration](./img/value-iteration.png)\n",
    "\n",
    "El algoritmo es muy similar al de evaluación de políticas iterativas. La diferencia es que, en lugar de calcular $\\mathbb{E}_\\pi [R_{t+1}+\\gamma v_k(S_{t+1}) | S_t=s]$, se toma el valor de la mejor acción. Al final, se devuelve la política greedy derivada de los valores calculados. Implementémoslo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4927b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration() -> np.ndarray: # No recibe ningún parámetro. Calcula valores y política a partir del modelo del entorno.\n",
    "    # TODO - Repite el algoritmo de evaluación de políticas iterativa pero tomando el G de la mejor acción para cada estado\n",
    "    # Una vez tengas los state_values, devuelve la política derivada de ellos\n",
    "    ...\n",
    "\n",
    "otra_optima = value_iteration()\n",
    "print(otra_optima)\n",
    "\n",
    "# COMPROBACIÓN\n",
    "assert(np.array_equal(otra_optima[2,0,1], [0., 0., 1.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e717640",
   "metadata": {},
   "source": [
    "Para terminar, comprobemos que la política obtenida así también funciona bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf58dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "simula_episodio(otra_optima, muestra_renders=True, imprime=True)\n",
    "env.close() # Cerramos, además, el entorno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2793887a",
   "metadata": {},
   "source": [
    "# ¡Enhorabuena!\n",
    "Has terminado este laboratorio. Ahora sabes cómo interactuar con OpenGym y cómo funcionan los principales algoritmos de programación dinámica para aprendizaje por refuerzo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
