{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56d76dc7",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eirasf/GCED-AA3/blob/main/lab6/lab6.ipynb)\n",
    "\n",
    "# Lab6: Aprendizaje por refuerzo - M√©todos de Temporal Difference\n",
    "\n",
    "En este laboratorio seguiremos explorando m√©todos tabulares de aprendizaje por refuerzo que no necesitan disponer de un modelo. En particular, estudiaremos los m√©todos de **Temporal Difference**.\n",
    "\n",
    "Para explorar estos m√©todos volveremos a utilizar [Gym](https://www.gymlibrary.dev/) con un entorno GridWorld, es decir, un tablero con casillas por las que se mueve el agente, tal como hicimos en el Lab4. En particular, vamos a utilizar el entorno `MiniGrid-DistShift1-v0`. En este entorno el agente deber√° llegar hasta la meta, pero esta vez evitando caer en la lava, que hace que se termine el episodio con recompensa 0.\n",
    "\n",
    "Carguemos el entorno y explor√©moslo brevemente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f802f26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si los paquetes no est√°n instalados, hay que ejecutar estas l√≠neas:\n",
    "#!pip install gymnasium[classic-control]\n",
    "#!pip install minigrid \n",
    "import gymnasium as gym\n",
    "import minigrid\n",
    "import numpy as np\n",
    "env = gym.make('MiniGrid-DistShift1-v0', render_mode='rgb_array')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TODO - Muestra el entorno para ver c√≥mo es el tablero. Para ello, recupera la funci√≥n muestra_entorno del lab4 y l√°nzala\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29f72b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Anotar las dimensiones del tablero para poder utilizarlas despu√©s\n",
    "NUM_COLUMNAS = ...\n",
    "NUM_FILAS = ...\n",
    "NUM_ORIENTACIONES = 4\n",
    "\n",
    "# Las acciones son las mismas que en el laboratorio 4. Nos quedaremos de nuevo solo con LEFT, RIGHT y FORWARD\n",
    "acciones = env.actions\n",
    "# Seleccionamos solo las tres acciones indicadas\n",
    "ACCIONES_UTILES = [acciones.left, acciones.right, acciones.forward]\n",
    "NUM_ACCIONES = len(ACCIONES_UTILES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f62fcd9",
   "metadata": {},
   "source": [
    "Comprueba el efecto de caer en la lava. El episodio deber√≠a terminarse y recibiendo el agente una recompensa de 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78367b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - Efect√∫a dos acciones para que el agente caiga en la lava y verifica que se recibe una recompensa de 0\n",
    "obs, reward, terminated, truncated, info = ...\n",
    "obs, reward, terminated, truncated, info = ...\n",
    "\n",
    "#COMPROBACIONES\n",
    "assert(reward==0)\n",
    "assert(terminated)\n",
    "_ = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dd4104",
   "metadata": {},
   "source": [
    "# Obtenci√≥n de pol√≠ticas √≥ptimas\n",
    "\n",
    "## M√©todos de Temporal Difference\n",
    "\n",
    "Los m√©todos basados en **Temporal Difference**, al igual que los m√©todos Montecarlo, carecen de informaci√≥n respecto a c√≥mo funciona el modelo (no conocen $p(s',r | s,a)$). Por ello, tambi√©n deben interactuar con el entorno para obtener muestras con las que despu√©s hacer estimaciones.\n",
    "\n",
    "A diferencia de los m√©todos Montecarlo, los m√©todos de Temporal Difference no esperan a terminar un episodio para actualizar los valores de los estados transitados. En su lugar, los m√©todos Temporal Difference aprovechan la idea de que el valor $v(S)$ de un estado $S$ tiene relaci√≥n con el valor de los estados $S'$ a los que se puede llegar desde $S$. Dicho de otra manera, los estados que pueden conducir a un estado malo ser√°n tambi√©n malos (y viceversa). Esta idea est√° codificada en las **ecuaciones de Bellman**:\n",
    "\n",
    "$$v_\\pi(s)=\\sum_{a}\\pi(a|s)\\sum_{s',r} p(s',r | s,a)\\left[r + \\gamma v_\\pi(s')\\right]$$\n",
    "\n",
    "La misma idea se puede aplicar a la hora de estimar el valor de ejecutar la acci√≥n $a$ estando en el estado $s$. Las acciones $a$ que lleven a estados $s'$ cuyas acciones tengan valores altos tendr√°n, a su vez, valores altos (y viceversa). \n",
    "\n",
    "$$q_\\pi(s,a)=\\sum_{s',r} p(s',r | s,a)\\left[r + \\gamma \\sum_{a'}\\pi(a'|s')q_\\pi(s',a')\\right]$$\n",
    "\n",
    "Aplicar esta idea nos permitir√° hacer *bootstrapping* en nuestros c√°lculos: a la hora de calcular el valor de un estado podremos aprovechar la aproximaci√≥n que tenemos para estados adyacentes.\n",
    "\n",
    "### SARSA\n",
    "El primer m√©todo que probaremos es SARSA. Este algoritmo se denomina as√≠ por las variables involucradas en cada paso de actualizaci√≥n de la estimaci√≥n de $Q(S,A)$. Las variables son $S_t,A_t,R_{t+1},S_{t+1},A_{t+1}$ y la actualizaci√≥n utiliza esta f√≥rmula:\n",
    "\n",
    "$$Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha [R_{t+1} + \\gamma Q(S_{t+1},A_{t+1}) - Q(S_t,A_t)]$$\n",
    "\n",
    "El algoritmo ejecutar√° repetidos episodios (potencialmente infinitos; es un algoritmo de control y el agente podr√≠a seguirlo utilizando durante toda su existencia) y, para cada paso del episodio har√° la actualizaci√≥n seg√∫n la f√≥rmula indicada arriba. Esto requiere que haya decidido la siguiente acci√≥n a tomar ($A_{t+1}$) antes de actualizar el valor de la acci√≥n anterior ($A_t$).\n",
    "\n",
    "![Sarsa](./img/sarsa.png)\n",
    "\n",
    "Para facilitar la implementaci√≥n, vamos primero a escribir un par de funciones auxiliares que nos permitan representar estados y muestrear acciones a partir de $Q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8e4c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.9 # Para este problema vamos a utilizar un factor de descuento de 0.9, es decir, las recompensas futuras se descontar√°n un 10% por cada paso que sea necesario para obtenerlas.\n",
    "\n",
    "env.max_steps = 5000 # Fijamos el n√∫mero m√°ximo de acciones por episodio a 5000, para permitir episodios largos\n",
    "\n",
    "# FUNCIONES AUXILIARES\n",
    "#TODO - Recupera la funci√≥n get_estado del laboratorio 4. Esta funci√≥n codifica el estado del entorno en una lista de tres elementos: columna, fila, orientaci√≥n\n",
    "def get_estado(env:gym.Env) -> Estado:\n",
    "...\n",
    "\n",
    "# COMPROBACI√ìN\n",
    "env.reset()\n",
    "estado_actual = get_estado(env)\n",
    "assert estado_actual.x == 0, f'El estado inmediatamente despu√©s de resetear debe indicar x=0 y el tuyo indica {estado_actual.x}'\n",
    "assert estado_actual.y == 0, f'El estado inmediatamente despu√©s de resetear debe indicar y=0 y el tuyo indica {estado_actual.y}'\n",
    "assert estado_actual.dir == 0, f'El estado inmediatamente despu√©s de resetear debe indicar dir=0 y el tuyo indica {estado_actual.dir}'\n",
    "\n",
    "# TODO - Escribe una funci√≥n que dados los valores q de las distintas acciones en un estado concreto, devuelva una acci√≥n haciendo una selecci√≥n epsilon-greedy a partir de los q_valores.\n",
    "def get_accion_epsilon_greedy(q_valores:np.ndarray, epsilon:float):\n",
    "    '''\n",
    "    Selecciona una acci√≥n bas√°ndose en los q_values proporcionados. Una fracci√≥n de las veces (indicada por epsilon) devolver√° una acci√≥n al azar\n",
    "    \n",
    "    Argumentos:\n",
    "    q_values -- Lista con los q valores de las acciones entre las que seleccionar.\n",
    "    epsilon -- Fracci√≥n de las veces que se devolver√° una acci√≥n al azar\n",
    "    '''\n",
    "    if np.random.random()<epsilon:\n",
    "        # TODO - Devuelve una acci√≥n al azar\n",
    "        return ...\n",
    "    # TODO - Devuelve el √≠ndice del mayor q valor\n",
    "    return ...\n",
    "\n",
    "\n",
    "# COMPROBACI√ìN\n",
    "assert(get_accion_epsilon_greedy([1,2,4],0)==2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fd9c29",
   "metadata": {},
   "source": [
    "Para algoritmos derivados de SARSA haremos _exploring starts_, es decir, comienzar√°n cada vez en un estado al azar. Esto facilita la exploraci√≥n y, por tanto, el entrenamiento ya que facilita que se den episodios que el agente sea capaz de resolver incluso con una pol√≠tica aleatoria.\n",
    "\n",
    "Deberemos crear funciones que nos permitan llevar esto a cabo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a61ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Escribe una funci√≥n que genere la codificaci√≥n para un estado v√°lido al azar. Devolver√° una lista con tres valores que indiquen columna, fila y orientaci√≥n\n",
    "def get_estado_aleatorio() -> Estado:\n",
    "    ...\n",
    "\n",
    "# TODO - Escribe una funci√≥n que establezca el entorno para que coincida con lo indicado por un estado dado\n",
    "def set_estado(env:gym.Env, estado: Estado):\n",
    "    ...\n",
    "\n",
    "# COMPROBACI√ìN\n",
    "env.reset()\n",
    "muestra_entorno(env) # Debe mostrar al agente en la casilla 0,0\n",
    "set_estado(env, Estado(x=2, y=3, dir=0))\n",
    "muestra_entorno(env) # Debe mostrar al agente en la casilla 2,3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb0684b",
   "metadata": {},
   "source": [
    "Ya tenemos lo necesario para implementar SARSA. Vamos a establecer un n√∫mero fijo de iteraciones y hacer que devuelva los valores $Q$ al terminar. Adem√°s, incluiremos mensajes al terminar cada episodio que indiquen:\n",
    " - El n√∫mero de episodio que se ha finalizado\n",
    " - La recompensa obtenida\n",
    " - El n√∫mero de acciones necesarias para completar el episodio\n",
    " - La recompensa media obtenida durante los √∫ltimos 100 episodios (si se han ejecutado al menos 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1751c82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(num_episodios:int = 500, ALPHA:float = 0.1, EPSILON:float = 0.25) -> np.ndarray:\n",
    "    # TODO - Inicializa los valores Q a cero con el shape adecuado\n",
    "    q_valores = ...\n",
    "    \n",
    "    # En ultimos100retornos almacenaremos los retornos de los √∫ltimos 100 episodios\n",
    "    ultimos100retornos = []\n",
    "    # TODO - Haz tantos episodios como indica num_episodes\n",
    "        # TODO - Devuelve el entorno a su estado inicia\n",
    "        # TODO - Initialize S\n",
    "        # TODO - Choose A from S using policy derived from Q (e.g. epsilon-greedy)\n",
    "        \n",
    "        retorno = 0\n",
    "        num_pasos = 0\n",
    "        # TODO - Loop for each step of episode (repite hasta que el episodio termine)\n",
    "            num_pasos += 1\n",
    "            # TODO - Take action A, observe R, S'\n",
    "            # TODO - Acumula la recompensa R a returns para poder imprimir el retorno del episodio\n",
    "            # TODO - Choose A' from S' using policy derived from Q (e.g. epsilon-greedy)\n",
    "            # TODO - Q(S,A) <- Q(S,A) + ALPHA * [R + GAMMA * Q(S',A') - Q(S,A)]\n",
    "            # TODO - S <- S'; A <- A'\n",
    "            \n",
    "        # Tras terminar el episodio almacenamos el retorno en los 100 √∫ltimos...\n",
    "        ultimos100retornos.append(retorno)\n",
    "        string_retornos100 = ''\n",
    "        if len(ultimos100retornos)==101: # ... y si hay ya m√°s de 100...\n",
    "            ultimos100retornos.pop(0) # ...quitamos el m√°s antiguo (para tener siempre 100, no m√°s)...\n",
    "            #... y preparamos el mensaje para mostrar la media de los retornos\n",
    "            string_retornos100=f'(retorno medio de {np.mean(ultimos100retornos)} en los √∫ltimos 100 episodios)'\n",
    "        # Mostramos el mensaje tras cada episodio\n",
    "        print(f'Terminado episodio {i} con retorno {retorno} en {num_pasos} pasos {string_retornos100}')\n",
    "    return q_valores\n",
    "\n",
    "# Entrenamos al agente usando los par√°metros por defecto de la funci√≥n que acabamos de definir\n",
    "q_valores_sarsa = sarsa()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96b23b5",
   "metadata": {},
   "source": [
    "El proceso de aprendizaje tiene una componente aleatoria importante, por lo que dos ejecuciones consecutivas pueden aprender valores $Q$ diferentes de las que se deriven pol√≠ticas $\\pi$ tambi√©n diferentes. No obstante, si has implementado bien el algoritmo, deber√≠as haber observado lo siguiente:\n",
    "  1. Inicialmente hay muchos episodios con retorno 0 (el agente acaba en la lava). La duraci√≥n de estos episodios es muy variable. Adem√°s, si imprimi√©semos `q_values` tras cada uno de estos episodios, ¬°ver√≠amos que $Q$ no se modifica! Cada paso utiliza $R$ y $Q(S',A')$ para actualizar $Q(S,A)$, pero si tanto $R$ como $Q(S',A')$ son cero, el valor de $Q(S,A)$ no cambiar√°.\n",
    "  1. Con el tiempo, y por pura casualidad, el agente terminar√° alg√∫n episodio llegando a la meta. Esto le reportar√° una recompensa de 1, lo que har√° que $Q(S_t,A_t)$ se actualice a un valor positivo.\n",
    "  1. La pr√≥xima vez que el agente est√© en $S_t$, elegir√° la acci√≥n $A_t$ (salvo que toque acci√≥n aleatoria por el $\\epsilon$), lo que har√° que termine el episodio con recompensa 1. **¬°Habr√° aprendido una pol√≠tica √∫til para la √∫ltima casilla!** Pero, adem√°s, el estado $S_{t-1}$ que llev√≥ a $S_t$ (usando la acci√≥n $A_{t-1}$) ver√° su $Q(S_{t-1},A_{t-1})$ actualizado a un valor positivo. **¬°Ya sabr√° una pol√≠tica √∫til para las dos √∫ltimas casillas!**\n",
    "  1. Mediante este proceso, la informaci√≥n respecto a qu√© pol√≠tica es √∫til se ir√° propagando desde las casillas aleda√±as a la meta a sus vecinas, y de estas a sus vecinas y as√≠ sucesivamente hasta cubrir el tablero entero. A esto hay que a√±adir que, tambi√©n por pura casualidad, el agente puede encontrar otros caminos que conducen a la meta y lanzar este mismo proceso de propagaci√≥n pero siguiendo otro camino que conduzca a la meta.\n",
    "  1. El efecto de esto es que, a medida que transcurren los episodios ocurren dos cosas:\n",
    "    - El n√∫mero de episodios con retorno positivo aumenta\n",
    "    - La duraci√≥n de los episodios con retorno positivos disminuye (el agente va m√°s tiempo por \"camino conocido\")\n",
    "  1. Esto repercute en el retorno medio de los 100 √∫ltimos episodios: comenzar√° siendo min√∫sculo, pero llegado un punto subir√° con gran velocidad.\n",
    "  1. El retorno de los √∫ltimos 100 episodios alcanzar√° un punto m√°ximo en torno al cual oscilar√° hacia el final.\n",
    "  \n",
    "  \n",
    "Visualicemos la pol√≠tica aprendida. Vamos a mostrar, para cada casilla, la orientaci√≥n que tiene el agente cuando la acci√≥n preferida (la que tiene mayor valor $Q$) sea `FORWARD`.\n",
    "\n",
    ">**Recordatorio**\n",
    ">\n",
    "> Las orientaciones se codifican as√≠:\n",
    "> - 0 $\\rightarrow$ derecha\n",
    "> - 1 $\\rightarrow$ abajo\n",
    "> - 2 $\\rightarrow$ izquierda\n",
    "> - 3 $\\rightarrow$ arriba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e183e902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta funci√≥n devuelve un car√°cter de flecha que representa la orientaci√≥n del agente para la cual los q valores recomiendan la acci√≥n forward\n",
    "def get_flecha_direccion_maximo_valor(q_values:np.ndarray) -> str:\n",
    "    accion_preferida_por_orientacion = q_values.argmax(axis=1) # Para cada una de las cuatro orientaciones, calculamos qu√© acci√≥n se prefiere\n",
    "    direccion = np.argmax(accion_preferida_por_orientacion == 2) # De las orientaciones que prefieran FORWARD, tomamos una (la primera)\n",
    "    # En funci√≥n de la orientaci√≥n, devolvemos la flecha apropiada.\n",
    "    if direccion==0:\n",
    "        return '‚Æï'\n",
    "    if direccion==1:\n",
    "        return '‚¨á'\n",
    "    if direccion==2:\n",
    "        return '‚¨Ö'\n",
    "    if direccion==3:\n",
    "        return '‚¨Ü'\n",
    "    return '?'\n",
    "    \n",
    "# Muestra la pol√≠tica por pantalla\n",
    "def dibuja_politica(q_valores:np.ndarray) -> None:\n",
    "    # Recorre las filas del tablero...\n",
    "    for i in range(q_valores.shape[1]):\n",
    "        # ... componiendo una l√≠nea por fila...\n",
    "        linea = ''\n",
    "        for j in range(q_valores.shape[0]): # Recorre las columnas\n",
    "            if i==0 and j==6: # Pintamos la meta\n",
    "                    linea+='üü•'\n",
    "            elif (i==0 or i==1)and j>1 and j<5: # Pintamos la lava\n",
    "                    linea+='üüß'\n",
    "            else:\n",
    "                linea+=get_flecha_direccion_maximo_valor(q_valores[j,i]) # Pintamos la flecha apropiada para esta casilla\n",
    "        # ...que finalmente imprime\n",
    "        print(linea)\n",
    "        \n",
    "# TODO - Muestra la pol√≠tica que se deriva de los valores aprendidos por SARSA\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c565d901",
   "metadata": {},
   "source": [
    "Hemos comprobado que nuestro agente tiene que hacer muchos episodios que no le llevan a aprender porque no le dan ninguna recompensa, lo cual provoca que no pueda actualizar $Q(S,A)$ para ninguno de los pares $S,A$ por los que transita.\n",
    "\n",
    "**¬øQu√© crees que ocurrir√≠a si di√©semos una recompensa negativa a caer en la lava? ¬øCambiar√≠a la velocidad a la que aprende el agente?**\n",
    "\n",
    "Haz la prueba a√±adiendo esto despu√©s de cada paso:\n",
    "```python\n",
    "    if completado and reward==0:\n",
    "        reward = -1\n",
    "```\n",
    "\n",
    "Modifica la funci√≥n `sarsa` y ejecuta la celda siguiente. **¬øQu√© observas?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5c26b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_valores_sarsa_con_lava_negativa = sarsa() # Sarsa debe haberse modificado para que caer a la lava proporcione recompensa -1\n",
    "dibuja_politica(q_valores_sarsa_con_lava_negativa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d72b5f8",
   "metadata": {},
   "source": [
    "### Q-learning\n",
    "El algoritmo Q-learning es de los m√°s populares dentro del Aprendizaje por Refuerzo. Es muy similar a SARSA, pero tiene una importante diferencia: es *off-policy*. Si SARSA hac√≠a las actualizaciones de $Q(S_t,A_t)$ en funci√≥n de la acci√≥n $A_{t+1}$ que tomaba, Q-learning va a hacer dichas actualizaciones independientemente de la acci√≥n que finalmente tome. Por tanto, estar√° utilizando una pol√≠tica para explorar pero otra para actualizar. En concreto, Q-learning actualiza seg√∫n esta f√≥rmula:\n",
    "\n",
    "$$Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha [R_{t+1} + \\gamma \\max_aQ(S_{t+1},a) - Q(S_t,A_t)]$$\n",
    "\n",
    "Q-learning no actualiza en funci√≥n de la acci√≥n que va a tomar, sino en funci√≥n de la mejor acci√≥n posible. El algoritmo completo aparece descrito a continuaci√≥n.\n",
    "\n",
    "![Q-learning](./img/q-learning.png)\n",
    "\n",
    "Implementemos Q-learning mostrando los mismos mensajes a cada paso que mostr√°bamos con SARSA. Mant√©n las recompensas negativas para cuando el agente caiga en la lava."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca4113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Implementa q_learning\n",
    "#  Utiliza el mismo c√≥digo que para sarsa pero cambiando la actualizaci√≥n\n",
    "#  Recuerda mantener las recompensas negativas para cuando el agente caiga en la lava y mostrar los mensajes\n",
    "def q_learning(num_episodios:int = 500, ALPHA:float = 0.1, EPSILON:float = 0.25) -> np.ndarray:\n",
    "    ...\n",
    "\n",
    "q_valores_q = q_learning()\n",
    "dibuja_politica(q_valores_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c470b8a7",
   "metadata": {},
   "source": [
    "### Comparativa SARSA vs Q-Learning\n",
    "Compara los resultados obtenidos por ambos algoritmos. Deber√≠as observar lo siguiente:\n",
    "  - La pol√≠tica obtenida por SARSA es m√°s conservadora (no quiere estar cerca de la lava), mientras que la de Q-learning es m√°s optimista (no le importa estar cerca de la lava; conf√≠a en su pol√≠tica).\n",
    "  - Por tanto, los episodios de Q-learning son m√°s cortos que los de SARSA.\n",
    "  - Sin embargo, al seguir una pol√≠tica $\\epsilon$-greedy, estar en una casilla contigua a la lava desemboca en una recompensa de -1 (caer a la lava) un $\\frac{\\epsilon}{4}$ de las veces, por lo que el retorno medio de Q-learning es m√°s bajo.\n",
    "  \n",
    "Completa la siguiente celda para comprobarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaaa5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprueba_politica_derivada(q_values:np.ndarray, epsilon:float) -> None:\n",
    "    env.reset()\n",
    "    # TODO - Escribe un bucle que simule un episodio completo siguiendo una pol√≠tica COMPLETAMENTE GREEDY respecto a q_values\n",
    "    # y muestra su duraci√≥n y su retorno\n",
    "    ...\n",
    "    print(f'La pol√≠tica greedy hace un episodio de {contador} pasos con un retorno de {retorno}')\n",
    "    \n",
    "    env.reset()\n",
    "    # TODO - Escribe otro bucle que simule 500 episodios siguiendo una pol√≠tica epsilon-greedy respecto a q_values\n",
    "    # y muestra su duraci√≥n media y su retorno medio\n",
    "    ...    \n",
    "    print(f'La pol√≠tica greedy hace episodios de {np.mean(duraciones)} pasos de media con un retorno de {np.mean(retornos)} de media')\n",
    "\n",
    "print('SARSA')\n",
    "comprueba_politica_derivada(q_valores_sarsa_con_lava_negativa, 0.25)\n",
    "dibuja_politica(q_valores_sarsa_con_lava_negativa)\n",
    "print('\\nQ-LEARNING')\n",
    "comprueba_politica_derivada(q_valores_q, 0.25)\n",
    "dibuja_politica(q_valores_q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
